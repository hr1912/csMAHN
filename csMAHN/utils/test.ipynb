{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T05:28:08.341847600Z",
     "start_time": "2024-03-14T05:28:03.996893100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/public/workspace/ruru_97/anaconda3/envs/schgnn/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": "<module 'utils.preprocess' from '/public/workspace/ruru_97/projects/schgnn/csMAHN/utils/preprocess.py'>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append('/public/workspace/ruru_97/projects/schgnn/csMAHN')\n",
    "import utils.preprocess as pp\n",
    "from utils.utility import *\n",
    "from utils.train import Trainer\n",
    "\n",
    "import importlib as im\n",
    "im.reload(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T05:28:08.352510900Z",
     "start_time": "2024-03-14T05:28:08.347915100Z"
    }
   },
   "outputs": [],
   "source": [
    "reverse=False\n",
    "aligned=True\n",
    "tissue=\"pancreas\"\n",
    "gse_ids=[\"GSE84113\", \"GSE84113\"]\n",
    "species=('human', 'mouse')\n",
    "dsnames=('human', 'mouse')\n",
    "path_data='/public/workspace/ruru_97/projects/data'\n",
    "resdir='/public/workspace/ruru_97/projects/schgnn/result'\n",
    "homo_method='biomart'\n",
    "key_class='cell_type'\n",
    "n_hvgs=2000\n",
    "n_degs=50\n",
    "\n",
    "seed = 123\n",
    "stages=[200, 200, 200]\n",
    "nfeats=64  # enbedding size #128\n",
    "hidden=64 # 128\n",
    "input_drop=0.2\n",
    "att_drop=0.2\n",
    "residual=True\n",
    "\n",
    "threshold=0.9 # 0.8\n",
    "lr=0.01  # lr = 0.01\n",
    "weight_decay=0.001\n",
    "patience=100\n",
    "enhance_gama=10\n",
    "simi_gama=0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T05:28:09.447367800Z",
     "start_time": "2024-03-14T05:28:08.351509800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/public/workspace/ruru_97/projects/schgnn/result/pancreas/pancreas-GSE84113_human-GSE84113_mouse-03-14-13.28.07/model_/mutistages\n",
      "----raw----\n",
      "                    reference  query\n",
      "alpha                    2326    191\n",
      "beta                     2525    894\n",
      "delta                     601    218\n",
      "ductal                   1077    275\n",
      "endothelial               252    139\n",
      "gamma                     255     41\n",
      "macrophage                 55     36\n",
      "quiescent_stellate        173     47\n",
      "----new----\n",
      "                    reference  query\n",
      "alpha                    2326    191\n",
      "beta                     2525    894\n",
      "delta                     601    218\n",
      "ductal                   1077    275\n",
      "endothelial               252    139\n",
      "gamma                     255     41\n",
      "macrophage                 55     36\n",
      "quiescent_stellate        173     47\n"
     ]
    }
   ],
   "source": [
    "# reverse reference to query, query to reference.\n",
    "if reverse:\n",
    "    gse_ids = gse_ids[::-1]\n",
    "    species = species[::-1]\n",
    "    dsnames = dsnames[::-1]\n",
    "seed_all(seed)\n",
    "path_homo = f'{path_data}/homo/{homo_method}/input/{species[0]}_to_{species[1]}.txt'\n",
    "path_specie_1 = f'{path_data}/ByTissue/{tissue}/{gse_ids[0]}/input/{dsnames[0]}.h5ad'\n",
    "path_specie_2 = f'{path_data}/ByTissue/{tissue}/{gse_ids[1]}/input/{dsnames[1]}.h5ad'\n",
    "\n",
    "# make file to save\n",
    "time_tag = make_nowtime_tag()\n",
    "curdir = f'{resdir}/{tissue}/{tissue}-{gse_ids[0]}_{dsnames[0]}-{gse_ids[1]}_{dsnames[1]}-{time_tag}'\n",
    "model_dir = os.path.join(curdir, 'model_')\n",
    "figdir = os.path.join(curdir, 'fig_')\n",
    "os.mkdir(curdir)\n",
    "os.mkdir(figdir)\n",
    "os.mkdir(model_dir)\n",
    "checkpt_file = model_dir + \"/mutistages\"\n",
    "print(checkpt_file)\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    res_dir = os.path.join(curdir, f'res_{i}')\n",
    "    os.mkdir(res_dir)\n",
    "homo = pd.read_csv(path_homo)\n",
    "adata_species_1 = sc.read_h5ad(path_specie_1)\n",
    "adata_species_2 = sc.read_h5ad(path_specie_2)\n",
    "\n",
    "if aligned:\n",
    "    adata_species_1, adata_species_2 = pp.aligned_type([adata_species_1, adata_species_2], 'cell_type')\n",
    "else:\n",
    "    pp.get_type_counts_info(adata_species_1, adata_species_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T05:28:09.457755700Z",
     "start_time": "2024-03-14T05:28:09.451854600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: refernece:GSE84113_human 7264 cells x 20125 gene -> query:GSE84113_mouse 1841 cells x 14878 gene in pancreas\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Task: refernece:{gse_ids[0]}_{dsnames[0]} {adata_species_1.shape[0]} cells x {adata_species_1.shape[1]} gene -> query:{gse_ids[1]}_{dsnames[1]} {adata_species_2.shape[0]} cells x {adata_species_2.shape[1]} gene in {tissue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T05:32:09.689103200Z",
     "start_time": "2024-03-14T05:28:09.455535600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homolog information follows\n",
      "ortholog_one2one      16612\n",
      "ortholog_many2many     5356\n",
      "ortholog_one2many      3174\n",
      "Name: Mouse homology type, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time2 of processing adatas is 204.9959225654602\n",
      "Leiden results:\n",
      "0    515\n",
      "1    276\n",
      "2    274\n",
      "3    256\n",
      "4    222\n",
      "5    137\n",
      "6     76\n",
      "7     48\n",
      "8     37\n",
      "Name: leiden, dtype: int64\n",
      "the time of leiden is 1.2902448177337646\n",
      "the time of degs is 23.64616823196411\n",
      "--------------hvgs, degs info---------------\n",
      "num of reference_hvgs,reference_degs,reference_higs are 2000,353,2125\n",
      "num of query_hvgs,query_degs,query_higs are 2000,389,2195\n",
      "--------------gene nodes info---------------\n",
      "num of reference_gene_node is 3305\n",
      "num of query_gene_node is 3037\n",
      "--------------homo edges---------------\n",
      "ortholog_one2one      2508\n",
      "ortholog_one2many      194\n",
      "ortholog_many2many     194\n",
      "Name: Mouse homology type, dtype: int64\n",
      "--------------homo edges---------------\n",
      "ortholog_one2one    2508\n",
      "Name: Mouse homology type, dtype: int64\n",
      "knn time is 7.228848934173584 s\n",
      "mnn time is 0.005603313446044922 s\n",
      "the time of compute mnn is  7.78513503074646 s\n",
      "Inter-net pairs ACC: 0.9758064516129032\n",
      "-------------------nodes info-------------------\n",
      "the num of cell feats is 503\n",
      "the num of cell nodes is 9105\n",
      "the num of gene nodes is 6342\n",
      "Times preprocess for graph:240.22\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# knn时间较长\n",
    "adatas, features_genes, nodes_genes, scnets, one2one, n2n = pp.process_for_graph([adata_species_1, adata_species_2],\n",
    "                                                                                    homo,\n",
    "                                                                                    key_class,\n",
    "                                                                                    'leiden',\n",
    "                                                                                    n_hvgs=n_hvgs,\n",
    "                                                                                    n_degs=n_degs)\n",
    "g, inter_net, one2one_gene_nodes_net, cell_label, n_classes, list_idx = pp.make_graph(adatas,\n",
    "                                                                                        aligned,\n",
    "                                                                                        key_class,\n",
    "                                                                                        features_genes,\n",
    "                                                                                        nodes_genes,\n",
    "                                                                                        scnets,\n",
    "                                                                                        one2one,\n",
    "                                                                                        n2n,\n",
    "                                                                                        has_mnn=True,\n",
    "                                                                                        seed=seed)\n",
    "end = time.time()\n",
    "# 包括预处理时间\n",
    "print('Times preprocess for graph:{:.2f}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:02:02.994496700Z",
     "start_time": "2024-03-14T06:02:02.953896600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'utils.train' from '/public/workspace/ruru_97/projects/schgnn/csMAHN/utils/train.py'>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils.train as train1\n",
    "im.reload(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:14:01.916075500Z",
     "start_time": "2024-03-14T06:03:09.423332800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYHGNN(\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (input_drop): Dropout(p=0.2, inplace=False)\n",
      "  (cell_embedings): ParameterDict(\n",
      "      (C): Parameter containing: [torch.FloatTensor of size 503x64]\n",
      "      (CC): Parameter containing: [torch.FloatTensor of size 503x64]\n",
      "      (CCGGC): Parameter containing: [torch.FloatTensor of size 503x64]\n",
      "      (CGGC): Parameter containing: [torch.FloatTensor of size 503x64]\n",
      "      (CGGCC): Parameter containing: [torch.FloatTensor of size 503x64]\n",
      "      (CCGGCC): Parameter containing: [torch.FloatTensor of size 503x64]\n",
      "  )\n",
      "  (cell_project_layers): Sequential(\n",
      "    (0): Conv1d1x1()\n",
      "    (1): LayerNorm((6, 64), eps=1e-05, elementwise_affine=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Conv1d1x1()\n",
      "    (5): LayerNorm((6, 64), eps=1e-05, elementwise_affine=True)\n",
      "    (6): PReLU(num_parameters=1)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (cell_semantic_aggr_layers): Transformer(\n",
      "    (query): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (att_drop): Dropout(p=0.2, inplace=False)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (cell_concat_project_layer): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (res_fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (classifier): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (outnorm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "# Params: 280317\n",
      "Epoch 0, train loss 23.9513, train acc 38.70\n",
      "Time: 1.7827,Val loss: 1.6751, Pred loss: 0.0000, Val acc: 40.01, Pred acc: 16.13, ami: 0.2071, f1: 0.17\n",
      "Epoch 5, train loss 8.1814, train acc 91.47\n",
      "Time: 1.4960,Val loss: 0.7391, Pred loss: 0.0000, Val acc: 91.05, Pred acc: 69.09, ami: 0.3954, f1: 0.66\n",
      "Epoch 10, train loss 4.3733, train acc 94.29\n",
      "Time: 1.2585,Val loss: 0.6394, Pred loss: 0.0000, Val acc: 94.49, Pred acc: 78.82, ami: 0.4383, f1: 0.79\n",
      "Epoch 11, train loss 3.9091, train acc 94.96\n",
      "Time: 1.5298,Val loss: 0.6074, Pred loss: 0.0000, Val acc: 94.77, Pred acc: 80.45, ami: 0.4556, f1: 0.81\n",
      "[Best Epoch 11],Val 94.7658, Pred 80.4454\n",
      "Epoch 12, train loss 3.4472, train acc 95.80\n",
      "Time: 1.2153,Val loss: 0.5728, Pred loss: 0.0000, Val acc: 95.66, Pred acc: 82.13, ami: 0.4711, f1: 0.82\n",
      "[Best Epoch 12],Val 95.6612, Pred 82.1293\n",
      "Epoch 13, train loss 3.0452, train acc 96.63\n",
      "Time: 1.3749,Val loss: 0.5401, Pred loss: 0.0000, Val acc: 96.56, Pred acc: 83.65, ami: 0.4882, f1: 0.84\n",
      "[Best Epoch 13],Val 96.5565, Pred 83.6502\n",
      "Epoch 14, train loss 2.7315, train acc 97.08\n",
      "Time: 1.5263,Val loss: 0.5110, Pred loss: 0.0000, Val acc: 97.04, Pred acc: 84.95, ami: 0.5066, f1: 0.85\n",
      "[Best Epoch 14],Val 97.0386, Pred 84.9538\n",
      "Epoch 15, train loss 2.4421, train acc 97.61\n",
      "Time: 1.5484,Val loss: 0.4845, Pred loss: 0.0000, Val acc: 97.45, Pred acc: 86.58, ami: 0.5410, f1: 0.86\n",
      "Epoch 15, train loss 2.4421, train acc 97.61\n",
      "Time: 1.5484,Val loss: 0.4845, Pred loss: 0.0000, Val acc: 97.45, Pred acc: 86.58, ami: 0.5410, f1: 0.86\n",
      "[Best Epoch 15],Val 97.4518, Pred 86.5834\n",
      "Epoch 16, train loss 2.2044, train acc 97.85\n",
      "Time: 1.4689,Val loss: 0.4591, Pred loss: 0.0000, Val acc: 97.87, Pred acc: 87.78, ami: 0.5639, f1: 0.88\n",
      "[Best Epoch 16],Val 97.8650, Pred 87.7784\n",
      "Epoch 17, train loss 1.9922, train acc 98.02\n",
      "Time: 1.5479,Val loss: 0.4352, Pred loss: 0.0000, Val acc: 97.93, Pred acc: 88.65, ami: 0.5813, f1: 0.88\n",
      "[Best Epoch 17],Val 97.9339, Pred 88.6475\n",
      "Epoch 18, train loss 1.7855, train acc 98.31\n",
      "Time: 1.2252,Val loss: 0.4132, Pred loss: 0.0000, Val acc: 98.21, Pred acc: 89.57, ami: 0.5948, f1: 0.89\n",
      "[Best Epoch 18],Val 98.2094, Pred 89.5709\n",
      "Epoch 19, train loss 1.6357, train acc 98.45\n",
      "Time: 1.3088,Val loss: 0.3936, Pred loss: 0.0000, Val acc: 98.28, Pred acc: 90.71, ami: 0.6187, f1: 0.91\n",
      "[Best Epoch 19],Val 98.2782, Pred 90.7116\n",
      "Epoch 20, train loss 1.4963, train acc 98.54\n",
      "Time: 1.4847,Val loss: 0.3760, Pred loss: 0.0000, Val acc: 98.42, Pred acc: 91.42, ami: 0.6360, f1: 0.91\n",
      "\n",
      "Epoch 20, train loss 1.4963, train acc 98.54\n",
      "Time: 1.4847,Val loss: 0.3760, Pred loss: 0.0000, Val acc: 98.42, Pred acc: 91.42, ami: 0.6360, f1: 0.91\n",
      "[Best Epoch 20],Val 98.4160, Pred 91.4177\n",
      "Epoch 21, train loss 1.3596, train acc 98.74\n",
      "Time: 1.4430,Val loss: 0.3595, Pred loss: 0.0000, Val acc: 98.48, Pred acc: 92.02, ami: 0.6509, f1: 0.92\n",
      "[Best Epoch 21],Val 98.4848, Pred 92.0152\n",
      "Epoch 22, train loss 1.2456, train acc 98.80\n",
      "Time: 1.3079,Val loss: 0.3439, Pred loss: 0.0000, Val acc: 98.55, Pred acc: 92.72, ami: 0.6726, f1: 0.93\n",
      "[Best Epoch 22],Val 98.5537, Pred 92.7214\n",
      "Epoch 23, train loss 1.1597, train acc 98.86\n",
      "Time: 1.5502,Val loss: 0.3289, Pred loss: 0.0000, Val acc: 98.76, Pred acc: 92.99, ami: 0.6821, f1: 0.93\n",
      "[Best Epoch 23],Val 98.7603, Pred 92.9929\n",
      "Epoch 24, train loss 1.0621, train acc 98.92\n",
      "Time: 1.3976,Val loss: 0.3146, Pred loss: 0.0000, Val acc: 98.76, Pred acc: 93.43, ami: 0.6937, f1: 0.93\n",
      "[Best Epoch 24],Val 98.7603, Pred 93.4275\n",
      "Epoch 25, train loss 0.9720, train acc 98.95\n",
      "Time: 1.3654,Val loss: 0.3009, Pred loss: 0.0000, Val acc: 98.83, Pred acc: 93.70, ami: 0.7010, f1: 0.94\n",
      "\n",
      "Epoch 25, train loss 0.9720, train acc 98.95\n",
      "Time: 1.3654,Val loss: 0.3009, Pred loss: 0.0000, Val acc: 98.83, Pred acc: 93.70, ami: 0.7010, f1: 0.94\n",
      "[Best Epoch 25],Val 98.8292, Pred 93.6991\n",
      "Epoch 26, train loss 0.9038, train acc 99.07\n",
      "Time: 1.5227,Val loss: 0.2882, Pred loss: 0.0000, Val acc: 98.83, Pred acc: 94.19, ami: 0.7100, f1: 0.94\n",
      "[Best Epoch 26],Val 98.8292, Pred 94.1879\n",
      "Epoch 27, train loss 0.8340, train acc 99.16\n",
      "Time: 1.3517,Val loss: 0.2765, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 94.46, ami: 0.7157, f1: 0.94\n",
      "[Best Epoch 27],Val 98.9669, Pred 94.4595\n",
      "Epoch 28, train loss 0.7842, train acc 99.16\n",
      "Time: 1.4613,Val loss: 0.2663, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 94.68, ami: 0.7207, f1: 0.95\n",
      "[Best Epoch 28],Val 98.9669, Pred 94.6768\n",
      "Epoch 30, train loss 0.6773, train acc 99.24\n",
      "Time: 1.4954,Val loss: 0.2498, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 94.89, ami: 0.7275, f1: 0.95\n",
      "\n",
      "Epoch 30, train loss 0.6773, train acc 99.24\n",
      "Time: 1.4954,Val loss: 0.2498, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 94.89, ami: 0.7275, f1: 0.95\n",
      "[Best Epoch 30],Val 98.9669, Pred 94.8941\n",
      "Epoch 31, train loss 0.6275, train acc 99.29\n",
      "Time: 1.4269,Val loss: 0.2428, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 95.00, ami: 0.7331, f1: 0.95\n",
      "[Best Epoch 31],Val 98.9669, Pred 95.0027\n",
      "Epoch 32, train loss 0.5956, train acc 99.33\n",
      "Time: 1.4451,Val loss: 0.2360, Pred loss: 0.0000, Val acc: 98.90, Pred acc: 95.17, ami: 0.7358, f1: 0.95\n",
      "[Best Epoch 32],Val 98.8981, Pred 95.1657\n",
      "Epoch 33, train loss 0.5544, train acc 99.35\n",
      "Time: 1.4304,Val loss: 0.2294, Pred loss: 0.0000, Val acc: 98.90, Pred acc: 95.33, ami: 0.7425, f1: 0.95\n",
      "[Best Epoch 33],Val 98.8981, Pred 95.3286\n",
      "Epoch 34, train loss 0.5225, train acc 99.36\n",
      "Time: 1.6306,Val loss: 0.2229, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 95.33, ami: 0.7438, f1: 0.95\n",
      "[Best Epoch 34],Val 98.9669, Pred 95.3286\n",
      "Epoch 35, train loss 0.4916, train acc 99.35\n",
      "Time: 1.6590,Val loss: 0.2166, Pred loss: 0.0000, Val acc: 98.90, Pred acc: 95.38, ami: 0.7476, f1: 0.95\n",
      "\n",
      "Epoch 35, train loss 0.4916, train acc 99.35\n",
      "Time: 1.6590,Val loss: 0.2166, Pred loss: 0.0000, Val acc: 98.90, Pred acc: 95.38, ami: 0.7476, f1: 0.95\n",
      "[Best Epoch 35],Val 98.8981, Pred 95.3829\n",
      "Epoch 36, train loss 0.4647, train acc 99.33\n",
      "Time: 1.5230,Val loss: 0.2102, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 95.65, ami: 0.7551, f1: 0.95\n",
      "[Best Epoch 36],Val 98.9669, Pred 95.6545\n",
      "Epoch 37, train loss 0.4419, train acc 99.35\n",
      "Time: 1.6468,Val loss: 0.2039, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 96.25, ami: 0.7581, f1: 0.96\n",
      "[Best Epoch 37],Val 99.1047, Pred 96.2520\n",
      "Epoch 38, train loss 0.4155, train acc 99.40\n",
      "Time: 1.6543,Val loss: 0.1980, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 96.58, ami: 0.7625, f1: 0.96\n",
      "[Best Epoch 38],Val 99.1047, Pred 96.5779\n",
      "Epoch 39, train loss 0.3955, train acc 99.36\n",
      "Time: 1.5267,Val loss: 0.1925, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 96.80, ami: 0.7647, f1: 0.97\n",
      "[Best Epoch 39],Val 99.1736, Pred 96.7952\n",
      "Epoch 40, train loss 0.3723, train acc 99.42\n",
      "Time: 1.3662,Val loss: 0.1874, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 96.90, ami: 0.7660, f1: 0.97\n",
      "\n",
      "Epoch 40, train loss 0.3723, train acc 99.42\n",
      "Time: 1.3662,Val loss: 0.1874, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 96.90, ami: 0.7660, f1: 0.97\n",
      "[Best Epoch 40],Val 99.1736, Pred 96.9039\n",
      "Epoch 44, train loss 0.3118, train acc 99.45\n",
      "Time: 1.5234,Val loss: 0.1682, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 96.58, ami: 0.7694, f1: 0.97\n",
      "[Best Epoch 44],Val 99.2424, Pred 96.5779\n",
      "Epoch 45, train loss 0.2963, train acc 99.45\n",
      "Time: 1.2247,Val loss: 0.1638, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 96.36, ami: 0.7650, f1: 0.96\n",
      "Epoch 50, train loss 0.2409, train acc 99.47\n",
      "Time: 1.6394,Val loss: 0.1461, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 95.44, ami: 0.7442, f1: 0.95\n",
      "Epoch 55, train loss 0.2005, train acc 99.64\n",
      "Time: 1.0844,Val loss: 0.1271, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 94.13, ami: 0.7448, f1: 0.94\n",
      "Epoch 60, train loss 0.1703, train acc 99.71\n",
      "Time: 1.2367,Val loss: 0.1130, Pred loss: 0.0000, Val acc: 99.52, Pred acc: 92.83, ami: 0.7451, f1: 0.92\n",
      "Epoch 65, train loss 0.1460, train acc 99.72\n",
      "Time: 1.2303,Val loss: 0.1095, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 93.16, ami: 0.7343, f1: 0.93\n",
      "Epoch 70, train loss 0.1295, train acc 99.83\n",
      "Time: 1.1037,Val loss: 0.1013, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 94.46, ami: 0.7514, f1: 0.94\n",
      "Epoch 75, train loss 0.1144, train acc 99.85\n",
      "Time: 1.2912,Val loss: 0.0919, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 95.22, ami: 0.7576, f1: 0.95\n",
      "Epoch 80, train loss 0.1050, train acc 99.67\n",
      "Time: 1.3878,Val loss: 0.0935, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 93.16, ami: 0.7375, f1: 0.93\n",
      "Epoch 84, train loss 0.1024, train acc 99.69\n",
      "Time: 1.3011,Val loss: 0.0861, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 96.09, ami: 0.7774, f1: 0.96\n",
      "[Best Epoch 84],Val 99.2424, Pred 96.0891\n",
      "Epoch 85, train loss 0.0991, train acc 99.69\n",
      "Time: 1.2560,Val loss: 0.0835, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 97.23, ami: 0.7898, f1: 0.97\n",
      "\n",
      "Epoch 85, train loss 0.0991, train acc 99.69\n",
      "Time: 1.2560,Val loss: 0.0835, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 97.23, ami: 0.7898, f1: 0.97\n",
      "[Best Epoch 85],Val 99.3113, Pred 97.2298\n",
      "Epoch 90, train loss 0.0915, train acc 99.79\n",
      "Time: 1.2199,Val loss: 0.0803, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.28, ami: 0.7811, f1: 0.97\n",
      "Epoch 95, train loss 0.0830, train acc 99.85\n",
      "Time: 1.7678,Val loss: 0.0751, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 96.58, ami: 0.7626, f1: 0.97\n",
      "Epoch 100, train loss 0.0765, train acc 99.91\n",
      "Time: 1.0997,Val loss: 0.0728, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 94.57, ami: 0.7443, f1: 0.94\n",
      "Epoch 105, train loss 0.0733, train acc 99.86\n",
      "Time: 1.1918,Val loss: 0.0702, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 93.92, ami: 0.7339, f1: 0.94\n",
      "Epoch 110, train loss 0.0694, train acc 99.86\n",
      "Time: 0.9088,Val loss: 0.0658, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 91.25, ami: 0.7329, f1: 0.91\n",
      "Epoch 115, train loss 0.0634, train acc 99.88\n",
      "Time: 0.6684,Val loss: 0.0664, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 94.02, ami: 0.7559, f1: 0.94\n",
      "Epoch 120, train loss 0.0606, train acc 99.86\n",
      "Time: 0.8655,Val loss: 0.0660, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 92.12, ami: 0.7283, f1: 0.92\n",
      "Epoch 125, train loss 0.0590, train acc 99.90\n",
      "Time: 1.2793,Val loss: 0.0583, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 92.29, ami: 0.7392, f1: 0.92\n",
      "Epoch 130, train loss 0.0534, train acc 99.91\n",
      "Time: 1.0295,Val loss: 0.0661, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 94.46, ami: 0.7478, f1: 0.94\n",
      "Epoch 135, train loss 0.0505, train acc 100.00\n",
      "Time: 0.9322,Val loss: 0.0558, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 96.14, ami: 0.7571, f1: 0.96\n",
      "Epoch 140, train loss 0.0479, train acc 100.00\n",
      "Time: 0.9647,Val loss: 0.0563, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 93.86, ami: 0.7458, f1: 0.93\n",
      "Epoch 145, train loss 0.0463, train acc 100.00\n",
      "Time: 0.8858,Val loss: 0.0594, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 95.49, ami: 0.7575, f1: 0.95\n",
      "Epoch 150, train loss 0.0453, train acc 99.95\n",
      "Time: 1.2351,Val loss: 0.0520, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 93.75, ami: 0.7376, f1: 0.93\n",
      "Epoch 155, train loss 0.0443, train acc 99.98\n",
      "Time: 1.0442,Val loss: 0.0522, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 93.16, ami: 0.7288, f1: 0.93\n",
      "Epoch 160, train loss 0.0412, train acc 99.98\n",
      "Time: 1.0099,Val loss: 0.0513, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 92.56, ami: 0.7397, f1: 0.92\n",
      "Epoch 165, train loss 0.0395, train acc 99.98\n",
      "Time: 1.0217,Val loss: 0.0535, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 88.54, ami: 0.7076, f1: 0.87\n",
      "Epoch 170, train loss 0.0383, train acc 99.97\n",
      "Time: 1.1886,Val loss: 0.0500, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 93.43, ami: 0.7444, f1: 0.93\n",
      "Epoch 175, train loss 0.0361, train acc 100.00\n",
      "Time: 0.8596,Val loss: 0.0460, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 92.61, ami: 0.7220, f1: 0.92\n",
      "Epoch 180, train loss 0.0358, train acc 100.00\n",
      "Time: 1.2199,Val loss: 0.0465, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 90.55, ami: 0.7142, f1: 0.90\n",
      "Epoch 185, train loss 0.0363, train acc 100.00\n",
      "Time: 1.1837,Val loss: 0.0470, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 86.91, ami: 0.6990, f1: 0.85\n",
      "Best Epoch 85, Val 99.3113, Pred 97.2298, F1 0.9717, AMI 0.7898\n",
      "Stage 0 history model:\n",
      "\tTrain acc 99.6903 Val acc 99.3113 Pred acc 97.2298\n",
      "Stage: 1, threshold 0.9\n",
      "\t\tpred confident nodes: 1099 / 1841, pred confident_level: 0.9990900754928589\n",
      "Epoch 0, train loss 27.5771, train acc 36.58\n",
      "Time: 1.6494,Val loss: 1.7629, Pred loss: 0.0000, Val acc: 37.53, Pred acc: 65.24, ami: 0.3914, f1: 0.70\n",
      "Epoch 5, train loss 9.7288, train acc 90.57\n",
      "Time: 1.0744,Val loss: 0.7929, Pred loss: 0.0000, Val acc: 90.50, Pred acc: 92.45, ami: 0.6739, f1: 0.93\n",
      "Epoch 10, train loss 5.4615, train acc 94.72\n",
      "Time: 0.9849,Val loss: 0.6711, Pred loss: 0.0000, Val acc: 93.80, Pred acc: 93.97, ami: 0.6950, f1: 0.95\n",
      "Epoch 11, train loss 4.8973, train acc 95.35\n",
      "Time: 1.2191,Val loss: 0.6467, Pred loss: 0.0000, Val acc: 94.83, Pred acc: 93.92, ami: 0.6971, f1: 0.95\n",
      "[Best Epoch 11],Val 94.8347, Pred 93.9164\n",
      "Epoch 12, train loss 4.3670, train acc 95.73\n",
      "Time: 0.9827,Val loss: 0.6207, Pred loss: 0.0000, Val acc: 95.39, Pred acc: 94.24, ami: 0.6988, f1: 0.95\n",
      "[Best Epoch 12],Val 95.3857, Pred 94.2423\n",
      "Epoch 13, train loss 4.0040, train acc 96.28\n",
      "Time: 1.0969,Val loss: 0.5933, Pred loss: 0.0000, Val acc: 95.94, Pred acc: 94.41, ami: 0.7101, f1: 0.95\n",
      "[Best Epoch 13],Val 95.9366, Pred 94.4052\n",
      "Epoch 14, train loss 3.6131, train acc 96.78\n",
      "Time: 0.8974,Val loss: 0.5658, Pred loss: 0.0000, Val acc: 96.49, Pred acc: 94.73, ami: 0.7170, f1: 0.96\n",
      "[Best Epoch 14],Val 96.4876, Pred 94.7311\n",
      "Epoch 15, train loss 3.2634, train acc 97.04\n",
      "Time: 0.8772,Val loss: 0.5368, Pred loss: 0.0000, Val acc: 96.76, Pred acc: 94.95, ami: 0.7213, f1: 0.96\n",
      "\n",
      "Epoch 15, train loss 3.2634, train acc 97.04\n",
      "Time: 0.8772,Val loss: 0.5368, Pred loss: 0.0000, Val acc: 96.76, Pred acc: 94.95, ami: 0.7213, f1: 0.96\n",
      "[Best Epoch 15],Val 96.7631, Pred 94.9484\n",
      "Epoch 18, train loss 2.4611, train acc 97.87\n",
      "Time: 1.1073,Val loss: 0.4492, Pred loss: 0.0000, Val acc: 97.73, Pred acc: 95.38, ami: 0.7263, f1: 0.96\n",
      "[Best Epoch 18],Val 97.7273, Pred 95.3829\n",
      "Epoch 19, train loss 2.2667, train acc 98.04\n",
      "Time: 1.2234,Val loss: 0.4243, Pred loss: 0.0000, Val acc: 97.87, Pred acc: 95.76, ami: 0.7305, f1: 0.96\n",
      "[Best Epoch 19],Val 97.8650, Pred 95.7632\n",
      "Epoch 20, train loss 2.0837, train acc 98.35\n",
      "Time: 1.4940,Val loss: 0.4025, Pred loss: 0.0000, Val acc: 98.14, Pred acc: 96.58, ami: 0.7461, f1: 0.97\n",
      "\n",
      "Epoch 20, train loss 2.0837, train acc 98.35\n",
      "Time: 1.4940,Val loss: 0.4025, Pred loss: 0.0000, Val acc: 98.14, Pred acc: 96.58, ami: 0.7461, f1: 0.97\n",
      "[Best Epoch 20],Val 98.1405, Pred 96.5779\n",
      "Epoch 21, train loss 1.9208, train acc 98.43\n",
      "Time: 1.2032,Val loss: 0.3834, Pred loss: 0.0000, Val acc: 98.28, Pred acc: 96.96, ami: 0.7540, f1: 0.97\n",
      "[Best Epoch 21],Val 98.2782, Pred 96.9582\n",
      "Epoch 22, train loss 1.7921, train acc 98.52\n",
      "Time: 1.0607,Val loss: 0.3665, Pred loss: 0.0000, Val acc: 98.21, Pred acc: 97.28, ami: 0.7622, f1: 0.97\n",
      "[Best Epoch 22],Val 98.2094, Pred 97.2841\n",
      "Epoch 23, train loss 1.6484, train acc 98.62\n",
      "Time: 0.9601,Val loss: 0.3514, Pred loss: 0.0000, Val acc: 98.35, Pred acc: 97.83, ami: 0.7768, f1: 0.98\n",
      "[Best Epoch 23],Val 98.3471, Pred 97.8273\n",
      "Epoch 24, train loss 1.5403, train acc 98.66\n",
      "Time: 0.8529,Val loss: 0.3372, Pred loss: 0.0000, Val acc: 98.62, Pred acc: 97.94, ami: 0.7834, f1: 0.98\n",
      "[Best Epoch 24],Val 98.6226, Pred 97.9359\n",
      "Epoch 25, train loss 1.4468, train acc 98.73\n",
      "Time: 0.7932,Val loss: 0.3233, Pred loss: 0.0000, Val acc: 98.62, Pred acc: 98.15, ami: 0.7891, f1: 0.98\n",
      "\n",
      "Epoch 25, train loss 1.4468, train acc 98.73\n",
      "Time: 0.7932,Val loss: 0.3233, Pred loss: 0.0000, Val acc: 98.62, Pred acc: 98.15, ami: 0.7891, f1: 0.98\n",
      "[Best Epoch 25],Val 98.6226, Pred 98.1532\n",
      "Epoch 26, train loss 1.3505, train acc 98.80\n",
      "Time: 0.8540,Val loss: 0.3098, Pred loss: 0.0000, Val acc: 98.69, Pred acc: 98.21, ami: 0.7901, f1: 0.98\n",
      "[Best Epoch 26],Val 98.6915, Pred 98.2075\n",
      "Epoch 28, train loss 1.1861, train acc 98.98\n",
      "Time: 0.8478,Val loss: 0.2859, Pred loss: 0.0000, Val acc: 98.69, Pred acc: 98.37, ami: 0.7903, f1: 0.98\n",
      "[Best Epoch 28],Val 98.6915, Pred 98.3705\n",
      "Epoch 30, train loss 1.0659, train acc 99.09\n",
      "Time: 0.8775,Val loss: 0.2673, Pred loss: 0.0000, Val acc: 98.69, Pred acc: 98.48, ami: 0.7900, f1: 0.98\n",
      "Epoch 31, train loss 0.9983, train acc 99.05\n",
      "Time: 1.0171,Val loss: 0.2587, Pred loss: 0.0000, Val acc: 98.83, Pred acc: 98.48, ami: 0.7909, f1: 0.98\n",
      "[Best Epoch 31],Val 98.8292, Pred 98.4791\n",
      "Epoch 34, train loss 0.8561, train acc 99.21\n",
      "Time: 1.2205,Val loss: 0.2350, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.48, ami: 0.7909, f1: 0.98\n",
      "[Best Epoch 34],Val 98.9669, Pred 98.4791\n",
      "Epoch 35, train loss 0.8085, train acc 99.21\n",
      "Time: 1.2580,Val loss: 0.2276, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.48, ami: 0.7944, f1: 0.98\n",
      "\n",
      "Epoch 35, train loss 0.8085, train acc 99.21\n",
      "Time: 1.2580,Val loss: 0.2276, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.48, ami: 0.7944, f1: 0.98\n",
      "[Best Epoch 35],Val 98.9669, Pred 98.4791\n",
      "Epoch 36, train loss 0.7653, train acc 99.24\n",
      "Time: 0.8021,Val loss: 0.2204, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.53, ami: 0.7979, f1: 0.99\n",
      "[Best Epoch 36],Val 98.9669, Pred 98.5334\n",
      "Epoch 37, train loss 0.7387, train acc 99.23\n",
      "Time: 0.7789,Val loss: 0.2131, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.42, ami: 0.8009, f1: 0.98\n",
      "[Best Epoch 37],Val 98.9669, Pred 98.4248\n",
      "Epoch 40, train loss 0.6386, train acc 99.29\n",
      "Time: 0.7988,Val loss: 0.1922, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.53, ami: 0.8006, f1: 0.99\n",
      "Epoch 41, train loss 0.6101, train acc 99.35\n",
      "Time: 1.2168,Val loss: 0.1875, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.64, ami: 0.8061, f1: 0.99\n",
      "[Best Epoch 41],Val 99.3113, Pred 98.6420\n",
      "Epoch 42, train loss 0.5864, train acc 99.35\n",
      "Time: 1.3918,Val loss: 0.1845, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 98.64, ami: 0.8073, f1: 0.99\n",
      "[Best Epoch 42],Val 99.1047, Pred 98.6420\n",
      "Epoch 45, train loss 0.5206, train acc 99.36\n",
      "Time: 1.4027,Val loss: 0.1737, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 98.48, ami: 0.8023, f1: 0.98\n",
      "Epoch 50, train loss 0.4364, train acc 99.40\n",
      "Time: 0.9295,Val loss: 0.1575, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.48, ami: 0.8057, f1: 0.98\n",
      "Epoch 51, train loss 0.4175, train acc 99.43\n",
      "Time: 1.1695,Val loss: 0.1543, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.53, ami: 0.8075, f1: 0.99\n",
      "[Best Epoch 51],Val 99.3113, Pred 98.5334\n",
      "Epoch 52, train loss 0.4069, train acc 99.43\n",
      "Time: 1.1448,Val loss: 0.1508, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.53, ami: 0.8115, f1: 0.99\n",
      "[Best Epoch 52],Val 99.3113, Pred 98.5334\n",
      "Epoch 53, train loss 0.3935, train acc 99.45\n",
      "Time: 1.2779,Val loss: 0.1469, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.53, ami: 0.8126, f1: 0.99\n",
      "[Best Epoch 53],Val 99.2424, Pred 98.5334\n",
      "Epoch 55, train loss 0.3734, train acc 99.47\n",
      "Time: 1.0169,Val loss: 0.1409, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.48, ami: 0.8089, f1: 0.98\n",
      "Epoch 60, train loss 0.3226, train acc 99.52\n",
      "Time: 1.3833,Val loss: 0.1309, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.48, ami: 0.8095, f1: 0.99\n",
      "Epoch 62, train loss 0.3038, train acc 99.55\n",
      "Time: 1.2139,Val loss: 0.1243, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.32, ami: 0.8134, f1: 0.98\n",
      "[Best Epoch 62],Val 99.1736, Pred 98.3161\n",
      "Epoch 65, train loss 0.2815, train acc 99.59\n",
      "Time: 1.2844,Val loss: 0.1189, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.59, ami: 0.8103, f1: 0.99\n",
      "Epoch 69, train loss 0.2551, train acc 99.69\n",
      "Time: 1.1210,Val loss: 0.1101, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.53, ami: 0.8134, f1: 0.99\n",
      "[Best Epoch 69],Val 99.2424, Pred 98.5334\n",
      "Epoch 70, train loss 0.2524, train acc 99.66\n",
      "Time: 1.1784,Val loss: 0.1086, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.42, ami: 0.8075, f1: 0.98\n",
      "Epoch 75, train loss 0.2264, train acc 99.71\n",
      "Time: 0.8739,Val loss: 0.1023, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.26, ami: 0.8084, f1: 0.98\n",
      "Epoch 77, train loss 0.2179, train acc 99.74\n",
      "Time: 1.0976,Val loss: 0.1047, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.15, ami: 0.8150, f1: 0.98\n",
      "[Best Epoch 77],Val 99.1736, Pred 98.1532\n",
      "Epoch 80, train loss 0.2092, train acc 99.74\n",
      "Time: 1.1369,Val loss: 0.0986, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.04, ami: 0.8080, f1: 0.98\n",
      "Epoch 85, train loss 0.1917, train acc 99.79\n",
      "Time: 1.0163,Val loss: 0.0945, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.37, ami: 0.8129, f1: 0.98\n",
      "Epoch 90, train loss 0.1776, train acc 99.81\n",
      "Time: 1.0604,Val loss: 0.0936, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.21, ami: 0.8144, f1: 0.98\n",
      "Epoch 91, train loss 0.1760, train acc 99.74\n",
      "Time: 1.0447,Val loss: 0.0946, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.21, ami: 0.8166, f1: 0.98\n",
      "[Best Epoch 91],Val 99.1736, Pred 98.2075\n",
      "Epoch 95, train loss 0.1645, train acc 99.72\n",
      "Time: 1.0059,Val loss: 0.0846, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.26, ami: 0.8072, f1: 0.98\n",
      "Epoch 98, train loss 0.1579, train acc 99.76\n",
      "Time: 0.9094,Val loss: 0.0881, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 98.21, ami: 0.8169, f1: 0.98\n",
      "[Best Epoch 98],Val 99.1047, Pred 98.2075\n",
      "Epoch 100, train loss 0.1560, train acc 99.79\n",
      "Time: 1.0598,Val loss: 0.0854, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.37, ami: 0.8117, f1: 0.98\n",
      "Epoch 105, train loss 0.1450, train acc 99.79\n",
      "Time: 1.1870,Val loss: 0.0824, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.15, ami: 0.8099, f1: 0.98\n",
      "Epoch 108, train loss 0.1397, train acc 99.81\n",
      "Time: 1.1082,Val loss: 0.0826, Pred loss: 0.0000, Val acc: 99.04, Pred acc: 98.04, ami: 0.8210, f1: 0.98\n",
      "[Best Epoch 108],Val 99.0358, Pred 98.0445\n",
      "Epoch 109, train loss 0.1373, train acc 99.76\n",
      "Time: 1.1116,Val loss: 0.0820, Pred loss: 0.0000, Val acc: 99.04, Pred acc: 97.66, ami: 0.8278, f1: 0.98\n",
      "[Best Epoch 109],Val 99.0358, Pred 97.6643\n",
      "Epoch 110, train loss 0.1385, train acc 99.69\n",
      "Time: 1.1452,Val loss: 0.0799, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 98.32, ami: 0.8094, f1: 0.98\n",
      "Epoch 115, train loss 0.1422, train acc 99.67\n",
      "Time: 1.0382,Val loss: 0.0744, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.42, ami: 0.8123, f1: 0.98\n",
      "Epoch 120, train loss 0.1341, train acc 99.69\n",
      "Time: 1.0190,Val loss: 0.0705, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.32, ami: 0.8088, f1: 0.98\n",
      "Epoch 125, train loss 0.1232, train acc 99.66\n",
      "Time: 0.6218,Val loss: 0.0640, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.59, ami: 0.8054, f1: 0.99\n",
      "Epoch 130, train loss 0.1157, train acc 99.81\n",
      "Time: 0.7816,Val loss: 0.0645, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.64, ami: 0.8097, f1: 0.99\n",
      "Epoch 135, train loss 0.1103, train acc 99.79\n",
      "Time: 0.8606,Val loss: 0.0604, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.59, ami: 0.8144, f1: 0.99\n",
      "Epoch 140, train loss 0.1029, train acc 99.86\n",
      "Time: 0.7623,Val loss: 0.0627, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.53, ami: 0.8080, f1: 0.99\n",
      "Epoch 145, train loss 0.1006, train acc 99.88\n",
      "Time: 0.9038,Val loss: 0.0666, Pred loss: 0.0000, Val acc: 99.04, Pred acc: 98.37, ami: 0.8120, f1: 0.98\n",
      "Epoch 150, train loss 0.1013, train acc 99.81\n",
      "Time: 0.8388,Val loss: 0.0647, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.32, ami: 0.8047, f1: 0.98\n",
      "Epoch 155, train loss 0.0956, train acc 99.90\n",
      "Time: 0.9396,Val loss: 0.0604, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.53, ami: 0.7963, f1: 0.99\n",
      "Epoch 160, train loss 0.0900, train acc 99.86\n",
      "Time: 0.6600,Val loss: 0.0538, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.59, ami: 0.8010, f1: 0.99\n",
      "Epoch 165, train loss 0.0853, train acc 99.93\n",
      "Time: 0.8660,Val loss: 0.0521, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 98.75, ami: 0.8054, f1: 0.99\n",
      "Epoch 170, train loss 0.0790, train acc 99.97\n",
      "Time: 0.6382,Val loss: 0.0513, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.70, ami: 0.8111, f1: 0.99\n",
      "Epoch 175, train loss 0.0761, train acc 99.93\n",
      "Time: 0.6850,Val loss: 0.0477, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 98.64, ami: 0.8075, f1: 0.99\n",
      "Epoch 180, train loss 0.0702, train acc 99.95\n",
      "Time: 0.9106,Val loss: 0.0464, Pred loss: 0.0000, Val acc: 99.52, Pred acc: 98.48, ami: 0.8085, f1: 0.98\n",
      "Epoch 185, train loss 0.0683, train acc 99.91\n",
      "Time: 1.1058,Val loss: 0.0483, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.59, ami: 0.8078, f1: 0.99\n",
      "Epoch 190, train loss 0.0660, train acc 99.97\n",
      "Time: 1.0953,Val loss: 0.0515, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.64, ami: 0.8049, f1: 0.99\n",
      "Epoch 195, train loss 0.0668, train acc 99.90\n",
      "Time: 0.7294,Val loss: 0.0509, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.42, ami: 0.8068, f1: 0.98\n",
      "Best Epoch 109, Val 99.0358, Pred 97.6643, F1 0.9778, AMI 0.8278\n",
      "Stage 1 history model:\n",
      "\tTrain acc 99.7591 Val acc 99.0358 Pred acc 97.6643\n",
      "Stage: 2, threshold 0.9\n",
      "\t\tpred confident nodes: 1747 / 1841, pred confident_level: 0.9902690052986145\n",
      "Epoch 0, train loss 29.9165, train acc 46.77\n",
      "Time: 1.8582,Val loss: 1.6554, Pred loss: 0.0000, Val acc: 48.62, Pred acc: 74.52, ami: 0.4415, f1: 0.77\n",
      "Epoch 5, train loss 10.5770, train acc 90.93\n",
      "Time: 1.0061,Val loss: 0.7620, Pred loss: 0.0000, Val acc: 91.32, Pred acc: 94.57, ami: 0.7393, f1: 0.95\n",
      "Epoch 10, train loss 6.1398, train acc 95.15\n",
      "Time: 1.0574,Val loss: 0.6278, Pred loss: 0.0000, Val acc: 95.59, Pred acc: 94.95, ami: 0.7339, f1: 0.96\n",
      "Epoch 11, train loss 5.5097, train acc 95.54\n",
      "Time: 1.0886,Val loss: 0.5989, Pred loss: 0.0000, Val acc: 96.01, Pred acc: 95.44, ami: 0.7466, f1: 0.96\n",
      "[Best Epoch 11],Val 96.0055, Pred 95.4373\n",
      "Epoch 12, train loss 4.9701, train acc 95.96\n",
      "Time: 0.9457,Val loss: 0.5742, Pred loss: 0.0000, Val acc: 96.56, Pred acc: 95.49, ami: 0.7490, f1: 0.97\n",
      "[Best Epoch 12],Val 96.5565, Pred 95.4916\n",
      "Epoch 13, train loss 4.5367, train acc 96.21\n",
      "Time: 1.0266,Val loss: 0.5519, Pred loss: 0.0000, Val acc: 96.69, Pred acc: 95.71, ami: 0.7536, f1: 0.97\n",
      "[Best Epoch 13],Val 96.6942, Pred 95.7089\n",
      "Epoch 14, train loss 4.1519, train acc 96.39\n",
      "Time: 1.2383,Val loss: 0.5303, Pred loss: 0.0000, Val acc: 96.97, Pred acc: 95.93, ami: 0.7604, f1: 0.97\n",
      "[Best Epoch 14],Val 96.9697, Pred 95.9261\n",
      "Epoch 15, train loss 3.7581, train acc 96.58\n",
      "Time: 1.1364,Val loss: 0.5078, Pred loss: 0.0000, Val acc: 96.97, Pred acc: 96.14, ami: 0.7632, f1: 0.97\n",
      "\n",
      "Epoch 15, train loss 3.7581, train acc 96.58\n",
      "Time: 1.1364,Val loss: 0.5078, Pred loss: 0.0000, Val acc: 96.97, Pred acc: 96.14, ami: 0.7632, f1: 0.97\n",
      "[Best Epoch 15],Val 96.9697, Pred 96.1434\n",
      "Epoch 16, train loss 3.4555, train acc 96.85\n",
      "Time: 0.8509,Val loss: 0.4836, Pred loss: 0.0000, Val acc: 97.18, Pred acc: 96.47, ami: 0.7650, f1: 0.97\n",
      "[Best Epoch 16],Val 97.1763, Pred 96.4693\n",
      "Epoch 17, train loss 3.1479, train acc 97.08\n",
      "Time: 0.9789,Val loss: 0.4591, Pred loss: 0.0000, Val acc: 97.38, Pred acc: 96.41, ami: 0.7679, f1: 0.97\n",
      "[Best Epoch 17],Val 97.3829, Pred 96.4150\n",
      "Epoch 19, train loss 2.6483, train acc 97.75\n",
      "Time: 1.1483,Val loss: 0.4108, Pred loss: 0.0000, Val acc: 97.59, Pred acc: 96.58, ami: 0.7697, f1: 0.97\n",
      "[Best Epoch 19],Val 97.5895, Pred 96.5779\n",
      "Epoch 20, train loss 2.4467, train acc 97.95\n",
      "Time: 1.1763,Val loss: 0.3876, Pred loss: 0.0000, Val acc: 98.14, Pred acc: 97.12, ami: 0.7809, f1: 0.97\n",
      "\n",
      "Epoch 20, train loss 2.4467, train acc 97.95\n",
      "Time: 1.1763,Val loss: 0.3876, Pred loss: 0.0000, Val acc: 98.14, Pred acc: 97.12, ami: 0.7809, f1: 0.97\n",
      "[Best Epoch 20],Val 98.1405, Pred 97.1211\n",
      "Epoch 21, train loss 2.2942, train acc 98.18\n",
      "Time: 1.0499,Val loss: 0.3654, Pred loss: 0.0000, Val acc: 98.28, Pred acc: 97.12, ami: 0.7829, f1: 0.97\n",
      "[Best Epoch 21],Val 98.2782, Pred 97.1211\n",
      "Epoch 22, train loss 2.0932, train acc 98.37\n",
      "Time: 1.1264,Val loss: 0.3450, Pred loss: 0.0000, Val acc: 98.35, Pred acc: 97.28, ami: 0.7864, f1: 0.97\n",
      "[Best Epoch 22],Val 98.3471, Pred 97.2841\n",
      "Epoch 23, train loss 1.9524, train acc 98.57\n",
      "Time: 1.1490,Val loss: 0.3270, Pred loss: 0.0000, Val acc: 98.62, Pred acc: 97.34, ami: 0.7939, f1: 0.97\n",
      "[Best Epoch 23],Val 98.6226, Pred 97.3384\n",
      "Epoch 24, train loss 1.8164, train acc 98.66\n",
      "Time: 0.8915,Val loss: 0.3118, Pred loss: 0.0000, Val acc: 98.76, Pred acc: 97.34, ami: 0.8000, f1: 0.97\n",
      "[Best Epoch 24],Val 98.7603, Pred 97.3384\n",
      "Epoch 25, train loss 1.6881, train acc 98.73\n",
      "Time: 0.6668,Val loss: 0.2989, Pred loss: 0.0000, Val acc: 98.83, Pred acc: 97.50, ami: 0.8024, f1: 0.98\n",
      "\n",
      "Epoch 25, train loss 1.6881, train acc 98.73\n",
      "Time: 0.6668,Val loss: 0.2989, Pred loss: 0.0000, Val acc: 98.83, Pred acc: 97.50, ami: 0.8024, f1: 0.98\n",
      "[Best Epoch 25],Val 98.8292, Pred 97.5014\n",
      "Epoch 29, train loss 1.3256, train acc 98.98\n",
      "Time: 0.8023,Val loss: 0.2556, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 97.99, ami: 0.8034, f1: 0.98\n",
      "[Best Epoch 29],Val 99.1047, Pred 97.9902\n",
      "Epoch 30, train loss 1.2454, train acc 98.97\n",
      "Time: 0.5077,Val loss: 0.2480, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.04, ami: 0.8041, f1: 0.98\n",
      "\n",
      "Epoch 30, train loss 1.2454, train acc 98.97\n",
      "Time: 0.5077,Val loss: 0.2480, Pred loss: 0.0000, Val acc: 98.97, Pred acc: 98.04, ami: 0.8041, f1: 0.98\n",
      "[Best Epoch 30],Val 98.9669, Pred 98.0445\n",
      "Epoch 33, train loss 1.0610, train acc 99.05\n",
      "Time: 0.8167,Val loss: 0.2281, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 97.72, ami: 0.8045, f1: 0.98\n",
      "[Best Epoch 33],Val 99.1047, Pred 97.7186\n",
      "Epoch 34, train loss 1.0149, train acc 99.02\n",
      "Time: 0.6084,Val loss: 0.2216, Pred loss: 0.0000, Val acc: 99.04, Pred acc: 97.61, ami: 0.8047, f1: 0.98\n",
      "[Best Epoch 34],Val 99.0358, Pred 97.6100\n",
      "Epoch 35, train loss 0.9707, train acc 99.05\n",
      "Time: 0.7804,Val loss: 0.2149, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 97.61, ami: 0.8087, f1: 0.98\n",
      "\n",
      "Epoch 35, train loss 0.9707, train acc 99.05\n",
      "Time: 0.7804,Val loss: 0.2149, Pred loss: 0.0000, Val acc: 99.10, Pred acc: 97.61, ami: 0.8087, f1: 0.98\n",
      "[Best Epoch 35],Val 99.1047, Pred 97.6100\n",
      "Epoch 38, train loss 0.8233, train acc 99.12\n",
      "Time: 0.5249,Val loss: 0.1963, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.77, ami: 0.8090, f1: 0.98\n",
      "[Best Epoch 38],Val 99.1736, Pred 97.7729\n",
      "Epoch 40, train loss 0.7550, train acc 99.23\n",
      "Time: 0.7142,Val loss: 0.1831, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.88, ami: 0.8113, f1: 0.98\n",
      "\n",
      "Epoch 40, train loss 0.7550, train acc 99.23\n",
      "Time: 0.7142,Val loss: 0.1831, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.88, ami: 0.8113, f1: 0.98\n",
      "[Best Epoch 40],Val 99.1736, Pred 97.8816\n",
      "Epoch 43, train loss 0.6621, train acc 99.29\n",
      "Time: 0.6476,Val loss: 0.1669, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 98.04, ami: 0.8114, f1: 0.98\n",
      "[Best Epoch 43],Val 99.3802, Pred 98.0445\n",
      "Epoch 44, train loss 0.6417, train acc 99.31\n",
      "Time: 0.7943,Val loss: 0.1628, Pred loss: 0.0000, Val acc: 99.45, Pred acc: 97.88, ami: 0.8124, f1: 0.98\n",
      "[Best Epoch 44],Val 99.4490, Pred 97.8816\n",
      "Epoch 45, train loss 0.6179, train acc 99.40\n",
      "Time: 0.8019,Val loss: 0.1593, Pred loss: 0.0000, Val acc: 99.45, Pred acc: 97.61, ami: 0.8157, f1: 0.98\n",
      "Epoch 45, train loss 0.6179, train acc 99.40\n",
      "Time: 0.8019,Val loss: 0.1593, Pred loss: 0.0000, Val acc: 99.45, Pred acc: 97.61, ami: 0.8157, f1: 0.98\n",
      "[Best Epoch 45],Val 99.4490, Pred 97.6100\n",
      "Epoch 50, train loss 0.5162, train acc 99.43\n",
      "Time: 0.7718,Val loss: 0.1432, Pred loss: 0.0000, Val acc: 99.45, Pred acc: 97.66, ami: 0.8203, f1: 0.98\n",
      "\n",
      "Epoch 50, train loss 0.5162, train acc 99.43\n",
      "Time: 0.7718,Val loss: 0.1432, Pred loss: 0.0000, Val acc: 99.45, Pred acc: 97.66, ami: 0.8203, f1: 0.98\n",
      "[Best Epoch 50],Val 99.4490, Pred 97.6643\n",
      "Epoch 51, train loss 0.5014, train acc 99.48\n",
      "Time: 1.2111,Val loss: 0.1409, Pred loss: 0.0000, Val acc: 99.52, Pred acc: 97.72, ami: 0.8225, f1: 0.98\n",
      "[Best Epoch 51],Val 99.5179, Pred 97.7186\n",
      "Epoch 55, train loss 0.4472, train acc 99.50\n",
      "Time: 1.1663,Val loss: 0.1332, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.56, ami: 0.8193, f1: 0.98\n",
      "Epoch 60, train loss 0.3850, train acc 99.54\n",
      "Time: 1.0561,Val loss: 0.1257, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.50, ami: 0.8108, f1: 0.98\n",
      "Epoch 65, train loss 0.3455, train acc 99.59\n",
      "Time: 0.9998,Val loss: 0.1143, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 97.66, ami: 0.8209, f1: 0.98\n",
      "Epoch 70, train loss 0.3073, train acc 99.60\n",
      "Time: 0.8292,Val loss: 0.1086, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.61, ami: 0.8157, f1: 0.98\n",
      "Epoch 75, train loss 0.2808, train acc 99.50\n",
      "Time: 0.9723,Val loss: 0.1023, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.28, ami: 0.8276, f1: 0.97\n",
      "Epoch 75, train loss 0.2808, train acc 99.50\n",
      "Time: 0.9723,Val loss: 0.1023, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.28, ami: 0.8276, f1: 0.97\n",
      "[Best Epoch 75],Val 99.1736, Pred 97.2841\n",
      "Epoch 80, train loss 0.2649, train acc 99.60\n",
      "Time: 0.7417,Val loss: 0.0973, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.39, ami: 0.8268, f1: 0.98\n",
      "Epoch 81, train loss 0.2616, train acc 99.59\n",
      "Time: 0.4705,Val loss: 0.0956, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.39, ami: 0.8295, f1: 0.98\n",
      "[Best Epoch 81],Val 99.3802, Pred 97.3927\n",
      "Epoch 85, train loss 0.2426, train acc 99.60\n",
      "Time: 0.6410,Val loss: 0.0964, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.88, ami: 0.8160, f1: 0.98\n",
      "Epoch 90, train loss 0.2246, train acc 99.67\n",
      "Time: 0.6628,Val loss: 0.0886, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 97.34, ami: 0.8272, f1: 0.98\n",
      "Epoch 95, train loss 0.2077, train acc 99.74\n",
      "Time: 0.8224,Val loss: 0.0840, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.83, ami: 0.8124, f1: 0.98\n",
      "Epoch 100, train loss 0.1979, train acc 99.81\n",
      "Time: 0.8807,Val loss: 0.0852, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 97.72, ami: 0.8258, f1: 0.98\n",
      "Epoch 105, train loss 0.1820, train acc 99.85\n",
      "Time: 0.8321,Val loss: 0.0810, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 97.66, ami: 0.8183, f1: 0.98\n",
      "Epoch 110, train loss 0.1790, train acc 99.81\n",
      "Time: 0.9783,Val loss: 0.0750, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 97.39, ami: 0.8221, f1: 0.98\n",
      "Epoch 115, train loss 0.1601, train acc 99.76\n",
      "Time: 0.7224,Val loss: 0.0747, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 97.99, ami: 0.8196, f1: 0.98\n",
      "Epoch 117, train loss 0.1604, train acc 99.69\n",
      "Time: 0.7237,Val loss: 0.0787, Pred loss: 0.0000, Val acc: 99.04, Pred acc: 97.34, ami: 0.8302, f1: 0.98\n",
      "[Best Epoch 117],Val 99.0358, Pred 97.3384\n",
      "Epoch 120, train loss 0.1562, train acc 99.71\n",
      "Time: 0.6487,Val loss: 0.0734, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 97.77, ami: 0.8131, f1: 0.98\n",
      "Epoch 125, train loss 0.1550, train acc 99.72\n",
      "Time: 0.7986,Val loss: 0.0753, Pred loss: 0.0000, Val acc: 99.04, Pred acc: 97.56, ami: 0.8188, f1: 0.98\n",
      "Epoch 130, train loss 0.1504, train acc 99.59\n",
      "Time: 0.7777,Val loss: 0.0640, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 97.50, ami: 0.8202, f1: 0.98\n",
      "Epoch 135, train loss 0.1410, train acc 99.74\n",
      "Time: 1.2286,Val loss: 0.0593, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.39, ami: 0.8241, f1: 0.98\n",
      "Epoch 140, train loss 0.1311, train acc 99.81\n",
      "Time: 0.8354,Val loss: 0.0553, Pred loss: 0.0000, Val acc: 99.52, Pred acc: 97.72, ami: 0.8241, f1: 0.98\n",
      "Epoch 145, train loss 0.1222, train acc 99.83\n",
      "Time: 0.7216,Val loss: 0.0601, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.21, ami: 0.8192, f1: 0.98\n",
      "Epoch 147, train loss 0.1219, train acc 99.83\n",
      "Time: 0.8100,Val loss: 0.0622, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 97.45, ami: 0.8317, f1: 0.98\n",
      "[Best Epoch 147],Val 99.3113, Pred 97.4470\n",
      "Epoch 150, train loss 0.1186, train acc 99.85\n",
      "Time: 0.5964,Val loss: 0.0566, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.45, ami: 0.8235, f1: 0.98\n",
      "Epoch 155, train loss 0.1133, train acc 99.85\n",
      "Time: 1.1042,Val loss: 0.0535, Pred loss: 0.0000, Val acc: 99.45, Pred acc: 97.07, ami: 0.8197, f1: 0.97\n",
      "Epoch 160, train loss 0.1198, train acc 99.83\n",
      "Time: 0.9011,Val loss: 0.0567, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 97.61, ami: 0.8155, f1: 0.98\n",
      "Epoch 165, train loss 0.1102, train acc 99.83\n",
      "Time: 1.0253,Val loss: 0.0563, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 97.50, ami: 0.8203, f1: 0.98\n",
      "Epoch 170, train loss 0.1103, train acc 99.86\n",
      "Time: 1.5805,Val loss: 0.0524, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.94, ami: 0.8106, f1: 0.98\n",
      "Epoch 175, train loss 0.1028, train acc 99.71\n",
      "Time: 1.2226,Val loss: 0.0549, Pred loss: 0.0000, Val acc: 99.24, Pred acc: 98.42, ami: 0.8121, f1: 0.98\n",
      "Epoch 180, train loss 0.1034, train acc 99.81\n",
      "Time: 0.8404,Val loss: 0.0505, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.61, ami: 0.8167, f1: 0.98\n",
      "Epoch 185, train loss 0.0974, train acc 99.76\n",
      "Time: 1.3505,Val loss: 0.0504, Pred loss: 0.0000, Val acc: 99.38, Pred acc: 97.88, ami: 0.8198, f1: 0.98\n",
      "Epoch 190, train loss 0.0950, train acc 99.90\n",
      "Time: 1.0214,Val loss: 0.0502, Pred loss: 0.0000, Val acc: 99.17, Pred acc: 98.15, ami: 0.8157, f1: 0.98\n",
      "Epoch 195, train loss 0.0935, train acc 99.90\n",
      "Time: 0.9681,Val loss: 0.0477, Pred loss: 0.0000, Val acc: 99.31, Pred acc: 98.26, ami: 0.8126, f1: 0.98\n",
      "Best Epoch 147, Val 99.3113, Pred 97.4470, F1 0.9759, AMI 0.8317\n"
     ]
    }
   ],
   "source": [
    "trainer = train1.Trainer(adatas,\n",
    "                    g,\n",
    "                    inter_net,\n",
    "                    list_idx,\n",
    "                    cell_label,\n",
    "                    n_classes,\n",
    "                    key_class,\n",
    "\n",
    "                    threshold=threshold)\n",
    "out = trainer.train(curdir=curdir, \n",
    "            checkpt_file=checkpt_file,\n",
    "            stages=[200,200,200],\n",
    "            nfeats=nfeats,\n",
    "            hidden=hidden,\n",
    "            enhance_gama=enhance_gama,\n",
    "            simi_gama=simi_gama)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "pre_label              alpha      beta     delta    ductal  endothelial  \\\ntrue_label                                                                \nalpha               0.973822  0.010471  0.010471  0.005236          0.0   \nbeta                0.006711  0.986577  0.006711  0.000000          0.0   \ndelta               0.013761  0.004587  0.977064  0.000000          0.0   \nductal              0.000000  0.000000  0.000000  1.000000          0.0   \nendothelial         0.000000  0.000000  0.000000  0.000000          1.0   \ngamma               0.731707  0.000000  0.000000  0.000000          0.0   \nmacrophage          0.000000  0.000000  0.000000  0.000000          0.0   \nquiescent_stellate  0.000000  0.000000  0.000000  0.000000          0.0   \n\npre_label              gamma  macrophage  quiescent_stellate  \ntrue_label                                                    \nalpha               0.000000         0.0                 0.0  \nbeta                0.000000         0.0                 0.0  \ndelta               0.004587         0.0                 0.0  \nductal              0.000000         0.0                 0.0  \nendothelial         0.000000         0.0                 0.0  \ngamma               0.268293         0.0                 0.0  \nmacrophage          0.000000         1.0                 0.0  \nquiescent_stellate  0.000000         0.0                 1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>pre_label</th>\n      <th>alpha</th>\n      <th>beta</th>\n      <th>delta</th>\n      <th>ductal</th>\n      <th>endothelial</th>\n      <th>gamma</th>\n      <th>macrophage</th>\n      <th>quiescent_stellate</th>\n    </tr>\n    <tr>\n      <th>true_label</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alpha</th>\n      <td>0.973822</td>\n      <td>0.010471</td>\n      <td>0.010471</td>\n      <td>0.005236</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>beta</th>\n      <td>0.006711</td>\n      <td>0.986577</td>\n      <td>0.006711</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>delta</th>\n      <td>0.013761</td>\n      <td>0.004587</td>\n      <td>0.977064</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.004587</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ductal</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>endothelial</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>gamma</th>\n      <td>0.731707</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.268293</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>macrophage</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>quiescent_stellate</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.get_confuse_matrix(is_probability=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T05:47:42.029425300Z",
     "start_time": "2024-03-14T05:47:41.955274400Z"
    }
   },
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "schgnn",
   "language": "python",
   "display_name": "schgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
