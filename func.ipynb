{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89639fa-3733-4ed3-95b5-2788c763ff94",
   "metadata": {},
   "source": [
    "```shell\n",
    "conda activate\n",
    "cd ~/link/res_publish\n",
    "\n",
    "jupyter nbconvert func.ipynb --to python\n",
    "\n",
    "jupyter nbconvert func_r_map_seruat.ipynb --to python\n",
    "mv func_r_map_seruat.py func_r_map_seruat.r\n",
    "\n",
    "jupyter nbconvert README.ipynb --to markdown\n",
    "\n",
    "echo 'finish'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37569406-e8b7-4a86-a423-acb3c22bd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/workspace/licanchengup/apps/miniconda3/envs/csMAHN/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from scipy import stats\n",
    "from scipy.io import mmwrite\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from json import loads, dumps\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "import came\n",
    "import csMAHN\n",
    "from csMAHN.utils import utility  # 避免pp 时的循环导入\n",
    "from csMAHN.utils import preprocess as pp\n",
    "from csMAHN.utils.train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bbc54-cd56-4f06-9076-e95e83c42ea6",
   "metadata": {},
   "source": [
    "## SAMap package and a new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b472301d-2db2-4021-98f8-242eda9e9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import samap\n",
    "from samalg import SAM\n",
    "from samap.mapping import SAMAP\n",
    "from samap import analysis as sana\n",
    "import scipy as sp\n",
    "\n",
    "map_sp_SAMap = {'human': 'hu',\n",
    "                'mouse': 'mm',\n",
    "                'zebrafish': 'zf',\n",
    "                'chicken': 'ch',\n",
    "                'macaque': 'ma'}\n",
    "\n",
    "\n",
    "def get_alignment_score_for_each_cell(sm, keys, n_top=0):\n",
    "    \"n_top 无效 但保留\"\n",
    "    def customize_compute_csim(samap, key, X=None, prepend=True, n_top=0):\n",
    "        splabels = sana.q(samap.adata.obs['species'])\n",
    "        skeys = splabels[np.sort(\n",
    "            np.unique(splabels, return_index=True)[1])]\n",
    "\n",
    "        cl = []\n",
    "        clu = []\n",
    "        for sid in skeys:\n",
    "            if prepend:\n",
    "                cl.append(\n",
    "                    sid +\n",
    "                    '_' +\n",
    "                    sana.q(\n",
    "                        samap.adata.obs[key])[\n",
    "                        samap.adata.obs['species'] == sid].astype('str').astype('object'))\n",
    "            else:\n",
    "                cl.append(sana.q(samap.adata.obs[key])[\n",
    "                          samap.adata.obs['species'] == sid])\n",
    "            clu.append(np.unique(cl[-1]))\n",
    "\n",
    "        clu = np.concatenate(clu)\n",
    "        cl = np.concatenate(cl)\n",
    "\n",
    "        CSIM = np.zeros((clu.size, clu.size))\n",
    "        if X is None:\n",
    "            X = samap.adata.obsp[\"connectivities\"].copy()\n",
    "\n",
    "        xi, yi = X.nonzero()\n",
    "        spxi = splabels[xi]\n",
    "        spyi = splabels[yi]\n",
    "\n",
    "        filt = spxi != spyi\n",
    "        di = X.data[filt]\n",
    "        xi = xi[filt]\n",
    "        yi = yi[filt]\n",
    "\n",
    "        px, py = xi, cl[yi]\n",
    "        p = px.astype('str').astype('object')+';'+py.astype('object')\n",
    "\n",
    "        A = pd.DataFrame(data=np.vstack((p, di)).T, columns=[\"x\", \"y\"])\n",
    "        valdict = sana.df_to_dict(A, key_key=\"x\", val_key=\"y\")\n",
    "        cell_scores = [valdict[k].sum() for k in valdict.keys()]\n",
    "        ixer = pd.Series(data=np.arange(clu.size), index=clu)\n",
    "        if len(valdict.keys()) > 0:\n",
    "            xc, yc = sana.substr(list(valdict.keys()), ';')\n",
    "            xc = xc.astype('int')\n",
    "            yc = ixer[yc].values\n",
    "            cell_cluster_scores = sp.sparse.coo_matrix(\n",
    "                (cell_scores, (xc, yc)), shape=(X.shape[0], clu.size)).A\n",
    "            return pd.DataFrame(\n",
    "                cell_cluster_scores,\n",
    "                index=samap.adata.obs.index,\n",
    "                columns=clu)\n",
    "        else:\n",
    "            raise Exception('[Error]')\n",
    "            # return np.zeros((clu.size, clu.size)), clu\n",
    "\n",
    "    if len(list(keys.keys())) < len(list(sm.sams.keys())):\n",
    "        samap = SAM(counts=sm.samap.adata[np.in1d(\n",
    "            sm.samap.adata.obs['species'], list(keys.keys()))])\n",
    "    else:\n",
    "        samap = sm.samap\n",
    "\n",
    "    clusters = []\n",
    "    ix = np.unique(samap.adata.obs['species'], return_index=True)[1]\n",
    "    skeys = sana.q(samap.adata.obs['species'])[np.sort(ix)]\n",
    "\n",
    "    for sid in skeys:\n",
    "        clusters.append(sana.q([sid+'_'+str(x)\n",
    "                        for x in sm.sams[sid].adata.obs[keys[sid]]]))\n",
    "\n",
    "    cl = np.concatenate(clusters)\n",
    "    l = \"{}_mapping_scores\".format(';'.join([keys[sid] for sid in skeys]))\n",
    "    samap.adata.obs[l] = pd.Categorical(cl)\n",
    "\n",
    "    # CSIMth, clu = _compute_csim(samap, l, n_top = n_top, prepend = False)\n",
    "    cell_cluster_scores = customize_compute_csim(\n",
    "        samap, l, n_top=n_top, prepend=False)\n",
    "    return cell_cluster_scores\n",
    "# cell_cluster_scores = get_alignment_score_for_each_cell(sm, keys, n_top = 0)\n",
    "# cell_cluster_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8715a758-314a-4684-aa62-fcc25234efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_link = Path(\"/public/workspace/licanchengup/link\")\n",
    "p_publish = p_link.joinpath(\"res_publish\")\n",
    "p_run = p_publish.joinpath(\"run\")\n",
    "p_plot = p_publish.joinpath(\"plot\")\n",
    "p_res = p_publish.joinpath(\"res\")\n",
    "p_cache = p_run.joinpath(\"cache\")\n",
    "p_df_varmap = p_publish.joinpath('homo/df_varmap.csv')\n",
    "\n",
    "map_sp = {k: v for k, v in zip(\n",
    "    'h,m,z,ma,c'.split(','),\n",
    "    'human,mouse,zebrafish,macaque,chicken'.split(',')\n",
    ")}\n",
    "map_sp_reverse = {v: k for k, v in map_sp.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ed36a-8e3c-4bba-93e7-beca9bc36e97",
   "metadata": {},
   "source": [
    "# F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bd7fef-5e00-4048-9b56-0b1b646f089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算混淆矩阵\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    classes = np.sort(np.union1d(np.unique(y_pred), np.unique(y_true)))\n",
    "    matrix = pd.DataFrame(0, index=classes, columns=classes)\n",
    "    matrix.update(pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred}\n",
    "    ).groupby(['y_true', 'y_pred'])['y_pred'].count().to_frame('count').reset_index().pivot(\n",
    "        index='y_true',\n",
    "        columns='y_pred',\n",
    "        values='count'\n",
    "    ).fillna(0)\n",
    "    )\n",
    "    matrix.index.name = 'true'\n",
    "    matrix.columns.name = 'pred'\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_more_with_confusion_matrix(data):\n",
    "    df = pd.DataFrame({'TP': np.diag(data),\n",
    "                       'FP': data.sum(axis=0) - np.diag(data),\n",
    "                       'FN': data.sum(axis=1) - np.diag(data)})\n",
    "    df['Precision'] = df.eval('TP /(TP + FP)').fillna(0)\n",
    "    df['Recall'] = df.eval('TP /(TP + FN)').fillna(0)\n",
    "    df['F1 Score'] = df.eval(\n",
    "        '2*(Precision * Recall) / (Precision + Recall)').fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_accuracy_with_confusion_matrix(data):\n",
    "    data = data.loc[:, data.columns.isin(data.index)]\n",
    "    return np.diag(data).sum() / np.sum(data.values)\n",
    "\n",
    "\n",
    "def calculate_F1Score_with_confusion_matrix(data, average='weighted'):\n",
    "    \"\"\"\n",
    "    average:\n",
    "        macro (default): 算数均值\n",
    "        weighted : 加权均值，以实际为真值的数量(TP+FN) 为权重\n",
    "        micro: ？？？？Sum statistics over all labels\n",
    "            分别对各类的TP , FP, FN 求和,在计算TP / (1/2(FP + FN))\n",
    "            咦,这个不需要分别计算各类的F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    res = None\n",
    "    if average == 'macro':\n",
    "        res = data['F1 Score'].mean()\n",
    "    elif average == 'weighted':\n",
    "        res = np.average(data['F1 Score'], weights=data.eval('TP+FN'))\n",
    "    elif average == 'micro':\n",
    "        res = data['TP'].sum() / (data['TP'].sum() + 1/2 *\n",
    "                                  (data['FP'].sum() + data['FN'].sum()))\n",
    "    else:\n",
    "        raise Exception('[Error] average= {}'.format(average))\n",
    "    return res\n",
    "\n",
    "\n",
    "def is_1v1(row):\n",
    "    res = None\n",
    "    if row['model'] == 'Seurat':\n",
    "        res = True\n",
    "    elif row['model'] in ['came', 'csMAHN']:\n",
    "        if 'is_1v1=True' in row['resdir_tag']:\n",
    "            res = True\n",
    "        if 'is_1v1=False' in row['resdir_tag']:\n",
    "            res = False\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_significance_marker(p_value, markers={\n",
    "    '**': 0.001,\n",
    "    '**': 0.01,\n",
    "    '*': 0.05\n",
    "}, not_significance_marker='ns'):\n",
    "\n",
    "    res = not_significance_marker\n",
    "    markers = pd.Series(markers)\n",
    "    markers = markers[markers > p_value]\n",
    "    if markers.size > 0:\n",
    "        res = markers[markers == markers.min()].index[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3c604-d2b0-4092-b616-4941c881914f",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2343c896-3c70-48b4-976d-5dde2f31de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefig(fig,fig_name,p_plot=p_plot):\n",
    "    fig.savefig(p_plot.joinpath(fig_name), transparent=True, dpi=200,bbox_inches ='tight')\n",
    "    print('[out][plot] {} \\n\\tin {}'.format(fig_name,p_plot))\n",
    "    \n",
    "def calculate_res_stat(row, q, update=False):\n",
    "    p_res_stats = row['dir'].joinpath('res_stats.josn')\n",
    "    res_stats = {}\n",
    "    if p_res_stats.exists() and (not update):\n",
    "        res_stats = loads(p_res_stats.read_text())\n",
    "    else:\n",
    "        df_obs = get_res_obs(row).query(q)\n",
    "        df_obs.index = df_obs.index.astype(str)\n",
    "        cm = calculate_confusion_matrix(\n",
    "            df_obs['true_label'], df_obs['pre_label'])\n",
    "        acc = calculate_accuracy_with_confusion_matrix(cm)\n",
    "        f1 = calculate_F1Score_with_confusion_matrix(\n",
    "            calculate_more_with_confusion_matrix(cm))\n",
    "        res_stats.update({\n",
    "            q: {'confusion_matrix': cm.to_json(orient='columns'),\n",
    "                'confusion_matrix_more': calculate_more_with_confusion_matrix(cm).to_json(orient='columns'),\n",
    "                'Accuracy': calculate_accuracy_with_confusion_matrix(cm),\n",
    "                'F1-score': calculate_F1Score_with_confusion_matrix(\n",
    "                calculate_more_with_confusion_matrix(cm))\n",
    "                }\n",
    "        })\n",
    "        p_res_stats.write_text(dumps(res_stats))\n",
    "\n",
    "    for k, v in res_stats.items():\n",
    "        res_stats[k]['confusion_matrix'] = pd.read_json(\n",
    "            StringIO(v['confusion_matrix']), orient='columns')\n",
    "        res_stats[k]['confusion_matrix_more'] = pd.read_json(\n",
    "            StringIO(v['confusion_matrix_more']), orient='columns')\n",
    "    return res_stats[q]\n",
    "\n",
    "\n",
    "def get_res_stat(row, q, key, update=False):\n",
    "    res_stat = calculate_res_stat(row, q, update)\n",
    "    return res_stat.setdefault(key, None)\n",
    "\n",
    "\n",
    "def time_tag_detect(p):\n",
    "    return True if re.match('.+;\\\\d{6}-\\\\d{4}$', Path(p).name) else False\n",
    "\n",
    "\n",
    "def time_tag_toggle(p):\n",
    "    p = Path(p)\n",
    "    assert p.exists(), '[not exists]\\n{}'.format(p)\n",
    "    p_res = p\n",
    "    if time_tag_detect(p):\n",
    "        # raise Exception('[time tag has existsed]\\n{}'.format(p.name))\n",
    "        p_res = p.with_name(re.sub(';\\\\d{6}-\\\\d{4}$', '', p_res.name))\n",
    "    else:\n",
    "        p_res = p.with_name(\n",
    "            '{};{}'.format(\n",
    "                p.name,\n",
    "                time.strftime(\n",
    "                    '%y%m%d-%H%M',\n",
    "                    time.localtime())))\n",
    "    assert not p_res.exists(), '[target has existed]\\n{}'.format(p_res)\n",
    "    p.rename(p_res)\n",
    "    return p_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a16c7-7a89-4e4c-bd60-9032a7cf8d31",
   "metadata": {},
   "source": [
    "# other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc671e4-1f9c-4138-b2e8-0a7549d60784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_agg(\n",
    "        obs,\n",
    "        groupby_list,\n",
    "        agg_dict,\n",
    "        dropna=True,\n",
    "        reindex=True,\n",
    "        rename_dict=None):\n",
    "    res = obs.groupby(groupby_list, dropna=dropna).agg(agg_dict)\n",
    "    if reindex:\n",
    "        res.columns = [\"_\".join(i) for i in res.columns]\n",
    "        res = res.index.to_frame().join(res)\n",
    "        res.index = np.arange(res.shape[0])\n",
    "    if isinstance(rename_dict, dict):\n",
    "        res = res.rename(columns=lambda k: rename_dict.setdefault(k, k))\n",
    "    return res\n",
    "\n",
    "\n",
    "def rm_rf(p):\n",
    "    if not p.exists():\n",
    "        return\n",
    "\n",
    "    if p.is_file():\n",
    "        p.unlink()\n",
    "\n",
    "    if p.is_dir():\n",
    "        for i in p.iterdir():\n",
    "            if i.is_file():\n",
    "                i.unlink()\n",
    "            if i.is_dir():\n",
    "                rm_rf(i)  # 递归\n",
    "        p.rmdir()\n",
    "\n",
    "\n",
    "def h5ad_to_mtx(adata, p_dir, prefixes=\"\", as_int=True):\n",
    "    \"\"\"\n",
    "    将adata对象保存为mtx\n",
    "    p_dir ： 输出路径\n",
    "    as_int : 是否将矩阵转换int类型\n",
    "        default True\n",
    "    \"\"\"\n",
    "    assert adata.obs.index.is_unique, '[Error] obs index is not unique'\n",
    "    assert adata.var.index.is_unique, '[Error] var index is not unique'\n",
    "\n",
    "    p_dir = Path(p_dir)\n",
    "    p_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # [out] genes.tsv\n",
    "    adata.var[\"gene_names\"] = adata.var_names.to_numpy()\n",
    "    if \"gene_ids\" not in adata.var_keys():\n",
    "        adata.var[\"gene_ids\"] = adata.var[\"gene_names\"]\n",
    "    df_genes = adata.var.loc[:, [\"gene_ids\", \"gene_names\"]]\n",
    "    df_genes.to_csv(\n",
    "        p_dir.joinpath(\"{}genes.tsv\".format(prefixes)),\n",
    "        header=False,\n",
    "        index=False,\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "\n",
    "    # [out] barcodes.tsv obs.csv\n",
    "    adata.obs.loc[:, []].to_csv(\n",
    "        p_dir.joinpath(\"{}barcodes.tsv\".format(prefixes)),\n",
    "        header=False,\n",
    "        index=True,\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "\n",
    "    if len(adata.obs_keys()) > 0:\n",
    "        adata.obs.to_csv(\n",
    "            p_dir.joinpath(\"{}obs.csv\".format(prefixes)), index=True\n",
    "        )\n",
    "\n",
    "    # [out] matrix.mtx\n",
    "    adata.X = csr_matrix(adata.X)\n",
    "    if as_int:\n",
    "        adata.X = adata.X.astype(int)\n",
    "    nonzero_index = [i[:10] for i in adata.X.nonzero()]\n",
    "    print(\n",
    "        \"frist 10 data.X nonzero elements:\\n\",\n",
    "        adata.X[nonzero_index[0], nonzero_index[1]],\n",
    "    )\n",
    "    mmwrite(\n",
    "        p_dir.joinpath(\"{}matrix.mtx\".format(prefixes)), adata.X.getH()\n",
    "    )\n",
    "    print(\"[out] {}\".format(p_dir))\n",
    "\n",
    "def get_path_varmap(\n",
    "        sp_ref,\n",
    "        sp_que,\n",
    "        p_df_varmap=p_df_varmap,\n",
    "        model='csMAHN'):\n",
    "    \"\"\"通过sp_ref 和 sp_que获取path_varmap\n",
    "    ./homo/df_varmap.csv 存储了\n",
    "    path_varmap路径及信息\n",
    "\"\"\"\n",
    "    if model in 'csMAHN,came'.split(','):\n",
    "        if isinstance(p_df_varmap, str) or isinstance(p_df_varmap, Path):\n",
    "            p_df_varmap = pd.read_csv(p_df_varmap)\n",
    "        index_ = p_df_varmap.query(\"sp_ref == '{}' & sp_que == '{}'\".format(\n",
    "            sp_ref, sp_que)).index\n",
    "        if index_.size == 1:\n",
    "            res = Path(p_df_varmap.loc[index_[0], 'path'])\n",
    "            if res.exists():\n",
    "                return res\n",
    "            else:\n",
    "                raise Exception(\"[not exists] {}\".format(res))\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"[get {} path]can not get speicifed and unique path\\nsp_ref\\tsp_que\\n{}\\t{}\".format(\n",
    "                    index_.size, sp_ref, sp_que))\n",
    "    elif model == 'SAMap':\n",
    "        return Path(p_df_varmap).parent.joinpath('SAMap/maps_gene_name')\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"[Error] can not find path_varmap with model '{}'\".format(model))\n",
    "\n",
    "def get_test_result_df(\n",
    "        p,\n",
    "        extract=\"^(?P<tissue>.+)_(?P<sp_ref>.+)-corss-(?P<sp_que>.+);(?P<model>came|csMAHN|Seurat|SAMap);(?P<name_ref>[\\\\w-]+)-map-(?P<name_que>[[\\\\w-]+);?(?P<resdir_tag>.+)?$\"):\n",
    "    p = Path(p)\n",
    "    df = pd.DataFrame({\"dir\": [i for i in p.iterdir() if i.is_dir()]})\n",
    "    df = df[df[\"dir\"].apply(lambda x: x.joinpath(\"finish\").exists())]\n",
    "    df[\"name\"] = df[\"dir\"].apply(lambda x: x.name)\n",
    "    print('\\n[extract]\\n{}'.format(extract))\n",
    "    df = df.join(\n",
    "        df[\"name\"].str.extract(\n",
    "            extract\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_res_obs(row):\n",
    "    \"\"\"\n",
    "    row 为来自get_test_result_df的行\n",
    "    dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\n",
    "    \"\"\"\n",
    "    def get_res_obs_csMAHN(row):\n",
    "        df = pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"res_2\", \"pre_out_2.csv\"), index_col=0\n",
    "        ).loc[:, \"true_label,pre_label,pre_label_NUM,max_prob\".split(\",\")]\n",
    "\n",
    "        p_am = row[\"dir\"].joinpath(\"adata_meta\")\n",
    "        if not p_am.exists():\n",
    "            print(\"[not exists] {}/{} \".format(row[\"dir\"].name, p_am.name))\n",
    "            # return df\n",
    "        df_obs = pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"adata_meta\", \"obs.csv\"), index_col=0\n",
    "        ).loc[:, [\"dataset\", \"cell_type\"]]\n",
    "\n",
    "        df_umap = pd.read_csv(\n",
    "            p_am.joinpath(\"obsm.csv\"), index_col=None\n",
    "        ).rename(columns={\"X_umap1\": \"UMAP1\", \"X_umap2\": \"UMAP2\"})\n",
    "        df_umap.index = df_obs.index\n",
    "\n",
    "        if not df_obs.index.is_unique:\n",
    "            if df_obs.apply(\n",
    "                    lambda row: \"{};{}\".format(\n",
    "                        row.name,\n",
    "                        row['dataset']),\n",
    "                    axis=1).is_unique:\n",
    "                df_obs.index = df_obs.apply(\n",
    "                    lambda row: \"{};{}\".format(\n",
    "                        row.name, row['dataset']), axis=1).to_numpy()\n",
    "                df.index = df_obs.index\n",
    "                df_umap.index = df_obs.index\n",
    "                print('[index] add dataset')\n",
    "            else:\n",
    "                raise Exception('[index not unique]')\n",
    "        df = df.join(df_obs).join(df_umap)\n",
    "\n",
    "        df[\"is_right\"] = df.eval(\"true_label == pre_label\")\n",
    "        df = df.loc[\n",
    "            :,\n",
    "            np.intersect1d(\n",
    "                \"dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\".split(\n",
    "                    \",\"\n",
    "                ),\n",
    "                df.columns,\n",
    "            ),\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "    def get_res_obs_came(row):\n",
    "        df = pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"obs.csv\"),\n",
    "            index_col=0,\n",
    "            usecols=\"original_name,dataset,REF,celltype,predicted,max_probs,is_right\".split(\n",
    "                \",\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        df.index = df.index.to_numpy()\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"REF\": \"true_label\",\n",
    "                \"predicted\": \"pre_label\",\n",
    "                \"max_probs\": \"max_prob\",\n",
    "            }\n",
    "        ).rename(columns={\"celltype\": \"cell_type\"})\n",
    "\n",
    "        df = df.loc[\n",
    "            :,\n",
    "            np.intersect1d(\n",
    "                \"dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\".split(\n",
    "                    \",\"\n",
    "                ),\n",
    "                df.columns,\n",
    "            ),\n",
    "        ]\n",
    "        p_am = row[\"dir\"].joinpath(\"adata_meta\")\n",
    "        if not p_am.exists():\n",
    "            print(\"[not exists] {}/{} \".format(row[\"dir\"].name, p_am.name))\n",
    "            return df\n",
    "\n",
    "        # get UMAP1,UMAP2\n",
    "        temp_df = pd.read_csv(\n",
    "            p_am.joinpath(\"obsm.csv\"), index_col=None\n",
    "        ).rename(columns={\"X_umap1\": \"UMAP1\", \"X_umap2\": \"UMAP2\"})\n",
    "        assert (\n",
    "            temp_df.shape[0] == df.shape[0]\n",
    "        ), \"[Error][length not equal] {} {}\".format(\n",
    "            temp_df.shape[0], df.shape[0]\n",
    "        )\n",
    "        temp_df.index = df.index\n",
    "        df = df.join(temp_df)\n",
    "\n",
    "        df[\"is_right\"] = df.eval(\"true_label == pre_label\")\n",
    "        df = df.loc[\n",
    "            :,\n",
    "            np.intersect1d(\n",
    "                \"dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\".split(\n",
    "                    \",\"\n",
    "                ),\n",
    "                df.columns,\n",
    "            ),\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "    def get_res_obs_Seurat(row):\n",
    "        return pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"obs.csv\"), index_col=0, low_memory=False\n",
    "        ).loc[\n",
    "            :,\n",
    "            \"UMAP1,UMAP2,cell_type,dataset,is_right,max_prob,pre_label,true_label\".split(\n",
    "                \",\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_res_obs_SAMap(row):\n",
    "        return get_res_obs_Seurat(row)\n",
    "\n",
    "    map_func = {\n",
    "        k: v\n",
    "        for k, v in zip('csMAHN,came,Seurat,SAMap'.split(','),\n",
    "                        [get_res_obs_csMAHN, get_res_obs_came, get_res_obs_Seurat, get_res_obs_SAMap])\n",
    "    }\n",
    "    assert row[\"model\"] in map_func.keys(\n",
    "    ), \"can note get res obs. model = {}\".format(row[\"model\"])\n",
    "\n",
    "    res = map_func[row[\"model\"]](row)\n",
    "    res['dataset_type'] = res['dataset'].map({\n",
    "        '{tissue}_{sp_ref}'.format(**row): 'ref',\n",
    "        '{tissue}_{sp_que}'.format(**row): 'que'\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def show_umap(row):\n",
    "    \"\"\"\n",
    "    row 为来自get_test_result_df的行\n",
    "\n",
    "    show test result umap\n",
    "    row\n",
    "        umap_dataset      :  png path\n",
    "        umap_umap         :  png path\n",
    "        p_cell_type_table :  csv path\n",
    "    \"\"\"\n",
    "    row['umap_dataset'] = Path(\n",
    "        row[\"dir\"]).joinpath(\n",
    "        \"figs\",\n",
    "        \"umap_dataset.png\")\n",
    "    row['umap_umap'] = Path(row[\"dir\"]).joinpath('figs', 'umap_umap.png')\n",
    "    row['p_cell_type_table'] = Path(\n",
    "        row[\"dir\"]).joinpath('group_counts.csv')\n",
    "    print(row['name'].ljust(75, '-'))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    ax[0].imshow(mpimg.imread(row[\"umap_dataset\"]))\n",
    "    ax[0].set_axis_off()\n",
    "    ax[1].imshow(mpimg.imread(row[\"umap_umap\"]))\n",
    "    ax[1].set_axis_off()\n",
    "    display(fig, pd.read_csv(row[\"p_cell_type_table\"], index_col=0))\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb74bc-fd06-4e99-8e30-51eea8f31859",
   "metadata": {},
   "source": [
    "# cross species Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f6eea-0f1e-470e-9828-cdc8e576a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adata(p):\n",
    "    def load_h5ad_from_mtx(p):\n",
    "        p = Path(p)\n",
    "        assert p.joinpath(\"matrix.mtx\").exists(\n",
    "        ), '[not exists]matrix.mtx\\nin{}'.format(p)\n",
    "        adata = sc.read_10x_mtx(p)\n",
    "        if p.joinpath(\"obs.csv\").exists():\n",
    "            adata.obs = pd.read_csv(p.joinpath(\"obs.csv\"), index_col=0)\n",
    "            adata.obs.to_numpy()\n",
    "            # adata.obs = adata.obs.loc[:, []].join(\n",
    "            #     pd.read_csv(p.joinpath(\"obs.csv\"), index_col=0))\n",
    "        else:\n",
    "            print('[not exists]obs.csv\\nin {}'.format(p))\n",
    "        return adata\n",
    "    p = Path(p)\n",
    "    if p.match(\"*.h5ad\"):\n",
    "        return sc.read_h5ad(p)\n",
    "    elif p.is_dir() and p.joinpath(\"matrix.mtx\").exists():\n",
    "        return load_h5ad_from_mtx(p)\n",
    "    else:\n",
    "        raise Exception(\"[can not load adata] {}\".format(p))\n",
    "\n",
    "\n",
    "def load_normalized_adata(p):\n",
    "    p = Path(p)\n",
    "    assert p.is_dir, '[Error] please get a path of dir'\n",
    "    p_h5ad = p.joinpath('normalize.h5ad')\n",
    "    adata = None\n",
    "    if p_h5ad.exists():\n",
    "        adata = sc.read_h5ad(p_h5ad)\n",
    "    else:\n",
    "        adata = load_adata(p)\n",
    "        print('[normalize adata]\\n{}\\n'.format(p_h5ad))\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "        adata.write_h5ad(p_h5ad)\n",
    "    return adata\n",
    "\n",
    "\n",
    "def get_1v1_matches(\n",
    "        df_match,\n",
    "        key_homology_type='homology_type',\n",
    "        value_homology_type='ortholog_one2one'):\n",
    "    \"\"\"\n",
    "    from came.pp.take_1v_matches\n",
    "    \"\"\"\n",
    "    l, r = df_match.columns[:2]\n",
    "    l_unique = df_match[l].value_counts(\n",
    "    ).to_frame().query(\"count == 1\").index\n",
    "    r_unique = df_match[r].value_counts(\n",
    "    ).to_frame().query(\"count == 1\").index\n",
    "    keep = pd.DataFrame({\n",
    "        'l_is_unique': df_match[l].isin(l_unique),\n",
    "        'r_is_unique': df_match[r].isin(r_unique)\n",
    "    }).min(axis=1)\n",
    "    df_match = df_match[keep]\n",
    "    df_match = df_match.query(\n",
    "        \"{} == '{}'\".format(\n",
    "            key_homology_type,\n",
    "            value_homology_type))\n",
    "    return df_match\n",
    "\n",
    "\n",
    "def get_homology_parameters(adata1, adata2, df_varmap):\n",
    "    res = {}\n",
    "    df_homo_paras = pd.DataFrame({\n",
    "        \"gn_ref\": adata1.var_names\n",
    "    }).merge(df_varmap, on='gn_ref', how='left')\n",
    "    res['homology_one2one_find'] = df_homo_paras['gn_que'].notna().sum()\n",
    "\n",
    "    df_homo_paras['gn_que'] = df_homo_paras.apply(\n",
    "        lambda row: 'not_o2o_' + row['gn_ref'] if pd.isna(\n",
    "            row['gn_que']) else row['gn_que'], axis=1)\n",
    "\n",
    "    assert (df_homo_paras['gn_ref'] == adata1.var_names).all(\n",
    "    ), \"df_homo_paras['gn_ref'] not equal adata1.var_names\"\n",
    "    assert df_homo_paras['gn_ref'].is_unique & df_homo_paras['gn_que'].is_unique, \"df_homo_paras gn_ref or gn_que is not unique\"\n",
    "    # came 和 csMAHN不做替换\n",
    "    # adata1.var.index = df_homo_paras['gn_que'].to_numpy()\n",
    "    res['homology_one2one_use'] = np.intersect1d(\n",
    "        df_homo_paras['gn_que'],\n",
    "        adata2.var.index).size\n",
    "    res = {k: int(v) for k, v in res.items()}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_type_counts_info(adatas, key_class, dsnames):\n",
    "    type_counts_list = []\n",
    "    for i in range(len(adatas)):\n",
    "        type_counts_list.append(pd.value_counts(adatas[i].obs[key_class]))\n",
    "    counts_info = pd.concat(type_counts_list, axis=1, keys=dsnames)\n",
    "    return counts_info\n",
    "\n",
    "\n",
    "def aligned_type(adatas, key_calss):\n",
    "    adata1 = adatas[0].copy()\n",
    "    adata2 = adatas[1].copy()\n",
    "    counts_info = get_type_counts_info(\n",
    "        adatas, key_calss, dsnames=[\"reference\", \"query\"]\n",
    "    )\n",
    "    print(\"----raw----\")\n",
    "    print(counts_info)\n",
    "    counts_info = counts_info.dropna(how=\"any\")\n",
    "    print(\"----new----\")\n",
    "    print(counts_info)\n",
    "\n",
    "    com_type = counts_info.index.tolist()\n",
    "    adata1 = adata1[adata1.obs[key_calss].isin(com_type)]\n",
    "    adata2 = adata2[adata2.obs[key_calss].isin(com_type)]\n",
    "    return adata1, adata2\n",
    "\n",
    "\n",
    "def unify_group_counts_index_name(resdir):\n",
    "    \"\"\"\n",
    "    不知为何，group_counts.index.name参差不齐\n",
    "    故将group_counts_unalign.index.name 赋给 group_counts.index.name\n",
    "    \"\"\"\n",
    "    p_group_counts = resdir.joinpath(\"group_counts.csv\")\n",
    "    p_group_counts_unalign = resdir.joinpath(\"group_counts_unalign.csv\")\n",
    "\n",
    "    lines = p_group_counts.read_text().split(\"\\n\")\n",
    "    lines[0] = \",\".join(\n",
    "        [p_group_counts_unalign.read_text().split(\"\\n\")[0].split(\",\")[0]]\n",
    "        + lines[0].split(\",\")[1:]\n",
    "    )\n",
    "    p_group_counts.write_text(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284785d-d2c5-47e4-afa2-dd9637085e2f",
   "metadata": {},
   "source": [
    "## came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498e303-4909-47c4-8e89-8693fae68e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precess_after_came(resdir, tissue_name, sp1, sp2, is_display=False):\n",
    "    unify_group_counts_index_name(resdir)\n",
    "\n",
    "    figdir = resdir / \"figs\"\n",
    "    sc.settings.figdir = figdir\n",
    "\n",
    "    display(\n",
    "        pd.read_csv(resdir / \"group_counts.csv\", index_col=0)\n",
    "    ) if is_display else None\n",
    "    obs = pd.read_csv(resdir / \"obs.csv\", index_col=0)\n",
    "\n",
    "    # umap\n",
    "    # the last layer of hidden states\n",
    "    h_dict = came.load_hidden_states(resdir / \"hidden_list.h5\")[-1]\n",
    "    adt = came.pp.make_adata(\n",
    "        h_dict[\"cell\"], obs=obs, assparse=False, ignore_index=True\n",
    "    )\n",
    "    sc.pp.neighbors(adt, n_neighbors=15, metric=\"cosine\", use_rep=\"X\")\n",
    "    sc.tl.umap(adt)\n",
    "\n",
    "    sc.pl.umap(adt, color=\"dataset\", save=\"_dataset.png\")\n",
    "    sc.pl.umap(adt, color=\"celltype\", save=\"_umap.png\")\n",
    "    adt.write_csvs(resdir.joinpath(\"adata_meta\"))\n",
    "    # # umap.csv umap坐标存储\n",
    "    # pd.DataFrame(\n",
    "    #     adt.obsm[\"X_umap\"],\n",
    "    #     columns=[\"umap_1\", \"umap_2\"],\n",
    "    #     index=sc.get.obs_df(adt, \"original_name\"),\n",
    "    # ).reset_index().to_csv(resdir.joinpath(\"umap.csv\"), index=False)\n",
    "\n",
    "    # test umap.csv\n",
    "    # temp_obs = pd.read_csv(resdir.joinpath(\"obs.csv\"))\n",
    "    # temp_obs = temp_obs.merge(pd.read_csv(resdir.joinpath(\"umap.csv\")),on=\"original_name\")\n",
    "    # temp_adata = sc.AnnData(obs=temp_obs)\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"dataset\")\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"celltype\")\n",
    "    # del temp_adata,temp_obs\n",
    "\n",
    "    # ratio\n",
    "    display(\n",
    "        obs[\"is_right\"].sum() / obs[\"is_right\"].size\n",
    "    ) if is_display else None\n",
    "\n",
    "    obs[\"name\"] = obs[\"original_name\"].str.extract(\";(.+)\", expand=False)\n",
    "    obs[\"sp\"] = obs[\"dataset\"].str.extract(\n",
    "        \"%s_(\\\\w+)\" % tissue_name, expand=False\n",
    "    )\n",
    "    # 物种\n",
    "    df_sp = (\n",
    "        group_agg(obs, [\"sp\"], {\"is_right\": [\"sum\", \"count\"]})\n",
    "        .eval(\"ratio = is_right_sum/is_right_count\")\n",
    "        .sort_values([\"sp\"])\n",
    "    )\n",
    "    df_sp[\"type\"] = \"species\"\n",
    "    # 各个dataset\n",
    "    df_dataset = (\n",
    "        group_agg(obs, [\"sp\", \"name\"], {\"is_right\": [\"sum\", \"count\"]})\n",
    "        .eval(\"ratio = is_right_sum/is_right_count\")\n",
    "        .sort_values([\"sp\", \"name\"])\n",
    "    )\n",
    "    df_dataset[\"type\"] = \"dataset\"\n",
    "    df_ratio = pd.concat([df_sp, df_dataset], axis=0)\n",
    "    df_ratio[\"tissue\"] = tissue_name\n",
    "    df_ratio = df_ratio.loc[\n",
    "        :,\n",
    "        \"tissue,type,sp,name,is_right_sum,is_right_count,ratio\".split(\",\"),\n",
    "    ]\n",
    "    display(df_ratio) if is_display else None\n",
    "    df_ratio.to_csv(resdir / \"ratio.csv\", index=False)\n",
    "    del df_sp, df_dataset, df_ratio\n",
    "\n",
    "    # heatmap\n",
    "    # all,reference,query\n",
    "    for q, sp in zip(\n",
    "        (\n",
    "            \"{col} == {col}\".format(col=obs.columns[0]),\n",
    "            # all, both sp1 and sp2\n",
    "            \"sp == '{sp}'\".format(sp=sp1),\n",
    "            \"sp == '{sp}'\".format(sp=sp2),\n",
    "        ),\n",
    "        (\"all\", sp1, sp2),\n",
    "    ):\n",
    "        res = (\n",
    "            group_agg(\n",
    "                obs.query(q),\n",
    "                [\"celltype\", \"predicted\"],\n",
    "                {\"predicted\": [\"count\"]},\n",
    "            )\n",
    "            .pivot(\n",
    "                index=\"celltype\",\n",
    "                columns=\"predicted\",\n",
    "                values=\"predicted_count\",\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        display(res) if is_display else None\n",
    "        res.to_csv(resdir / f\"predicted_count_{sp}.csv\", index=True)\n",
    "        ax = sns.heatmap(\n",
    "            data=stats.zscore(res, axis=1), cmap=plt.get_cmap(\"Greens\")\n",
    "        )\n",
    "        ax.set_title(f\"{tissue_name}-{sp}\")\n",
    "        ax.figure.savefig(\n",
    "            figdir / f\"heatmap_ratio_{ax.get_title()}.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=120,\n",
    "        )\n",
    "        ax.figure.clear()\n",
    "\n",
    "\n",
    "def run_came(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path(\n",
    "        \"/public/workspace/licanchengup/download/test/test_result\"\n",
    "    ),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2), **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    version:0.0.5\n",
    "    kvargs:\n",
    "        n_epochs: int\n",
    "            default,500,但是为了与csMAHN统一\n",
    "            n_epochs = sum(kvargs.setdefault(\"n_epochs\",[100,200,200]))\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameter settings\n",
    "    # n_epochs = 500\n",
    "    n_epochs = sum(kvargs.setdefault(\"n_epochs\", [100, 200, 200]))\n",
    "    batch_size = None\n",
    "    n_pass = 100\n",
    "    use_scnets = True\n",
    "    ntop_deg = 50\n",
    "    ntop_deg_nodes = 50\n",
    "    node_source = \"deg,hvg\"\n",
    "    # keep_non1v1_feats = True\n",
    "    keep_non1v1_feats = not kvargs.setdefault(\"is_1v1\", False)\n",
    "\n",
    "    # setting directory for results\n",
    "    if len(resdir_tag) > 0:\n",
    "        resdir_tag = f\"{tissue_name}_{sp1}-corss-{sp2};{resdir_tag}\"\n",
    "    else:\n",
    "        resdir_tag = f\"{tissue_name}_{sp1}-corss-{sp2}\"\n",
    "\n",
    "    resdir = resdir / resdir_tag\n",
    "\n",
    "    # 终止 判断\n",
    "    p_finish = resdir.joinpath(\"finish\")\n",
    "    if p_finish.exists():\n",
    "        # precess_after_came(resdir,tissue_name,sp1, sp2)\n",
    "        print(\n",
    "            \"[has finish]{} {}\".format(\n",
    "                time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "                resdir.name)\n",
    "        )\n",
    "        return\n",
    "    print(\n",
    "        \"[start]{} {}\".format(\n",
    "            time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "            resdir.name\n",
    "\n",
    "        ))\n",
    "    # return\n",
    "\n",
    "    figdir = resdir / \"figs\"\n",
    "    sc.settings.figdir = figdir\n",
    "    resdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    finish_content = [\"[strat] %f\" % time.time()]\n",
    "\n",
    "    # # setting\n",
    "    dsnames = (f\"{tissue_name}_{sp1}\", f\"{tissue_name}_{sp2}\")\n",
    "    dsn1, dsn2 = dsnames\n",
    "    homo_method = \"biomart\"\n",
    "\n",
    "    # load data\n",
    "    adata_raw1 = load_adata(path_adata1)\n",
    "    adata_raw2 = load_adata(path_adata2)\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_raw1, adata_raw2 = limite_func(adata_raw1, adata_raw2)\n",
    "\n",
    "    # group_counts_unalign.csv\n",
    "    pd.concat(\n",
    "        [\n",
    "            adata_raw1.obs[key_class1].value_counts(),\n",
    "            adata_raw2.obs[key_class2].value_counts(),\n",
    "        ],\n",
    "        axis=1,\n",
    "        keys=dsnames,\n",
    "    ).to_csv(resdir.joinpath(\"group_counts_unalign.csv\"), index=True)\n",
    "    # align\n",
    "    if aligned:\n",
    "        adata_raw1, adata_raw2 = aligned_type(\n",
    "            [adata_raw1, adata_raw2], key_calss=key_class1\n",
    "        )\n",
    "\n",
    "    # 保存obs ,即真正测试的细胞的mata\n",
    "    adata_raw1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    adata_raw2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat(\n",
    "        [\n",
    "            adata_raw1.obs[key_class1].value_counts(),\n",
    "            adata_raw2.obs[key_class2].value_counts(),\n",
    "        ],\n",
    "        axis=1,\n",
    "        keys=dsnames,\n",
    "    )\n",
    "    # came会自行导出\n",
    "    # temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    # if temp.shape[0] < 2:\n",
    "    #     # 错误标记\n",
    "    #     print(\"[Error][group_counts no any item]\")\n",
    "    #     finish_content.append(\n",
    "    #         \"[Error][group_counts no any item] %f\" % time.time()\n",
    "    #     )\n",
    "    #     p_finish.with_name(\"error\").write_text(\"\\n\".join(finish_content))\n",
    "    #     return\n",
    "\n",
    "    adatas = [adata_raw1, adata_raw2]\n",
    "    print(\"cell count --> %d\" % sum([i.shape[0] for i in adatas]))\n",
    "\n",
    "    df_varmap = pd.read_csv(path_varmap, usecols=range(3))\n",
    "    df_varmap.columns = [\"gn_ref\", \"gn_que\", \"homology_type\"]\n",
    "    if kvargs.setdefault(\"is_1v1\", False):\n",
    "        df_varmap = get_1v1_matches(df_varmap)\n",
    "        homology_parameter = get_homology_parameters(\n",
    "            adata_raw1, adata_raw2, df_varmap)\n",
    "        print(\"\"\"\n",
    "[homology one2one]find {homology_one2one_find} genes\n",
    "[homology one2one]use {homology_one2one_use} genes\"\"\".format(\n",
    "            **homology_parameter))\n",
    "        kvargs.update(homology_parameter)\n",
    "\n",
    "    df_varmap_1v1 = came.pp.take_1v1_matches(df_varmap)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir)})\n",
    "    finish_content.append(\"[finish before run] %f\" % time.time())\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    came_inputs, (adata1, adata2) = came.pipeline.preprocess_unaligned(\n",
    "        adatas,\n",
    "        key_class=key_class1,\n",
    "        use_scnets=use_scnets,\n",
    "        ntop_deg=ntop_deg,\n",
    "        ntop_deg_nodes=ntop_deg_nodes,\n",
    "        node_source=node_source,\n",
    "    )\n",
    "\n",
    "    outputs = came.pipeline.main_for_unaligned(\n",
    "        **came_inputs,\n",
    "        df_varmap=df_varmap,\n",
    "        df_varmap_1v1=df_varmap_1v1,\n",
    "        dataset_names=dsnames,\n",
    "        key_class1=key_class1,\n",
    "        key_class2=key_class2,\n",
    "        do_normalize=True,\n",
    "        keep_non1v1_feats=keep_non1v1_feats,\n",
    "        n_epochs=n_epochs,\n",
    "        resdir=resdir,\n",
    "        n_pass=n_pass,\n",
    "        batch_size=batch_size,\n",
    "        plot_results=True,\n",
    "    )\n",
    "\n",
    "    finish_content.append(\"[finish run] %f\" % time.time())\n",
    "\n",
    "    dpair = outputs[\"dpair\"]\n",
    "    trainer = outputs[\"trainer\"]\n",
    "    h_dict = outputs[\"h_dict\"]\n",
    "    out_cell = outputs[\"out_cell\"]\n",
    "    predictor = outputs[\"predictor\"]\n",
    "\n",
    "    obs_ids1, obs_ids2 = dpair.obs_ids1, dpair.obs_ids2\n",
    "    obs = dpair.obs\n",
    "    classes = predictor.classes\n",
    "    # 后处理\n",
    "    precess_after_came(resdir, tissue_name, sp1, sp2)\n",
    "    finish_content.append(\"[finish after run] %f\" % time.time())\n",
    "\n",
    "    # 完成标记\n",
    "    resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "    finish_content.append(\"[end] %f\" % time.time())\n",
    "    p_finish.write_text(\"\\n\".join(finish_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f2e23-a640-4c52-a78d-ad0aedb4064b",
   "metadata": {},
   "source": [
    "## csMAHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6278e-2f7e-4bd3-8595-0b816ed6d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precess_after_csMAHN(\n",
    "    resdir, tissue_name, sp1, sp2, is_display=False, **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    kvargs:\n",
    "        adt\n",
    "    \"\"\"\n",
    "    unify_group_counts_index_name(resdir)\n",
    "\n",
    "    assert resdir.joinpath(\"res_2\").exists(), \"[not exists] res_2\"\n",
    "    figdir = resdir / \"figs\"\n",
    "    sc.settings.figdir = figdir\n",
    "\n",
    "    display(\n",
    "        pd.read_csv(resdir / \"group_counts.csv\", index_col=0)\n",
    "    ) if is_display else None\n",
    "\n",
    "    # 为obs添加dataset\n",
    "    obs = pd.read_csv(\n",
    "        resdir.joinpath(\"res_2\", \"pre_out_2.csv\"), index_col=0\n",
    "    )\n",
    "    if not obs.index.is_unique:\n",
    "        print(\"[obs index is not unique]\")\n",
    "\n",
    "    pre_obs = {\n",
    "        f\"{tissue_name}_{k}\": pd.read_csv(\n",
    "            resdir.joinpath(file_name), index_col=0\n",
    "        )\n",
    "        for k, file_name in zip([sp1, sp2], [\"obs_ref.csv\", \"obs_que.csv\"])\n",
    "    }\n",
    "    for k, value in pre_obs.items():\n",
    "        value[\"dataset\"] = k\n",
    "        pre_obs[k] = value.loc[:, [\"dataset\"]]\n",
    "    obs = pd.concat(pre_obs.values()).join(obs)\n",
    "    obs[\"is_right\"] = obs[\"true_label\"] == obs[\"pre_label\"]\n",
    "    del pre_obs\n",
    "\n",
    "    # umap\n",
    "    adt = kvargs.setdefault(\"adt\", None)\n",
    "    assert adt is not None\n",
    "    sc.pp.neighbors(adt, n_neighbors=15, metric=\"cosine\", use_rep=\"X\")\n",
    "    sc.tl.umap(adt)\n",
    "    sc.pl.umap(adt, color=\"dataset\", save=\"_dataset.png\")\n",
    "    sc.pl.umap(adt, color=\"cell_type\", save=\"_umap.png\")\n",
    "\n",
    "    adt.write_csvs(resdir.joinpath(\"adata_meta\"))\n",
    "    # # 存储adt.X 和 umap坐标\n",
    "    # pd.DataFrame(adt.X, index=adt.obs.index).to_orc(resdir.joinpath(\"adt.X.orc\"))\n",
    "    # pd.DataFrame(\n",
    "    #     adt.obsm[\"X_umap\"], columns=[\"umap_1\", \"umap_2\"], index=adt.obs.index\n",
    "    # ).assign(**{k: adt.obs[k] for k in adt.obs.keys()}).reset_index().to_csv(\n",
    "    #     resdir.joinpath(\"umap.csv\"), index=False\n",
    "    # )\n",
    "\n",
    "    # # test umap.csv\n",
    "    # temp_obs = pd.read_csv(resdir.joinpath(\"res_2\", \"pre_out_2.csv\"), index_col=0)\n",
    "    # temp_obs = temp_obs.join(pd.read_csv(resdir.joinpath(\"umap.csv\"),index_col=0))\n",
    "    # temp_adata = sc.AnnData(obs=temp_obs)\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"dataset\")\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"cell_type\")\n",
    "    # del temp_adata,temp_obs\n",
    "\n",
    "    # ratio\n",
    "    obs[\"name\"] = obs.index.to_series().str.extract(\n",
    "        \";([^;]+)$\", expand=False\n",
    "    )\n",
    "    obs[\"sp\"] = obs[\"dataset\"].str.extract(\n",
    "        \"%s_(\\\\w+)\" % tissue_name, expand=False\n",
    "    )\n",
    "    display(\n",
    "        obs[\"is_right\"].sum() / obs[\"is_right\"].size\n",
    "    ) if is_display else None\n",
    "    # 物种\n",
    "    df_sp = group_agg(obs, [\"sp\"], {\"is_right\": [\"count\", \"sum\"]})\n",
    "    df_sp[\"type\"] = \"species\"\n",
    "    # 各个dataset\n",
    "    df_dataset = group_agg(\n",
    "        obs, [\"sp\", \"name\"], {\"is_right\": [\"count\", \"sum\"]}\n",
    "    )\n",
    "    df_dataset[\"type\"] = \"dataset\"\n",
    "\n",
    "    df_ratio = pd.concat([df_sp, df_dataset]).eval(\n",
    "        \"ratio = is_right_sum/is_right_count\"\n",
    "    )\n",
    "    df_ratio[\"tissue\"] = tissue_name\n",
    "    df_ratio = df_ratio.loc[\n",
    "        :,\n",
    "        \"tissue,type,sp,name,is_right_sum,is_right_count,ratio\".split(\",\"),\n",
    "    ]\n",
    "    df_ratio.to_csv(resdir / \"ratio.csv\", index=False)\n",
    "    del df_sp, df_dataset, df_ratio\n",
    "\n",
    "    # heatmap\n",
    "    # all,reference,query\n",
    "    for q, sp in zip(\n",
    "        (\n",
    "            \"{col} == {col}\".format(col=obs.columns[0]),\n",
    "            # all, both sp1 and sp2\n",
    "            \"sp == '{sp}'\".format(sp=sp1),\n",
    "            \"sp == '{sp}'\".format(sp=sp2),\n",
    "        ),\n",
    "        (\"all\", sp1, sp2),\n",
    "    ):\n",
    "        res = (\n",
    "            group_agg(\n",
    "                obs.query(q),\n",
    "                [\"true_label\", \"pre_label\"],\n",
    "                {\"pre_label\": [\"count\"]},\n",
    "            )\n",
    "            .pivot(\n",
    "                index=\"true_label\",\n",
    "                columns=\"pre_label\",\n",
    "                values=\"pre_label_count\",\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        display(res) if is_display else None\n",
    "        res.to_csv(resdir / f\"predicted_count_{sp}.csv\", index=True)\n",
    "\n",
    "        ax = sns.heatmap(\n",
    "            data=stats.zscore(res, axis=1), cmap=plt.get_cmap(\"Greens\")\n",
    "        )\n",
    "        ax.set_title(f\"{tissue_name}-{sp}\")\n",
    "        ax.figure.savefig(\n",
    "            figdir / f\"heatmap_ratio_{ax.get_title()}.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=120,\n",
    "        )\n",
    "        ax.figure.clear()\n",
    "\n",
    "\n",
    "def run_csMAHN(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path('.'),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "\n",
    "    **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    version = 0.0.9\n",
    "    kvargs:\n",
    "        n_epochs:\n",
    "            default,[100, 200, 300]\n",
    "            stages,即res_0,res_1，res_2 的 epochs\n",
    "            累加制，res_0,res_1，res_2,实际epochs分别为100,300,600\n",
    "            故最终epochs为stages之和\n",
    "            stages = kvargs.setdefault(\"n_epochs\",[100, 200, 300])\n",
    "\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "    \"\"\"\n",
    "    homo_method = 'biomart'\n",
    "    n_hvgs = 2000\n",
    "    n_degs = 50\n",
    "    seed = 123\n",
    "    stages = kvargs.setdefault(\n",
    "        'n_epochs', [\n",
    "            100, 200, 200])  # [200, 200, 200]\n",
    "    nfeats = 64  # enbedding size #128\n",
    "    hidden = 64  # 128\n",
    "    input_drop = 0.2\n",
    "    att_drop = 0.2\n",
    "    residual = True\n",
    "\n",
    "    threshold = 0.9  # 0.8\n",
    "    lr = 0.01  # lr = 0.01\n",
    "    weight_decay = 0.001\n",
    "    patience = 100\n",
    "    enhance_gama = 10\n",
    "    simi_gama = 0.1\n",
    "\n",
    "    path_specie_1 = path_adata1\n",
    "    path_specie_2 = path_adata2\n",
    "    tissue = tissue_name\n",
    "    species = [sp1, sp2]\n",
    "    dsnames = (f\"{tissue_name}_{sp1}\", f\"{tissue_name}_{sp2}\")\n",
    "    assert key_class1 == key_class2, \"key_class is not equal\"\n",
    "    key_class = key_class1\n",
    "\n",
    "    # make file to save\n",
    "    resdir_tag = f\"{tissue_name}_{sp1}-corss-{sp2};{resdir_tag}\" if len(\n",
    "        resdir_tag) > 0 else f\"{tissue_name}_{sp1}-corss-{sp2}\"\n",
    "    curdir = os.path.join(resdir, resdir_tag)\n",
    "    resdir = Path(curdir)\n",
    "    model_dir = os.path.join(curdir, 'model_')\n",
    "    figdir = os.path.join(curdir, 'figs')\n",
    "    [Path(_).mkdir(exist_ok=True, parents=True)\n",
    "     for _ in [curdir, model_dir, figdir]]\n",
    "    for i in range(len(stages)):\n",
    "        res_dir = os.path.join(curdir, f'res_{i}')\n",
    "        Path(res_dir).mkdir(exist_ok=True, parents=True)\n",
    "    checkpt_file = os.path.join(model_dir, \"mutistages\")\n",
    "    print(checkpt_file)\n",
    "\n",
    "    # is finish\n",
    "    p_finish = Path(resdir).joinpath(\"finish\")\n",
    "    if p_finish.exists():\n",
    "        print(\n",
    "            \"[has finish]{} {}\".format(\n",
    "                time.strftime('%y%m%d-%H%M', time.localtime()), resdir.name\n",
    "            ))\n",
    "        return\n",
    "    print(\n",
    "        \"[start]{} {}\".format(\n",
    "            time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "            resdir.name\n",
    "        ))\n",
    "\n",
    "    finish_content = [\"[strat] %f\" % time.time()]\n",
    "    print('[path_varmap] {}'.format(path_varmap))\n",
    "    adata_species_1 = load_adata(path_specie_1)\n",
    "    adata_species_2 = load_adata(path_specie_2)\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_species_1, adata_species_2 = limite_func(\n",
    "        adata_species_1, adata_species_2\n",
    "    )\n",
    "    # group_counts_unalign.csv\n",
    "    pd.concat([adata_species_1.obs[key_class].value_counts(),\n",
    "               adata_species_2.obs[key_class].value_counts(),],\n",
    "              axis=1, keys=dsnames,).to_csv(\n",
    "        resdir.joinpath(\"group_counts_unalign.csv\"), index=True\n",
    "    )\n",
    "    # 仅保留公共细胞类群\n",
    "    if aligned:\n",
    "        adata_species_1, adata_species_2 = pp.aligned_type(\n",
    "            [adata_species_1, adata_species_2], key_class\n",
    "        )\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat([adata_species_1.obs[key_class].value_counts(),\n",
    "                      adata_species_2.obs[key_class].value_counts(),],\n",
    "                     axis=1, keys=dsnames)\n",
    "    print(temp)\n",
    "    temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    # if temp.shape[0] < 2:\n",
    "    #     # 错误标记\n",
    "    #     print(\"[Error][group_counts no any item]\")\n",
    "    #     finish_content.append(\n",
    "    #         \"[Error][group_counts no any item] %f\" % time.time()\n",
    "    #     )\n",
    "    #     p_finish.with_name(\"error\").write_text(\"\\n\".join(finish_content))\n",
    "    #     return\n",
    "\n",
    "    adata_species_1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    adata_species_2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    # homo = pd.read_csv(path_varmap)\n",
    "    homo = pd.read_csv(path_varmap, usecols=range(3))\n",
    "    homo.columns = [\"gn_ref\", \"gn_que\", \"homology_type\"]\n",
    "    if kvargs.setdefault(\"is_1v1\", False):\n",
    "        homo = get_1v1_matches(homo)\n",
    "        homology_parameter = get_homology_parameters(\n",
    "            adata_species_1, adata_species_2, homo)\n",
    "        print(\"\"\"\n",
    "[homology one2one]find {homology_one2one_find} genes\n",
    "[homology one2one]use {homology_one2one_use} genes\"\"\".format(\n",
    "            **homology_parameter))\n",
    "        kvargs.update(homology_parameter)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir)})\n",
    "\n",
    "    print(\n",
    "        \"\"\"Task: refernece:{} {} cells x {} gene -> query:{} {} cells x {} gene in {}\"\"\".format(\n",
    "            dsnames[0],\n",
    "            adata_species_1.shape[0],\n",
    "            adata_species_1.shape[1],\n",
    "            dsnames[1],\n",
    "            adata_species_2.shape[0],\n",
    "            adata_species_2.shape[1],\n",
    "            tissue))\n",
    "\n",
    "    start = time.time()\n",
    "    finish_content.append(\"[finish before run] %f\" % time.time())\n",
    "    # knn时间较长\n",
    "    print(\"\\n[process_for_graph]\\n\".center(100, '-'))\n",
    "    adatas, features_genes, nodes_genes, scnets, one2one, n2n = pp.process_for_graph(\n",
    "        [adata_species_1, adata_species_2], homo, key_class, 'leiden', n_hvgs=n_hvgs, n_degs=n_degs)\n",
    "    g, inter_net, one2one_gene_nodes_net, cell_label, n_classes, list_idx = pp.make_graph(adatas,\n",
    "                                                                                          aligned,\n",
    "                                                                                          key_class,\n",
    "                                                                                          features_genes,\n",
    "                                                                                          nodes_genes,\n",
    "                                                                                          scnets,\n",
    "                                                                                          one2one,\n",
    "                                                                                          n2n,\n",
    "                                                                                          has_mnn=True,\n",
    "                                                                                          seed=seed)\n",
    "    end = time.time()\n",
    "    # 包括预处理时间\n",
    "    print('Times preprocess for graph:{:.2f}'.format(end - start))\n",
    "    print(\"\\n[Trainer]\\n\".center(100, '-'))\n",
    "    trainer = Trainer(adatas,\n",
    "                      g,\n",
    "                      inter_net,\n",
    "                      list_idx,\n",
    "                      cell_label,\n",
    "                      n_classes,\n",
    "                      threshold=threshold,\n",
    "                      key_class=key_class)\n",
    "    print(\"\\n[train]\\n\".center(100, '-'))\n",
    "    trainer.train(curdir=curdir,\n",
    "                  checkpt_file=checkpt_file,\n",
    "                  nfeats=nfeats,\n",
    "                  hidden=hidden,\n",
    "                  enhance_gama=enhance_gama,\n",
    "                  simi_gama=simi_gama)\n",
    "\n",
    "    finish_content.append(\"[finish run] %f\" % time.time())\n",
    "    adt = sc.AnnData(\n",
    "        trainer.embedding_hidden.detach().numpy(),\n",
    "        obs=pd.concat(\n",
    "            [\n",
    "                adatas[0]\n",
    "                .obs.loc[:, [key_class]]\n",
    "                .assign(dataset=dsnames[0]),\n",
    "                adatas[1]\n",
    "                .obs.loc[:, [key_class]]\n",
    "                .assign(dataset=dsnames[1]),\n",
    "            ]\n",
    "        ).rename(columns={key_class: \"cell_type\"}),\n",
    "    )\n",
    "    # plot_umap(trainer.embedding_hidden, adatas, dsnames, figdir)\n",
    "    precess_after_csMAHN(\n",
    "        resdir, tissue_name, sp1, sp2, is_display=False, adt=adt\n",
    "    )\n",
    "    # 完成标记\n",
    "    resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "    finish_content.append(\"[end] %f\" % time.time())\n",
    "    p_finish.write_text(\"\\n\".join(finish_content))\n",
    "    # return trainer, adatas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9eac9f-ada1-4262-afbe-4193b3dfc8a6",
   "metadata": {},
   "source": [
    "## SAMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f913bc6-a2f2-469b-9c6c-5a48eba92e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SAMap(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path('.'),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "    **kvargs\n",
    "):\n",
    "\n",
    "    path_specie_1 = path_adata1\n",
    "    path_specie_2 = path_adata2\n",
    "    tissue = tissue_name\n",
    "    species = [sp1, sp2]\n",
    "    dsnames = (f\"{tissue_name}_{sp1}\", f\"{tissue_name}_{sp2}\")\n",
    "    assert key_class1 == key_class2, \"key_class is not equal\"\n",
    "    key_class = key_class1\n",
    "\n",
    "    # make file to save\n",
    "    resdir_tag = f\"{tissue_name}_{sp1}-corss-{sp2};{resdir_tag}\" if len(\n",
    "        resdir_tag) > 0 else f\"{tissue_name}_{sp1}-corss-{sp2}\"\n",
    "    curdir = os.path.join(resdir, resdir_tag)\n",
    "    resdir = Path(curdir)\n",
    "    figdir = os.path.join(curdir, 'figs')\n",
    "    [Path(_).mkdir(exist_ok=True, parents=True)\n",
    "     for _ in [curdir, figdir]]\n",
    "\n",
    "    # is finish\n",
    "    p_finish = Path(resdir).joinpath(\"finish\")\n",
    "    if p_finish.exists():\n",
    "        print(\n",
    "            \"[has finish]{} {}\".format(\n",
    "                time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "                resdir.name))\n",
    "        return\n",
    "    print(\n",
    "        \"[start]{} {}\".format(\n",
    "            time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "            resdir.name\n",
    "        ))\n",
    "\n",
    "    finish_content = [\"[strat] %f\" % time.time()]\n",
    "    print('[path_varmap] {}'.format(path_varmap))\n",
    "    adata_1 = load_adata(path_specie_1)\n",
    "    adata_2 = load_adata(path_specie_2)\n",
    "    assert pd.Series(\n",
    "        np.concatenate(\n",
    "            (adata_1.obs.index, adata_2.obs.index))).is_unique, '[Error] index is not unique'\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_1, adata_2 = limite_func(\n",
    "        adata_1, adata_2\n",
    "    )\n",
    "    # group_counts_unalign.csv\n",
    "    pd.concat([adata_1.obs[key_class].value_counts(),\n",
    "               adata_2.obs[key_class].value_counts(),],\n",
    "              axis=1, keys=dsnames,).to_csv(\n",
    "        resdir.joinpath(\"group_counts_unalign.csv\"), index=True\n",
    "    )\n",
    "    # 仅保留公共细胞类群\n",
    "    if aligned:\n",
    "        adata_1, adata_2 = pp.aligned_type(\n",
    "            [adata_1, adata_2], key_class\n",
    "        )\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat([adata_1.obs[key_class].value_counts(),\n",
    "                      adata_2.obs[key_class].value_counts(),],\n",
    "                     axis=1, keys=dsnames)\n",
    "    print(temp)\n",
    "    temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    # if temp.shape[0] < 2:\n",
    "    #     # 错误标记\n",
    "    #     print(\"[Error][group_counts no any item]\")\n",
    "    #     finish_content.append(\n",
    "    #         \"[Error][group_counts no any item] %f\" % time.time()\n",
    "    #     )\n",
    "    #     p_finish.with_name(\"error\").write_text(\"\\n\".join(finish_content))\n",
    "    #     return\n",
    "\n",
    "    adata_1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    adata_2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir)})\n",
    "\n",
    "    start = time.time()\n",
    "    finish_content.append(\"[finish before run] %f\" % time.time())\n",
    "    # SAMap -----------------------------------------------------\n",
    "\n",
    "    sq_ref_SAMap = map_sp_SAMap[map_sp[sp1]]\n",
    "    sq_que_SAMap = map_sp_SAMap[map_sp[sp2]]\n",
    "    _SAMP = {\n",
    "        sq_ref_SAMap: SAM(counts=adata_1),\n",
    "        sq_que_SAMap: SAM(counts=adata_2)\n",
    "    }\n",
    "    [v.preprocess_data() for k, v in _SAMP.items()]\n",
    "    [v.run() for k, v in _SAMP.items()]\n",
    "\n",
    "    keys = {\n",
    "        sq_ref_SAMap: key_class1,\n",
    "        sq_que_SAMap: key_class2\n",
    "\n",
    "    }\n",
    "    sm = SAMAP(\n",
    "        _SAMP,\n",
    "        # f_maps + \"{}{}/{}_to_{}.txt\".format(id2, id1, id2, id1)\n",
    "        # 。。。昂,不能传Path,得传str,还得加个 os.sep\n",
    "        f_maps=str(path_varmap) if str(path_varmap).endswith(\n",
    "            os.sep) else str(path_varmap) + os.sep,\n",
    "        keys=keys\n",
    "\n",
    "    )\n",
    "\n",
    "    # run SAMap\n",
    "    sm.run(pairwise=False)\n",
    "    samap = sm.samap  # SAM object with three species stitched together\n",
    "\n",
    "    finish_content.append(\"[finish run] %f\" % time.time())\n",
    "\n",
    "    alignment_score = samap.adata.obs.loc[:, ['species']].join(\n",
    "        get_alignment_score_for_each_cell(sm, keys))\n",
    "    alignment_score.to_csv(resdir.joinpath(\n",
    "        'alignment_score_for_each_cell.csv'), index=True)\n",
    "    alignment_score.head(2)\n",
    "\n",
    "    alignment_score_query = alignment_score\\\n",
    "        .query(\"species == '{}'\".format(sq_que_SAMap))\\\n",
    "        .filter(regex=\"^{}\".format(sq_ref_SAMap))\n",
    "    # 取每个细胞 在各类型上alignment score 最大值对应的类型\n",
    "    alignment_score_query['pre_label'] = [\n",
    "        alignment_score_query.columns[i] for i in np.argmax(\n",
    "            alignment_score_query, axis=1)]\n",
    "    alignment_score_query['pre_label'] = alignment_score_query['pre_label'].str.replace(\n",
    "        '{}_'.format(sq_ref_SAMap), '').str.replace('{}_'.format(sq_que_SAMap), '')\n",
    "    alignment_score_query['max_prob'] = alignment_score_query.filter(\n",
    "        regex=\"^{}\".format(sq_ref_SAMap)).max(axis=1)\n",
    "    alignment_score_query.head(2)\n",
    "\n",
    "    # constrect the res_obs\n",
    "    res_obs = samap.adata.obs.loc[:, ['species']].copy()\n",
    "    res_obs = res_obs.join(pd.DataFrame(samap.adata.obsm['X_umap'],\n",
    "                                        columns='UMAP1,UMAP2'.split(','),\n",
    "                                        index=samap.adata.obs.index))\n",
    "    res_obs['dataset'] = res_obs['species'].map(\n",
    "        {k: v for k, v in zip([sq_ref_SAMap, sq_que_SAMap], dsnames)})\n",
    "    res_obs['cell_type'] = samap.adata.obs['{};{}_mapping_scores'.format(key_class1, key_class2)].str.replace(\n",
    "        '^{}_'.format(sq_ref_SAMap), '', regex=True\n",
    "    ).str.replace(\n",
    "        '^{}_'.format(sq_que_SAMap), '', regex=True\n",
    "    )\n",
    "    res_obs['true_label'] = res_obs['cell_type']\n",
    "    res_obs = res_obs.drop(columns='species')\n",
    "\n",
    "    res_obs = res_obs.join(\n",
    "        alignment_score_query.loc[:, 'pre_label,max_prob'.split(',')])\n",
    "    res_obs['is_right'] = res_obs.eval('true_label == pre_label')\n",
    "    res_obs.to_csv(resdir.joinpath('obs.csv'), index=True)\n",
    "\n",
    "    # df_ratio\n",
    "    df_ratio = group_agg(res_obs, 'dataset,is_right'.split(','), {\n",
    "        'is_right': ['sum']\n",
    "    }).merge(\n",
    "        res_obs['dataset'].value_counts().to_frame(name='dataset_count'),\n",
    "        on='dataset'\n",
    "    )\n",
    "    df_ratio = df_ratio.query('is_right').drop(\n",
    "        columns='is_right').rename(\n",
    "        columns={\n",
    "            'dataset_count': 'is_right_count'})\n",
    "    df_ratio = df_ratio.join(df_ratio['dataset'].str.extract(\n",
    "        '(?P<tissue>[^_]+)_(?P<sp>[^_]+)'))\n",
    "    assert df_ratio['sp'].is_unique, '[Error] not unique'\n",
    "    df_ratio.index = df_ratio['sp'].to_numpy()\n",
    "    df_ratio.loc[sp1, 'is_right_sum'] = np.nan\n",
    "\n",
    "    df_ratio['ratio'] = df_ratio.eval(\"is_right_sum/is_right_count\")\n",
    "\n",
    "    df_ratio['type'] = 'species'\n",
    "    df_ratio['name'] = ''\n",
    "    df_ratio = df_ratio.loc[:,\n",
    "                            'tissue,type,sp,name,is_right_sum,is_right_count,ratio'.split(',')]\n",
    "    df_ratio.to_csv(resdir.joinpath('ratio.csv'), index=False)\n",
    "\n",
    "    # plot umap\n",
    "    samap.adata.obs['cell_type'] = samap.adata.obs['{};{}_mapping_scores'.format(key_class1, key_class2)].str.replace(\n",
    "        '^{}_'.format(sq_ref_SAMap), '', regex=True).str.replace('^{}_'.format(sq_que_SAMap), '', regex=True)\n",
    "    samap.adata.obs['dataset'] = samap.adata.obs['species'].map(\n",
    "        {k: v for k, v in zip([sq_ref_SAMap, sq_que_SAMap], dsnames)})\n",
    "    display(samap.adata.obs.head(2))\n",
    "\n",
    "    ax = sc.pl.umap(samap.adata, color='dataset', show=False)\n",
    "    ax.figure.savefig(\n",
    "        Path(figdir).joinpath('umap_dataset.png'),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=120)\n",
    "    ax = sc.pl.umap(samap.adata, color='cell_type', show=False)\n",
    "    ax.figure.savefig(\n",
    "        Path(figdir).joinpath('umap_umap.png'),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=120)\n",
    "\n",
    "    # 完成标记\n",
    "    resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "    finish_content.append(\"[end] %f\" % time.time())\n",
    "    p_finish.write_text(\"\\n\".join(finish_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df548d2e-9528-48b1-abaf-98db1d6e1d51",
   "metadata": {},
   "source": [
    "## run_cross_species_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a35cd-0b71-46cd-b19e-08cc5c4cf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_species_models(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    aligned=False,\n",
    "    resdir_tag=\"\",\n",
    "    resdir=Path(\n",
    "        \"/public/workspace/licanchengup/download/test/test_result\"\n",
    "    ),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "    models='came,csMAHN'.split(','),\n",
    "    **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "        kvargs:\n",
    "        n_epochs:\n",
    "            default,[100, 200, 300]\n",
    "            stages,即res_0,res_1，res_2 的 epochs\n",
    "            累加制，res_0,res_1，res_2,实际epochs分别为100,300,600\n",
    "            故最终epochs为stages之和\n",
    "            stages = kvargs.setdefault(\"n_epochs\",[100, 200, 300])\n",
    "\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "\"\"\"\n",
    "    map_func = {\n",
    "        'came': run_came,\n",
    "        'csMAHN': run_csMAHN,\n",
    "        'SAMap': run_SAMap\n",
    "    }\n",
    "    assert pd.Series([model in map_func.keys() for model in models]).all(\n",
    "    ), \"[Error] not all models in {}\\nmodels {}\".format(','.join(map_func.keys()), ','.join(models), )\n",
    "    for model in models:\n",
    "        path_varmap = get_path_varmap(\n",
    "            map_sp[sp1], map_sp[sp2], model=model)\n",
    "        # print('[path_varmap] {}\\t{}'.format(model, Path(path_varmap).name))\n",
    "        map_func[model](\n",
    "            path_adata1,\n",
    "            path_adata2,\n",
    "            key_class1,\n",
    "            key_class2,\n",
    "            sp1,\n",
    "            sp2,\n",
    "            tissue_name,\n",
    "            path_varmap,\n",
    "            aligned=aligned,\n",
    "            resdir_tag=\";\".join([model, resdir_tag]),\n",
    "            resdir=resdir,\n",
    "            limite_func=limite_func,\n",
    "            **kvargs,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csMAHN",
   "language": "python",
   "name": "csmahn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
