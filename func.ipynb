{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89639fa-3733-4ed3-95b5-2788c763ff94",
   "metadata": {},
   "source": [
    "# func\n",
    "\n",
    "最后更新时间2024年4月16日\n",
    "\n",
    "构建在csMHAN的基础上的函数库\n",
    "\n",
    "由`run_cross_species_models`负责SAMap,came,csMAHN的调度\n",
    "\n",
    "`statistics` 计算Accuracy,F1-score\n",
    "\n",
    "`time_tag` 精确到分钟 %y%m%d-%H%M\n",
    "\n",
    "`random_` 随机函数集\n",
    "\n",
    "`plot` 获取color(重现了scanpy的color),绘制umap\n",
    "\n",
    "`pdf2_` 构建pdf的函数集\n",
    "\n",
    "`other functions` group_agg df_apply_merge_field ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```shell\n",
    "conda activate\n",
    "cd ~/link/res_publish\n",
    "\n",
    "jupyter nbconvert func.ipynb --to python\n",
    "\n",
    "jupyter nbconvert func_r_map_seurat.ipynb --to python\n",
    "mv func_r_map_seurat.py func_r_map_seurat.r\n",
    "\n",
    "jupyter nbconvert README.ipynb --to markdown\n",
    "\n",
    "echo 'finish'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37569406-e8b7-4a86-a423-acb3c22bd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/workspace/licanchengup/apps/miniconda3/envs/publish/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-11 14:43:57.042871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from json import loads, dumps\n",
    "from io import StringIO\n",
    "from itertools import zip_longest\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scanpy.plotting.palettes as palettes\n",
    "from scipy import stats\n",
    "from scipy.io import mmwrite\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import PyPDF2\n",
    "import reportlab\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from reportlab.lib.utils import ImageReader\n",
    "from io import BytesIO\n",
    "\n",
    "import came\n",
    "\n",
    "import csMAHN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900f121-6822-4f4f-9eb9-23f844d248e5",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8715a758-314a-4684-aa62-fcc25234efb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# p_root = Path('[you path]')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m p_root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m      3\u001b[0m p_run \u001b[38;5;241m=\u001b[39m p_root\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m p_plot \u001b[38;5;241m=\u001b[39m p_root\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# p_root = Path('[you path]')\n",
    "p_root = Path(__file__).absolute().parent\n",
    "p_run = p_root.joinpath(\"run\")\n",
    "p_plot = p_root.joinpath(\"plot\")\n",
    "p_res = p_root.joinpath(\"res\")\n",
    "p_cache = p_run.joinpath(\"cache\")\n",
    "p_pdf = p_plot.joinpath('pdf')\n",
    "\n",
    "\n",
    "p_df_varmap = p_root.joinpath('homo/df_varmap.csv')\n",
    "assert p_df_varmap.exists(), '[not exists] {}'.format(p_df_varmap)\n",
    "p_maps_SAMap = p_root.joinpath('homo/SAMap/maps_gene_name')\n",
    "assert p_maps_SAMap.exists(), '[not exists] {}'.format(p_maps_SAMap)\n",
    "[_.mkdir(parents=True, exist_ok=True) for _ in [\n",
    "    p_run, p_plot, p_res, p_cache, p_pdf]]\n",
    "\n",
    "map_sp = {k: v for k, v in zip(\n",
    "    'h,m,z,ma,c,x'.split(','),\n",
    "    'human,mouse,zebrafish,macaque,chicken,xenopus'.split(',')\n",
    ")}\n",
    "map_sp_reverse = {v: k for k, v in map_sp.items()}\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bbc54-cd56-4f06-9076-e95e83c42ea6",
   "metadata": {},
   "source": [
    "# SAMap package and new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a8b3c-3c9a-428f-8af0-7acb950a3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import samap\n",
    "from samalg import SAM\n",
    "from samap.mapping import SAMAP\n",
    "from samap import analysis as sana\n",
    "import scipy as sp\n",
    "import typing\n",
    "\n",
    "map_sp_SAMap = {'human': 'hu',\n",
    "                'mouse': 'mm',\n",
    "                'zebrafish': 'zf',\n",
    "                'chicken': 'ch',\n",
    "                'macaque': 'ma',\n",
    "                'xenopus': 'xe'\n",
    "                }\n",
    "\n",
    "\n",
    "def get_alignment_score_for_each_cell(sm, keys, n_top=0):\n",
    "    \"n_top 无效 但保留\"\n",
    "    def customize_compute_csim(samap, key, X=None, prepend=True, n_top=0):\n",
    "        splabels = sana.q(samap.adata.obs['species'])\n",
    "        skeys = splabels[np.sort(\n",
    "            np.unique(splabels, return_index=True)[1])]\n",
    "\n",
    "        cl = []\n",
    "        clu = []\n",
    "        for sid in skeys:\n",
    "            if prepend:\n",
    "                cl.append(\n",
    "                    sid +\n",
    "                    '_' +\n",
    "                    sana.q(\n",
    "                        samap.adata.obs[key])[\n",
    "                        samap.adata.obs['species'] == sid].astype('str').astype('object'))\n",
    "            else:\n",
    "                cl.append(sana.q(samap.adata.obs[key])[\n",
    "                          samap.adata.obs['species'] == sid])\n",
    "            clu.append(np.unique(cl[-1]))\n",
    "\n",
    "        clu = np.concatenate(clu)\n",
    "        cl = np.concatenate(cl)\n",
    "\n",
    "        CSIM = np.zeros((clu.size, clu.size))\n",
    "        if X is None:\n",
    "            X = samap.adata.obsp[\"connectivities\"].copy()\n",
    "\n",
    "        xi, yi = X.nonzero()\n",
    "        spxi = splabels[xi]\n",
    "        spyi = splabels[yi]\n",
    "\n",
    "        filt = spxi != spyi\n",
    "        di = X.data[filt]\n",
    "        xi = xi[filt]\n",
    "        yi = yi[filt]\n",
    "\n",
    "        px, py = xi, cl[yi]\n",
    "        p = px.astype('str').astype('object')+';'+py.astype('object')\n",
    "\n",
    "        A = pd.DataFrame(data=np.vstack((p, di)).T, columns=[\"x\", \"y\"])\n",
    "        valdict = sana.df_to_dict(A, key_key=\"x\", val_key=\"y\")\n",
    "        cell_scores = [valdict[k].sum() for k in valdict.keys()]\n",
    "        ixer = pd.Series(data=np.arange(clu.size), index=clu)\n",
    "        if len(valdict.keys()) > 0:\n",
    "            xc, yc = sana.substr(list(valdict.keys()), ';')\n",
    "            xc = xc.astype('int')\n",
    "            yc = ixer[yc].values\n",
    "            cell_cluster_scores = sp.sparse.coo_matrix(\n",
    "                (cell_scores, (xc, yc)), shape=(X.shape[0], clu.size)).A\n",
    "            return pd.DataFrame(\n",
    "                cell_cluster_scores,\n",
    "                index=samap.adata.obs.index,\n",
    "                columns=clu)\n",
    "        else:\n",
    "            raise Exception('[Error]')\n",
    "            # return np.zeros((clu.size, clu.size)), clu\n",
    "\n",
    "    if len(list(keys.keys())) < len(list(sm.sams.keys())):\n",
    "        samap = SAM(counts=sm.samap.adata[np.in1d(\n",
    "            sm.samap.adata.obs['species'], list(keys.keys()))])\n",
    "    else:\n",
    "        samap = sm.samap\n",
    "\n",
    "    clusters = []\n",
    "    ix = np.unique(samap.adata.obs['species'], return_index=True)[1]\n",
    "    skeys = sana.q(samap.adata.obs['species'])[np.sort(ix)]\n",
    "\n",
    "    for sid in skeys:\n",
    "        clusters.append(sana.q([sid+'_'+str(x)\n",
    "                        for x in sm.sams[sid].adata.obs[keys[sid]]]))\n",
    "\n",
    "    cl = np.concatenate(clusters)\n",
    "    l = \"{}_mapping_scores\".format(';'.join([keys[sid] for sid in skeys]))\n",
    "    samap.adata.obs[l] = pd.Categorical(cl)\n",
    "\n",
    "    # CSIMth, clu = _compute_csim(samap, l, n_top = n_top, prepend = False)\n",
    "    cell_cluster_scores = customize_compute_csim(\n",
    "        samap, l, n_top=n_top, prepend=False)\n",
    "    return cell_cluster_scores\n",
    "# cell_cluster_scores = get_alignment_score_for_each_cell(sm, keys, n_top = 0)\n",
    "# cell_cluster_scores\n",
    "\n",
    "\n",
    "# 重新定义SAMAP 的初始化函数__init__ 和 samap.mapping._calculate_blast_graph ,使得可以进行1v1的运算\n",
    "# gnnm, gns, gns_dict = _calculate_blast_graph(\n",
    "#     ids, f_maps=f_maps, reciprocate=True, eval_thr=eval_thr)\n",
    "# 修改为 [追加参数is_1v1]\n",
    "# gnnm, gns, gns_dict = _calculate_blast_graph(\n",
    "#     ids, f_maps=f_maps, reciprocate=True, eval_thr=eval_thr,\n",
    "#     is_1v1=is_1v1)\n",
    "def customize__init__for_SAMAP(\n",
    "    self,\n",
    "    sams: dict,\n",
    "    f_maps: typing.Optional[str] = \"maps/\",\n",
    "    names: typing.Optional[dict] = None,\n",
    "    keys: typing.Optional[dict] = None,\n",
    "    resolutions: typing.Optional[dict] = None,\n",
    "    gnnm: typing.Optional[tuple] = None,\n",
    "    save_processed: typing.Optional[bool] = True,\n",
    "    eval_thr: typing.Optional[float] = 1e-6,\n",
    "    is_1v1=False\n",
    "):\n",
    "    \"\"\"Initializes and preprocess data structures for SAMap algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sams : dict of string OR SAM\n",
    "        Dictionary of (indexed by species IDs):\n",
    "        The path to an unprocessed '.h5ad' `AnnData` object for organisms.\n",
    "        OR\n",
    "        A processed and already-run SAM object.\n",
    "\n",
    "    f_maps : string, optional, default 'maps/'\n",
    "        Path to the `maps` directory output by `map_genes.sh`.\n",
    "        By default assumes it is in the local directory.\n",
    "\n",
    "    names : dict of list of 2D tuples or Nx2 numpy.ndarray, optional, default None\n",
    "        If BLAST was run on a transcriptome with Fasta headers that do not match\n",
    "        the gene symbols used in the dataset, you can pass a list of tuples mapping\n",
    "        the Fasta header name to the Dataset gene symbol:\n",
    "        (Fasta header name , Dataset gene symbol). Transcripts with the same gene\n",
    "        symbol will be collapsed into a single node in the gene homology graph.\n",
    "        By default, the Fasta header IDs are assumed to be equivalent to the\n",
    "        gene symbols used in the dataset.\n",
    "\n",
    "        The above mapping should be contained in a dicitonary keyed by the corresponding species.\n",
    "        For example, if we have `hu` and `mo` species and the `hu` BLAST results need to be translated,\n",
    "        then `names = {'hu' : mapping}, where `mapping = [(Fasta header 1, Gene symbol 1), ... , (Fasta header n, Gene symbol n)]`.\n",
    "\n",
    "    keys : dict, optional, default None\n",
    "        Dictionary of obs keys indexed by species to use for determining maximum\n",
    "        neighborhood size of each cell.\n",
    "\n",
    "    resolutions : dict, optional, default None\n",
    "        Dictionary of leiden clustering resolutions indexed by species. This parameter is ignored if\n",
    "        `keys` is set.\n",
    "\n",
    "    gnnm : tuple(scipy.sparse.csr_matrix,numpy array, dict[numpy array])\n",
    "        If the homology graph was already computed, you can pass it here in the form of a tuple:\n",
    "        (sparse adjacency matrix, numpy array of genes, dictionary of species-specific genes).\n",
    "        Note that all genes must be prefixed with their species IDs, e.g. `hu_SOX2` instead of `SOX2`.\n",
    "\n",
    "        This is the tuple returned by `_calculate_blast_graph(...)` or `_coarsen_eggnog_graph(...)`.\n",
    "\n",
    "    save_processed : bool, optional, default False\n",
    "        If True saves the processed SAM objects corresponding to each species to an `.h5ad` file.\n",
    "        This argument is unused if preloaded SAM objects are passed in to SAMAP.\n",
    "\n",
    "    eval_thr : float, optional, default 1e-6\n",
    "        E-value threshold above which BLAST results will be filtered out.\n",
    "    \"\"\"\n",
    "    print(\"[a new __init__ for SAMAP]\")\n",
    "    for key, data in zip(sams.keys(), sams.values()):\n",
    "        if not (isinstance(data, str) or isinstance(data, SAM)):\n",
    "            raise TypeError(\n",
    "                f\"Input data {key} must be either a path or a SAM object.\")\n",
    "\n",
    "    ids = list(sams.keys())\n",
    "\n",
    "    if keys is None:\n",
    "        keys = {}\n",
    "        for sid in ids:\n",
    "            keys[sid] = 'leiden_clusters'\n",
    "\n",
    "    if resolutions is None:\n",
    "        resolutions = {}\n",
    "        for sid in ids:\n",
    "            resolutions[sid] = 3\n",
    "\n",
    "    for sid in ids:\n",
    "        data = sams[sid]\n",
    "        key = keys[sid]\n",
    "        res = resolutions[sid]\n",
    "\n",
    "        if isinstance(data, str):\n",
    "            print(\"Processing data {} from:\\n{}\".format(sid, data))\n",
    "            sam = SAM()\n",
    "            sam.load_data(data)\n",
    "            sam.preprocess_data(\n",
    "                sum_norm=\"cell_median\",\n",
    "                norm=\"log\",\n",
    "                thresh_low=0.0,\n",
    "                thresh_high=0.96,\n",
    "                min_expression=1,\n",
    "            )\n",
    "            sam.run(\n",
    "                preprocessing=\"StandardScaler\",\n",
    "                npcs=100,\n",
    "                weight_PCs=False,\n",
    "                k=20,\n",
    "                n_genes=3000,\n",
    "                weight_mode='rms'\n",
    "            )\n",
    "        else:\n",
    "            sam = data\n",
    "\n",
    "        if key == \"leiden_clusters\":\n",
    "            sam.leiden_clustering(res=res)\n",
    "\n",
    "        if \"PCs_SAMap\" not in sam.adata.varm.keys():\n",
    "            samap.mapping.prepare_SAMap_loadings(sam)\n",
    "\n",
    "        if save_processed and isinstance(data, str):\n",
    "            sam.save_anndata(data.split('.h5ad')[0]+'_pr.h5ad')\n",
    "\n",
    "        sams[sid] = sam\n",
    "\n",
    "    if gnnm is None:\n",
    "        gnnm, gns, gns_dict = samap.mapping._calculate_blast_graph(\n",
    "            ids, f_maps=f_maps, reciprocate=True, eval_thr=eval_thr,\n",
    "            is_1v1=is_1v1\n",
    "        )\n",
    "        if names is not None:\n",
    "            gnnm, gns_dict, gns = _coarsen_blast_graph(\n",
    "                gnnm, gns, names\n",
    "            )\n",
    "\n",
    "        gnnm = samap.mapping._filter_gnnm(gnnm, thr=0.25)\n",
    "    else:\n",
    "        gnnm, gns, gns_dict = gnnm\n",
    "\n",
    "    gns_list = []\n",
    "    ges_list = []\n",
    "    for sid in ids:\n",
    "        samap.utils.prepend_var_prefix(sams[sid], sid)\n",
    "        ge = sana.q(sams[sid].adata.var_names)\n",
    "        gn = gns_dict[sid]\n",
    "        gns_list.append(gn[np.in1d(gn, ge)])\n",
    "        ges_list.append(ge)\n",
    "\n",
    "    f = np.in1d(gns, np.concatenate(gns_list))\n",
    "    gns = gns[f]\n",
    "    gnnm = gnnm[f][:, f]\n",
    "    A = pd.DataFrame(data=np.arange(gns.size)[None, :], columns=gns)\n",
    "    ges = np.concatenate(ges_list)\n",
    "    ges = ges[np.in1d(ges, gns)]\n",
    "    ix = A[ges].values.flatten()\n",
    "    gnnm = gnnm[ix][:, ix]\n",
    "    gns = ges\n",
    "\n",
    "    gns_dict = {}\n",
    "    for i, sid in enumerate(ids):\n",
    "        gns_dict[sid] = ges[np.in1d(ges, gns_list[i])]\n",
    "\n",
    "        print(\n",
    "            \"{} `{}` gene symbols match between the datasets and the BLAST graph.\".format(\n",
    "                gns_dict[sid].size, sid))\n",
    "\n",
    "    for sid in sams:\n",
    "        if not sp.sparse.issparse(sams[sid].adata.X):\n",
    "            sams[sid].adata.X = sp.sparse.csr_matrix(sams[sid].adata.X)\n",
    "\n",
    "    smap = samap.mapping._Samap_Iter(sams, gnnm, gns_dict, keys=keys)\n",
    "    self.sams = sams\n",
    "    self.gnnm = gnnm\n",
    "    self.gns_dict = gns_dict\n",
    "    self.gns = gns\n",
    "    self.ids = ids\n",
    "    self.smap = smap\n",
    "\n",
    "\n",
    "def customize_calculate_blast_graph(\n",
    "        ids,\n",
    "        f_maps=\"maps/\",\n",
    "        eval_thr=1e-6,\n",
    "        reciprocate=False,\n",
    "        is_1v1=False):\n",
    "    print('[new_calculate_blast_graph]')\n",
    "\n",
    "    gns = []\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    Vs = []\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        id1 = ids[i]\n",
    "        for j in range(i, len(ids)):\n",
    "            id2 = ids[j]\n",
    "            if i != j:\n",
    "                if os.path.exists(f_maps + \"{}{}\".format(id1, id2)):\n",
    "                    fA = f_maps + \\\n",
    "                        \"{}{}/{}_to_{}.txt.gz\".format(id1, id2, id1, id2)\n",
    "                    fB = f_maps + \\\n",
    "                        \"{}{}/{}_to_{}.txt.gz\".format(id1, id2, id2, id1)\n",
    "                elif os.path.exists(f_maps + \"{}{}\".format(id2, id1)):\n",
    "                    fA = f_maps + \\\n",
    "                        \"{}{}/{}_to_{}.txt.gz\".format(id2, id1, id1, id2)\n",
    "                    fB = f_maps + \\\n",
    "                        \"{}{}/{}_to_{}.txt.gz\".format(id2, id1, id2, id1)\n",
    "                else:\n",
    "                    raise FileExistsError(\n",
    "                        \"BLAST mapping tables with the input IDs ({} and {}) not found in the specified path.\".format(\n",
    "                            id1, id2))\n",
    "                if is_1v1:\n",
    "                    fA = fA.replace('.txt', '_1v1.txt')\n",
    "                    fB = fB.replace('.txt', '_1v1.txt')\n",
    "                print(\n",
    "                    \"{} {}\".format(\n",
    "                        os.path.basename(fA),\n",
    "                        os.path.basename(fB)))\n",
    "                A = pd.read_csv(fA, sep=\"\\t\", header=None, index_col=0)\n",
    "                B = pd.read_csv(fB, sep=\"\\t\", header=None, index_col=0)\n",
    "\n",
    "                A.columns = A.columns.astype(\"<U100\")\n",
    "                B.columns = B.columns.astype(\"<U100\")\n",
    "\n",
    "                A = A[A.index.astype(\"str\") != \"nan\"]\n",
    "                A = A[A.iloc[:, 0].astype(\"str\") != \"nan\"]\n",
    "                B = B[B.index.astype(\"str\") != \"nan\"]\n",
    "                B = B[B.iloc[:, 0].astype(\"str\") != \"nan\"]\n",
    "\n",
    "                A.index = samap.mapping._prepend_blast_prefix(A.index, id1)\n",
    "                B[B.columns[0]] = samap.mapping._prepend_blast_prefix(\n",
    "                    B.iloc[:, 0].values.flatten(), id1)\n",
    "\n",
    "                B.index = samap.mapping._prepend_blast_prefix(B.index, id2)\n",
    "                A[A.columns[0]] = samap.mapping._prepend_blast_prefix(\n",
    "                    A.iloc[:, 0].values.flatten(), id2)\n",
    "\n",
    "                i1 = np.where(A.columns == \"10\")[0][0]\n",
    "                i3 = np.where(A.columns == \"11\")[0][0]\n",
    "\n",
    "                inA = sana.q(A.index)\n",
    "                inB = sana.q(B.index)\n",
    "\n",
    "                inA2 = sana.q(A.iloc[:, 0])\n",
    "                inB2 = sana.q(B.iloc[:, 0])\n",
    "                gn1 = np.unique(np.append(inB2, inA))\n",
    "                gn2 = np.unique(np.append(inA2, inB))\n",
    "                gn = np.append(gn1, gn2)\n",
    "                gnind = pd.DataFrame(\n",
    "                    data=np.arange(\n",
    "                        gn.size)[\n",
    "                        None,\n",
    "                        :],\n",
    "                    columns=gn)\n",
    "\n",
    "                A.index = pd.Index(gnind[A.index].values.flatten())\n",
    "                B.index = pd.Index(gnind[B.index].values.flatten())\n",
    "                A[A.columns[0]] = gnind[A.iloc[:, 0].values.flatten()\n",
    "                                        ].values.flatten()\n",
    "                B[B.columns[0]] = gnind[B.iloc[:, 0].values.flatten()\n",
    "                                        ].values.flatten()\n",
    "\n",
    "                Arows = np.vstack((A.index, A.iloc[:, 0], A.iloc[:, i3])).T\n",
    "                Arows = Arows[A.iloc[:, i1].values.flatten()\n",
    "                              <= eval_thr, :]\n",
    "                gnnm1 = sp.sparse.lil_matrix((gn.size,) * 2)\n",
    "                gnnm1[Arows[:, 0].astype(\"int32\"), Arows[:, 1].astype(\n",
    "                    \"int32\")] = Arows[:, 2]  # -np.log10(Arows[:,2]+1e-200)\n",
    "\n",
    "                Brows = np.vstack((B.index, B.iloc[:, 0], B.iloc[:, i3])).T\n",
    "                Brows = Brows[B.iloc[:, i1].values.flatten()\n",
    "                              <= eval_thr, :]\n",
    "                gnnm2 = sp.sparse.lil_matrix((gn.size,) * 2)\n",
    "                gnnm2[Brows[:, 0].astype(\"int32\"), Brows[:, 1].astype(\n",
    "                    \"int32\")] = Brows[:, 2]  # -np.log10(Brows[:,2]+1e-200)\n",
    "\n",
    "                gnnm = (gnnm1 + gnnm2).tocsr()\n",
    "                gnnms = (gnnm + gnnm.T) / 2\n",
    "                if reciprocate:\n",
    "                    gnnm.data[:] = 1\n",
    "                    gnnms = gnnms.multiply(gnnm).multiply(gnnm.T).tocsr()\n",
    "                gnnm = gnnms\n",
    "\n",
    "                f1 = np.where(np.in1d(gn, gn1))[0]\n",
    "                f2 = np.where(np.in1d(gn, gn2))[0]\n",
    "                f = np.append(f1, f2)\n",
    "                gn = gn[f]\n",
    "                gnnm = gnnm[f, :][:, f]\n",
    "\n",
    "                V = gnnm.data\n",
    "                X, Y = gnnm.nonzero()\n",
    "\n",
    "                Xs.extend(gn[X])\n",
    "                Ys.extend(gn[Y])\n",
    "                Vs.extend(V)\n",
    "                gns.extend(gn)\n",
    "\n",
    "    gns = np.unique(gns)\n",
    "    gns_sp = np.array([x.split('_')[0] for x in gns])\n",
    "    gns2 = []\n",
    "    gns_dict = {}\n",
    "    for sid in ids:\n",
    "        gns2.append(gns[gns_sp == sid])\n",
    "        gns_dict[sid] = gns2[-1]\n",
    "    gns = np.concatenate(gns2)\n",
    "    indexer = pd.Series(index=gns, data=np.arange(gns.size))\n",
    "\n",
    "    X = indexer[Xs].values\n",
    "    Y = indexer[Ys].values\n",
    "    gnnm = sp.sparse.coo_matrix(\n",
    "        (Vs, (X, Y)), shape=(\n",
    "            gns.size, gns.size)).tocsr()\n",
    "\n",
    "    return gnnm, gns, gns_dict\n",
    "\n",
    "\n",
    "SAMAP.__init__ = customize__init__for_SAMAP\n",
    "samap.mapping._calculate_blast_graph = customize_calculate_blast_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3c604-d2b0-4092-b616-4941c881914f",
   "metadata": {},
   "source": [
    "# statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343c896-3c70-48b4-976d-5dde2f31de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score [start]\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算混淆矩阵\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    classes = np.sort(np.union1d(np.unique(y_pred), np.unique(y_true)))\n",
    "    matrix = pd.DataFrame(0, index=classes, columns=classes)\n",
    "    matrix.update(pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred}\n",
    "    ).groupby(['y_true', 'y_pred'])['y_pred'].count().to_frame('count').reset_index().pivot(\n",
    "        index='y_true',\n",
    "        columns='y_pred',\n",
    "        values='count'\n",
    "    ).fillna(0)\n",
    "    )\n",
    "    matrix.index.name = 'true'\n",
    "    matrix.columns.name = 'pred'\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_more_with_confusion_matrix(data):\n",
    "    df = pd.DataFrame({'TP': np.diag(data),\n",
    "                       'FP': data.sum(axis=0) - np.diag(data),\n",
    "                       'FN': data.sum(axis=1) - np.diag(data)})\n",
    "    df['Precision'] = df.eval('TP /(TP + FP)').fillna(0)\n",
    "    df['Recall'] = df.eval('TP /(TP + FN)').fillna(0)\n",
    "    df['F1 Score'] = df.eval(\n",
    "        '2*(Precision * Recall) / (Precision + Recall)').fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_accuracy_with_confusion_matrix(data):\n",
    "    data = data.loc[:, data.columns.isin(data.index)]\n",
    "    return np.diag(data).sum() / np.sum(data.values)\n",
    "\n",
    "\n",
    "def calculate_F1Score_with_confusion_matrix(data, average='weighted'):\n",
    "    \"\"\"\n",
    "    average:\n",
    "        macro (default): 算数均值\n",
    "        weighted : 加权均值，以实际为真值的数量(TP+FN) 为权重\n",
    "        micro: ？？？？Sum statistics over all labels\n",
    "            分别对各类的TP , FP, FN 求和,在计算TP / (1/2(FP + FN))\n",
    "            咦,这个不需要分别计算各类的F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    res = None\n",
    "    if average == 'macro':\n",
    "        res = data['F1 Score'].mean()\n",
    "    elif average == 'weighted':\n",
    "        res = np.average(data['F1 Score'], weights=data.eval('TP+FN'))\n",
    "    elif average == 'micro':\n",
    "        res = data['TP'].sum() / (data['TP'].sum() + 1/2 *\n",
    "                                  (data['FP'].sum() + data['FN'].sum()))\n",
    "    else:\n",
    "        raise Exception('[Error] average= {}'.format(average))\n",
    "    return res\n",
    "\n",
    "\n",
    "def calculate_res_stat(row, q, update=False):\n",
    "    p_res_stats = row['dir'].joinpath('res_stats.json')\n",
    "    res_stats = {}\n",
    "    if p_res_stats.exists() and (not update):\n",
    "        res_stats = loads(p_res_stats.read_text())\n",
    "    else:\n",
    "        df_obs = get_res_obs(row).query(q)\n",
    "        df_obs.index = df_obs.index.astype(str)\n",
    "        cm = calculate_confusion_matrix(\n",
    "            df_obs['true_label'], df_obs['pre_label'])\n",
    "        acc = calculate_accuracy_with_confusion_matrix(cm)\n",
    "        f1 = calculate_F1Score_with_confusion_matrix(\n",
    "            calculate_more_with_confusion_matrix(cm))\n",
    "        res_stats.update({\n",
    "            q: {'confusion_matrix': cm.to_json(orient='columns'),\n",
    "                'confusion_matrix_more': calculate_more_with_confusion_matrix(cm).to_json(orient='columns'),\n",
    "                'Accuracy': calculate_accuracy_with_confusion_matrix(cm),\n",
    "                'F1-score': calculate_F1Score_with_confusion_matrix(\n",
    "                calculate_more_with_confusion_matrix(cm))\n",
    "                }\n",
    "        })\n",
    "        p_res_stats.write_text(dumps(res_stats))\n",
    "\n",
    "    for k, v in res_stats.items():\n",
    "        res_stats[k]['confusion_matrix'] = pd.read_json(\n",
    "            StringIO(v['confusion_matrix']), orient='columns')\n",
    "        res_stats[k]['confusion_matrix_more'] = pd.read_json(\n",
    "            StringIO(v['confusion_matrix_more']), orient='columns')\n",
    "    return res_stats[q]\n",
    "\n",
    "# F1 score [end]\n",
    "\n",
    "\n",
    "def get_res_stat(row, q, key, update=False):\n",
    "    res_stat = calculate_res_stat(row, q, update)\n",
    "    return res_stat.setdefault(key, None)\n",
    "\n",
    "\n",
    "def get_significance_marker(p_value, markers={\n",
    "    '**': 0.001,\n",
    "    '**': 0.01,\n",
    "    '*': 0.05\n",
    "}, not_significance_marker='ns'):\n",
    "\n",
    "    res = not_significance_marker\n",
    "    markers = pd.Series(markers)\n",
    "    markers = markers[markers > p_value]\n",
    "    if markers.size > 0:\n",
    "        res = markers[markers == markers.min()].index[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd0e7f-9f3f-4dff-bf6f-579d7958e6ac",
   "metadata": {},
   "source": [
    "# time_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23d1ca-2c79-4b1c-9ff8-f54f23ef4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tag_get():\n",
    "    return time.strftime('%y%m%d-%H%M', time.localtime())\n",
    "\n",
    "\n",
    "def time_tag_detect(p):\n",
    "    return True if re.match('.+;\\\\d{6}-\\\\d{4}$', Path(p).name) else False\n",
    "\n",
    "\n",
    "def time_tag_toggle(p):\n",
    "    p = Path(p)\n",
    "    assert p.exists(), '[not exists]\\n{}'.format(p)\n",
    "    p_res = p\n",
    "    if time_tag_detect(p):\n",
    "        # raise Exception('[time tag has existsed]\\n{}'.format(p.name))\n",
    "        p_res = p.with_name(re.sub(';\\\\d{6}-\\\\d{4}$', '', p_res.name))\n",
    "    else:\n",
    "        p_res = p.with_name(\n",
    "            '{};{}'.format(\n",
    "                p.name, time_tag_get()))\n",
    "    assert not p_res.exists(), '[target has existed]\\n{}'.format(p_res)\n",
    "    p.rename(p_res)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f14c7-4ce3-4d8c-b096-d02a4d270ecb",
   "metadata": {},
   "source": [
    "# random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdf000-6739-41b5-b99a-e0eb3573131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(arr, size, min_size=5, seed=None, replace=False):\n",
    "    arr = np.array(arr)\n",
    "    _rng = np.random.default_rng(seed)  # 若为None,则相当于没有设置种子\n",
    "    if size < min_size:\n",
    "        print(\n",
    "            '[exchange size] size = {},min_size = {} '.format(\n",
    "                size, min_size))\n",
    "        size = min_size\n",
    "    if arr.size < size:\n",
    "        print(\n",
    "            '[exchange size] size = {},arr.size = {} '.format(\n",
    "                size, min_size))\n",
    "        size = arr.size\n",
    "    return _rng.choice(arr, size, replace=replace)\n",
    "\n",
    "\n",
    "def random_get_seeds(length=1, seeds=None):\n",
    "    \"\"\"\n",
    "core:\n",
    "    random_choice(list('0123456789'),12,seed=i,replace=True)\n",
    "seeds: default is range(length)\n",
    "    \"\"\"\n",
    "    if seeds is None:\n",
    "        seeds = range(length)\n",
    "    else:\n",
    "        seeds = np.unique(seeds)\n",
    "        assert len(\n",
    "            seeds) == length, '[Error] length must be equal ot lenght of seeds'\n",
    "    return np.array(\n",
    "        [''.join(random_choice(list('0123456789'), 12, seed=i, replace=True))\n",
    "         for i in seeds]).astype(int)\n",
    "\n",
    "\n",
    "def random_choice_df_calss(\n",
    "        df,\n",
    "        key_class,\n",
    "        ratio,\n",
    "        min_size=50,\n",
    "        seeds_for_get_seeds=None):\n",
    "    item_index = df.index.to_numpy()\n",
    "    item_class = df[key_class].to_numpy().astype(str)\n",
    "    assert pd.Series(item_index).is_unique, '[Error] not unique'\n",
    "\n",
    "    _class = [np.where(item_class == _) for _ in np.unique(item_class)]\n",
    "    _index = [item_index[_] for _ in _class]\n",
    "\n",
    "    _index_choice = np.concatenate([\n",
    "        random_choice(_arr, int(len(_arr)*ratio),\n",
    "                      min_size=min_size, replace=False,\n",
    "                      seed=_seed)\n",
    "        for (_arr, _seed) in zip(_index, random_get_seeds(\n",
    "            df[key_class].unique().size, seeds_for_get_seeds\n",
    "        ))\n",
    "    ])\n",
    "    return _index_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af180bb-bbde-42ef-945a-cc7c8d7975e4",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2da5f9-948a-4877-b46a-61c7415e4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_colors = {\n",
    "    \"gold2\": \"#eec900\",\n",
    "    \"firebrick3\": \"#cd2626\",\n",
    "    \"khaki2\": \"#eee685\",\n",
    "    \"slategray3\": \"#9fb6cd\",\n",
    "    \"palegreen3\": \"#7ccd7c\",\n",
    "    \"tomato2\": \"#ee5c42\",\n",
    "    \"grey80\": \"#cccccc\",\n",
    "    \"grey90\": \"#e5e5e5\",\n",
    "    \"wheat4\": \"#8b7e66\",\n",
    "    \"grey65\": \"#a6a6a6\",\n",
    "    \"grey10\": \"#1a1a1a\",\n",
    "    \"grey20\": \"#333333\",\n",
    "    \"grey50\": \"#7f7f7f\",\n",
    "    \"grey30\": \"#4d4d4d\",\n",
    "    \"grey40\": \"#666666\",\n",
    "    \"antiquewhite2\": \"#eedfcc\",\n",
    "    \"grey77\": \"#c4c4c4\",\n",
    "    \"snow4\": \"#8b8989\",\n",
    "    \"chartreuse3\": \"#66cd00\",\n",
    "    \"yellow4\": \"#8b8b00\",\n",
    "    \"darkolivegreen2\": \"#bcee68\",\n",
    "    \"olivedrab3\": \"#9acd32\",\n",
    "    \"azure3\": \"#c1cdcd\",\n",
    "    \"violetred\": \"#d02090\",\n",
    "    \"mediumpurple3\": \"#8968cd\",\n",
    "    \"purple4\": \"#551a8b\",\n",
    "    \"seagreen4\": \"#2e8b57\",\n",
    "    \"lightblue3\": \"#9ac0cd\",\n",
    "    \"orchid3\": \"#b452cd\",\n",
    "    \"indianred 3\": \"#cd5555\",\n",
    "    \"grey60\": \"#999999\",\n",
    "    \"mediumorchid1\": \"#e066ff\",\n",
    "    \"plum3\": \"#cd96cd\",\n",
    "    \"palevioletred3\": \"#cd6889\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_color_map(serise, color_missing_value=\"lightgray\",\n",
    "                  offset=0, filter_offset=True):\n",
    "    \"\"\"输入顺序 决定map_color key 的顺序  决定图例顺序\"\"\"\n",
    "    serise = pd.Series(serise)\n",
    "    serise = pd.Series(serise.unique())\n",
    "    has_missing_value = serise.isna().any()\n",
    "    # palettes 是scanpy/scanpy/plotting/palettes\n",
    "    serise = pd.Series(np.concatenate(\n",
    "        (['_{}'.format(i) for i in range(offset)], serise.dropna().astype(str))))\n",
    "    palette = None\n",
    "    if serise.size <= 20:\n",
    "        palette = palettes.default_20\n",
    "    elif serise.size <= 28:\n",
    "        palette = palettes.default_28\n",
    "    elif serise.size <= len(palettes.default_102):  # 103 colors\n",
    "        palette = palettes.default_102\n",
    "    else:\n",
    "        raise Exception(\"[categories too long] {}\".format(serise.size))\n",
    "\n",
    "    # palette是个list 元类为Sequence\n",
    "    # 决定_set_colors_for_categorical_obs的分支\n",
    "    # palette = palette[: categories.size]\n",
    "    # display(palette,type(palette),isinstance(palette, cabc.Sequence))\n",
    "    # additional_colors 替换\n",
    "    for i in range(len(palette)):\n",
    "        if palette[i] in additional_colors.keys():\n",
    "            palette[i] = additional_colors[palette[i]]\n",
    "\n",
    "    # palette 可能比 catgories 长，但当较短元素完成迭代后，zip将结束迭代\n",
    "    map_color = {k: v for k, v in zip(serise, palette)}\n",
    "    if has_missing_value:\n",
    "        map_color.update({'nan': color_missing_value})\n",
    "    # if return_type == \"dict\":\n",
    "    #     return {k: v for k, v in zip(categories, colors_list)}\n",
    "    # else:\n",
    "    # return pd.DataFrame({key: categories, \"{}_color\".format(key):\n",
    "    # colors_list})\n",
    "    if filter_offset:\n",
    "        map_color = {\n",
    "            k: v\n",
    "            for _, (k, v) in zip(\n",
    "                ~pd.Series(map_color.keys()).str.match('_\\\\d+'),\n",
    "                map_color.items())\n",
    "            if _\n",
    "        }\n",
    "    return map_color\n",
    "\n",
    "\n",
    "def get_color(i, size=10):\n",
    "    return list(\n",
    "        get_color_map(\n",
    "            [],\n",
    "            offset=max(\n",
    "                i+1,\n",
    "                size),\n",
    "            filter_offset=False).values())[i]\n",
    "\n",
    "\n",
    "def show_color_map(color_map, marker='.', size=40,\n",
    "                   fontdict=None,\n",
    "                   ax=None, return_fig=False):\n",
    "    if ax:\n",
    "        fig = ax.figure\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(\n",
    "            1, 0.5*len(color_map.keys())))\n",
    "    if isinstance(marker, str):\n",
    "        marker = np.repeat(marker, len(color_map.keys()))\n",
    "    for i, ((k, v), m) in enumerate(zip(color_map.items(), marker)):\n",
    "        ax.scatter(0, len(color_map.keys())-i,\n",
    "                   label=k, c=v, s=size, marker=m)\n",
    "        ax.text(0.1, len(color_map.keys())-i - 0.08, k, fontdict=fontdict)\n",
    "    ax.set_xlim(-0.1, 0.25)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    # else:\n",
    "    #     display(fig)\n",
    "    #     fig.clear()\n",
    "\n",
    "\n",
    "def show_color(i):\n",
    "    show_color_map(get_color_map([], offset=i+1, filter_offset=False))\n",
    "\n",
    "\n",
    "def savefig(fig, fig_name, p_plot=p_plot):\n",
    "    fig.savefig(\n",
    "        p_plot.joinpath(fig_name),\n",
    "        transparent=True,\n",
    "        dpi=200,\n",
    "        bbox_inches='tight')\n",
    "    print('[out][plot] {} \\n\\tin {}'.format(fig_name, p_plot))\n",
    "\n",
    "\n",
    "def plot_umap(\n",
    "        adata,\n",
    "        key_color,\n",
    "        color_map=None,\n",
    "        size=5,\n",
    "        marker='.',\n",
    "        ax=None,\n",
    "        show_legend=False,\n",
    "        save_file_name='',\n",
    "        p_plot=p_plot,\n",
    "        **kvarg_scatter):\n",
    "\n",
    "    if color_map is None:\n",
    "        color_map = get_color_map(np.sort(adata.obs[key_color].unique()))\n",
    "    assert adata.obs[key_color].isin(\n",
    "        color_map.keys()).all(), \"[Error] not color\"\n",
    "\n",
    "    # color_map filter\n",
    "    color_map = {\n",
    "        k: v\n",
    "        for k, v in color_map.items()\n",
    "        if k in adata.obs[key_color].unique()\n",
    "    }\n",
    "\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    [\n",
    "        ax.scatter(\n",
    "            adata[adata.obs[key_color] == label].obsm['X_umap'][:, 0],\n",
    "            adata[adata.obs[key_color] == label].obsm['X_umap'][:, 1],\n",
    "            label=label,\n",
    "            s=size,\n",
    "            marker=marker,\n",
    "            c=color_map[label], **kvarg_scatter\n",
    "        )\n",
    "\n",
    "        for label in color_map.keys()\n",
    "    ]\n",
    "    ax.set_axis_off()\n",
    "    # legend\n",
    "    if show_legend:\n",
    "        ax.legend()\n",
    "        ax.get_legend().set(\n",
    "            # 固定于左下角 （0,0）\n",
    "            loc='lower left',\n",
    "            bbox_to_anchor=(0, 0),\n",
    "            frame_on=False\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if ax.legend():\n",
    "            ax.legend().set_visible(False)\n",
    "    # save\n",
    "    if save_file_name:\n",
    "        savefig(\n",
    "            ax.figure, save_file_name, p_plot=p_plot)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# colors_article\n",
    "# https://mp.weixin.qq.com/s/1NtTeTS1N8G3gDnMnaGnjw\n",
    "colors_article = {\n",
    "    '2': [\n",
    "        '#40DAFF,#FF5c5c'.split(','), '#6262FF,#FF6060'.split(','),\n",
    "        '#D59B3A,#3D4A78'.split(','), '#1A908C,#D17133'.split(','),\n",
    "        '#387DB8,#E11A1D'.split(','), '#179B73,#D48AAF'.split(','),\n",
    "        '#FFDD14,#AC592A'.split(','), '#C381A8,#407BAE'.split(',')],\n",
    "    '3': [\n",
    "        '#FB8D62,#8DA0CD,#66C2A5'.split(','),\n",
    "        '#2DABB2,#DAAB36,#F0552B'.split(','),\n",
    "        '#CCD6BC,#EBC4B8,#CACDE8'.split(',')],\n",
    "    '4': [\n",
    "        '#F5AD65,#91CCAE,#795291,#F6C6D6'.split(','),\n",
    "        '#DB80AE,#8C96B8,#EC8360,#54B097'.split(',')\n",
    "    ],\n",
    "    '5': [\n",
    "        '#A5D3ED,#ED949A,#EEC48A,#B5AAD5,#5382BA'.split(','),\n",
    "        '#6194C9,#FE8D00,#0E5FDB,#970030,#681A98'.split(','),\n",
    "        '#E64B35,#4DBBD5,#00A087,#3C5488,#F39B7F'.split(',')]\n",
    "}\n",
    "\n",
    "\n",
    "def get_colors_article(\n",
    "        keys=[],\n",
    "        index=1,\n",
    "        as_dict=True,\n",
    "        colors_article=colors_article):\n",
    "\n",
    "    keys = list(pd.Series(keys).unique())\n",
    "    res = colors_article[str(len(keys))][index]\n",
    "    if as_dict:\n",
    "        if not keys:\n",
    "            keys = ['_{}'.format(i) for i in range(len(res))]\n",
    "        res = {k: color\n",
    "               for k, color in zip(keys, res)\n",
    "               }\n",
    "    return res\n",
    "\n",
    "\n",
    "def show_colors_article(length, colors_article=colors_article, **kvargs):\n",
    "    nrows, ncols = 1, len(colors_article[str(length)])\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, figsize=(\n",
    "            4*ncols, 4*nrows))\n",
    "    axs = np.ravel(axs)\n",
    "    for i, (ax) in enumerate(axs):\n",
    "        show_color_map(\n",
    "            get_colors_article(\n",
    "                range(length),\n",
    "                i),\n",
    "            ax=ax,\n",
    "            **kvargs)\n",
    "        ax.set_title('index = {}'.format(i), loc='left')\n",
    "        ax.margins(2, 2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b06cc-c8ff-475e-b8fb-fdc8c1a2e405",
   "metadata": {},
   "source": [
    "# pdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c04ecc-5c6a-4273-972a-ca286b398fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2_save(pdf_writer, file_name, p_dir=p_pdf):\n",
    "    with Path(p_dir).joinpath(file_name).open('wb') as output:\n",
    "        pdf_writer.write(output)\n",
    "    print('[out][pdf] {}'.format(file_name))\n",
    "\n",
    "\n",
    "def pdf2_get_page(p, page=0):\n",
    "    return PyPDF2.PdfFileReader(\n",
    "        Path(p).open(\n",
    "            mode='rb'),\n",
    "        strict=False).getPage(page)\n",
    "\n",
    "\n",
    "def pdf2_get_allpages(p):\n",
    "    pdf_reader = PyPDF2.PdfFileReader(\n",
    "        Path(p).open(mode='rb'), strict=False)\n",
    "    return [pdf_reader.getPage(page)\n",
    "            for page in range(pdf_reader.getNumPages())]\n",
    "\n",
    "\n",
    "def pdf2_get_im(file_path, wight=None, hight=None, keep_hw_raio=True):\n",
    "    if isinstance(file_path, ImageReader):\n",
    "        im = file_path\n",
    "    else:\n",
    "        im = ImageReader(file_path)\n",
    "\n",
    "    w, h = im.getSize()\n",
    "    if keep_hw_raio:\n",
    "        if wight and (not hight):\n",
    "            hight = wight/w*h\n",
    "        if (not wight) and hight:\n",
    "            wight = hight/h*w\n",
    "        assert wight and hight, '[Error] one of hight and wight must be specified\\n hight = {}, wight={}'.format(\n",
    "            hight, wight)\n",
    "    else:\n",
    "        assert hight and wight, '[Error] hight and wight must be specified\\n hight = {}, wight={}'.format(\n",
    "            hight, wight)\n",
    "    return im, wight, hight, w, h\n",
    "\n",
    "\n",
    "def pdf2_canvas_add_formatted_text(\n",
    "    can, x, y, text, psfontname='arial', fontsize=14\n",
    "):\n",
    "    text_obj = can.beginText()\n",
    "    # text_obj.setTextOrigin(5*inch, 11*inch)\n",
    "    text_obj.setFont('arial', size=14)\n",
    "    text_obj.setTextOrigin(x, y)\n",
    "    text_obj.textLines(text)\n",
    "    can.drawText(text_obj)\n",
    "    print('[canvas_add_formatted_text][add] {}'.format(len(text)))\n",
    "\n",
    "\n",
    "def pdf2_merge_with_pages(pages, file_name, p_dir=p_pdf):\n",
    "    pdf_writer = PyPDF2.PdfFileWriter()\n",
    "    for page in pages:\n",
    "        pdf_writer.addPage(page)\n",
    "    pdf2_save(pdf_writer, file_name, p_pdf)\n",
    "\n",
    "def pdf2_merge(file_paths, out_file_name, p_dir=p_pdf):\n",
    "    pages = []\n",
    "    for i in file_paths:\n",
    "        pages = pages + pdf2_get_allpages(i)\n",
    "    pdf2_merge_with_pages(pages, out_file_name, p_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a16c7-7a89-4e4c-bd60-9032a7cf8d31",
   "metadata": {},
   "source": [
    "# other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc671e4-1f9c-4138-b2e8-0a7549d60784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_agg(\n",
    "        obs,\n",
    "        groupby_list,\n",
    "        agg_dict=None,\n",
    "        dropna=True,\n",
    "        reindex=True,\n",
    "        rename_dict=None):\n",
    "    if None is agg_dict:\n",
    "        agg_dict = {groupby_list[-1]: ['count']}\n",
    "    res = obs.groupby(\n",
    "        groupby_list,\n",
    "        dropna=dropna,\n",
    "        observed=False).agg(agg_dict)\n",
    "    if reindex:\n",
    "        res.columns = [\"_\".join(i) for i in res.columns]\n",
    "        res = res.index.to_frame().join(res)\n",
    "        res.index = np.arange(res.shape[0])\n",
    "    if isinstance(rename_dict, dict):\n",
    "        res = res.rename(columns=lambda k: rename_dict.setdefault(k, k))\n",
    "    return res\n",
    "\n",
    "\n",
    "def rm_rf(p):\n",
    "    if not p.exists():\n",
    "        return\n",
    "\n",
    "    if p.is_file():\n",
    "        p.unlink()\n",
    "\n",
    "    if p.is_dir():\n",
    "        for i in p.iterdir():\n",
    "            if i.is_file():\n",
    "                i.unlink()\n",
    "            if i.is_dir():\n",
    "                rm_rf(i)  # 递归\n",
    "        p.rmdir()\n",
    "\n",
    "\n",
    "def h5ad_to_mtx(adata, p_dir, prefixes=\"\", as_int=True):\n",
    "    \"\"\"\n",
    "    将adata对象保存为mtx\n",
    "    p_dir ： 输出路径\n",
    "    as_int : 是否将矩阵转换int类型\n",
    "        default True\n",
    "    \"\"\"\n",
    "    assert adata.obs.index.is_unique, '[Error] obs index is not unique'\n",
    "    assert adata.var.index.is_unique, '[Error] var index is not unique'\n",
    "\n",
    "    p_dir = Path(p_dir)\n",
    "    p_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # [out] genes.tsv\n",
    "    adata.var[\"gene_names\"] = adata.var_names.to_numpy()\n",
    "    if \"gene_ids\" not in adata.var_keys():\n",
    "        adata.var[\"gene_ids\"] = adata.var[\"gene_names\"]\n",
    "    df_genes = adata.var.loc[:, [\"gene_ids\", \"gene_names\"]]\n",
    "    df_genes.to_csv(\n",
    "        p_dir.joinpath(\"{}genes.tsv\".format(prefixes)),\n",
    "        header=False,\n",
    "        index=False,\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "\n",
    "    # [out] barcodes.tsv obs.csv\n",
    "    adata.obs.loc[:, []].to_csv(\n",
    "        p_dir.joinpath(\"{}barcodes.tsv\".format(prefixes)),\n",
    "        header=False,\n",
    "        index=True,\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "\n",
    "    if len(adata.obs_keys()) > 0:\n",
    "        adata.obs.to_csv(\n",
    "            p_dir.joinpath(\"{}obs.csv\".format(prefixes)), index=True\n",
    "        )\n",
    "\n",
    "    # [out] matrix.mtx\n",
    "    adata.X = csr_matrix(adata.X)\n",
    "    if as_int:\n",
    "        adata.X = adata.X.astype(int)\n",
    "    nonzero_index = [i[:10] for i in adata.X.nonzero()]\n",
    "    print(\n",
    "        \"frist 10 adata.X nonzero elements:\\n\",\n",
    "        adata.X[nonzero_index[0], nonzero_index[1]],\n",
    "    )\n",
    "    mmwrite(\n",
    "        p_dir.joinpath(\"{}matrix.mtx\".format(prefixes)), adata.X.getH()\n",
    "    )\n",
    "    print(\"[out] {}\".format(p_dir))\n",
    "\n",
    "\n",
    "def get_path_varmap(\n",
    "        sp_ref,\n",
    "        sp_que,\n",
    "        p_df_varmap=p_df_varmap,\n",
    "        p_maps_SAMap=p_maps_SAMap,\n",
    "        model='csMAHN'):\n",
    "    \"\"\"通过sp_ref 和 sp_que获取path_varmap\n",
    "    ./homo/df_varmap.csv 存储了\n",
    "    path_varmap路径及信息\n",
    "\"\"\"\n",
    "    p_df_varmap = Path(p_df_varmap)\n",
    "    if model in 'csMAHN,came,csMAHN_before_custom_trainer'.split(','):\n",
    "        df_varmap = pd.read_csv(p_df_varmap)\n",
    "        index_ = df_varmap.query(\n",
    "            \"sp_ref == '{}' & sp_que == '{}'\".format(\n",
    "                sp_ref, sp_que)).index\n",
    "        assert index_.size == 1, \"[get {} path]can not get speicifed and unique path\\nsp_ref\\tsp_que\\n{}\\t{}\".format(\n",
    "            index_.size, sp_ref, sp_que)\n",
    "        res = Path(df_varmap.loc[index_[0], 'path'])\n",
    "        if not res.is_absolute():\n",
    "            res = p_df_varmap.parent.joinpath(res)\n",
    "        assert res.exists(), \"[not exists] {}\".format(res)\n",
    "        return res\n",
    "\n",
    "    elif model == 'SAMap':\n",
    "        return p_maps_SAMap\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"[Error] can not find path_varmap with model '{}'\".format(model))\n",
    "\n",
    "def exchange_gn(sp_ref,sp_que,gns,return_type='array'):\n",
    "    gns = pd.Series(gns)\n",
    "    df_varmap = pd.read_csv(get_path_varmap(sp_ref,sp_que),\n",
    "                skiprows=[0],\n",
    "                names='gn_ref,gn_que,gn_type'.split(','))\\\n",
    "        .dropna(axis=0)\n",
    "    df_varmap = df_varmap[df_varmap['gn_ref'].isin(gns)].copy()\n",
    "    res = df_varmap['gn_que'].to_numpy()\n",
    "\n",
    "    if return_type == 'array':\n",
    "        res = df_varmap['gn_que'].to_numpy()\n",
    "    elif return_type == 'df':\n",
    "        res = df_varmap\n",
    "    elif return_type == 'array_with_not_match':\n",
    "        res = np.concatenate([gns[~gns.isin(df_varmap['gn_ref'])],\n",
    "                    df_varmap['gn_que']])\n",
    "    else:\n",
    "        Warning('[Waring] return_type exchange to array')\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_test_result_df(\n",
    "        p,\n",
    "        extract=\"^(?P<tissue>.+)_(?P<sp_ref>.+)-corss-(?P<sp_que>.+);(?P<model>came|csMAHN|Seurat|SAMap);(?P<name_ref>[\\\\w-]+)-map-(?P<name_que>[[\\\\w-]+);?(?P<resdir_tag>.+)?$\"):\n",
    "    p = Path(p)\n",
    "    df = pd.DataFrame({\"dir\": [i for i in p.iterdir() if i.is_dir()]})\n",
    "    df = df[df[\"dir\"].apply(lambda x: x.joinpath(\"finish\").exists())]\n",
    "    df[\"name\"] = df[\"dir\"].apply(lambda x: x.name)\n",
    "    print('\\n[extract]\\n{}'.format(extract))\n",
    "    df = df.join(\n",
    "        df[\"name\"].str.extract(\n",
    "            extract\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_1v1(row):\n",
    "    res = None\n",
    "    if row['model'] == 'seurat':\n",
    "        res = True\n",
    "    elif row['model'] in ['came', 'csMAHN']:\n",
    "        if 'is_1v1=True' in row['resdir_tag']:\n",
    "            res = True\n",
    "        if 'is_1v1=False' in row['resdir_tag']:\n",
    "            res = False\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_res_obs(row):\n",
    "    \"\"\"\n",
    "    row 为来自get_test_result_df的行\n",
    "    dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\n",
    "    \"\"\"\n",
    "    def get_res_obs_csMAHN(row):\n",
    "        df = pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"res_2\", \"pre_out_2.csv\"), index_col=0\n",
    "        ).loc[:, \"true_label,pre_label,pre_label_NUM,max_prob\".split(\",\")]\n",
    "\n",
    "        p_am = row[\"dir\"].joinpath(\"adata_meta\")\n",
    "        if not p_am.exists():\n",
    "            print(\"[not exists] {}/{} \".format(row[\"dir\"].name, p_am.name))\n",
    "            # return df\n",
    "        df_obs = pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"adata_meta\", \"obs.csv\"), index_col=0\n",
    "        ).loc[:, [\"dataset\", \"cell_type\"]]\n",
    "\n",
    "        df_umap = pd.read_csv(\n",
    "            p_am.joinpath(\"obsm.csv\"), index_col=None\n",
    "        ).rename(columns={\"X_umap1\": \"UMAP1\", \"X_umap2\": \"UMAP2\"})\n",
    "        df_umap.index = df_obs.index\n",
    "\n",
    "        if not df_obs.index.is_unique:\n",
    "            if df_obs.apply(\n",
    "                    lambda row: \"{};{}\".format(\n",
    "                        row.name,\n",
    "                        row['dataset']),\n",
    "                    axis=1).is_unique:\n",
    "                df_obs.index = df_obs.apply(\n",
    "                    lambda row: \"{};{}\".format(\n",
    "                        row.name, row['dataset']), axis=1).to_numpy()\n",
    "                df.index = df_obs.index\n",
    "                df_umap.index = df_obs.index\n",
    "                print('[index] add dataset')\n",
    "            else:\n",
    "                raise Exception('[index not unique]')\n",
    "        df = df.join(df_obs).join(df_umap)\n",
    "\n",
    "        df[\"is_right\"] = df.eval(\"true_label == pre_label\")\n",
    "        df = df.loc[\n",
    "            :,\n",
    "            np.intersect1d(\n",
    "                \"dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\".split(\n",
    "                    \",\"\n",
    "                ),\n",
    "                df.columns,\n",
    "            ),\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "    def get_res_obs_came(row):\n",
    "        df = pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"obs.csv\"),\n",
    "            index_col=0,\n",
    "            usecols=\"original_name,dataset,REF,celltype,predicted,max_probs,is_right\".split(\n",
    "                \",\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        df.index = df.index.to_numpy()\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                # \"REF\": \"\",\n",
    "                \"predicted\": \"pre_label\",\n",
    "                \"max_probs\": \"max_prob\",\n",
    "                \"celltype\": \"cell_type\"\n",
    "            }\n",
    "        )\n",
    "        df['true_label'] = df['cell_type']\n",
    "\n",
    "        df = df.loc[\n",
    "            :,\n",
    "            np.intersect1d(\n",
    "                \"dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\".split(\n",
    "                    \",\"\n",
    "                ),\n",
    "                df.columns,\n",
    "            ),\n",
    "        ]\n",
    "        p_am = row[\"dir\"].joinpath(\"adata_meta\")\n",
    "        if not p_am.exists():\n",
    "            print(\"[not exists] {}/{} \".format(row[\"dir\"].name, p_am.name))\n",
    "            return df\n",
    "\n",
    "        # get UMAP1,UMAP2\n",
    "        temp_df = pd.read_csv(\n",
    "            p_am.joinpath(\"obsm.csv\"), index_col=None\n",
    "        ).rename(columns={\"X_umap1\": \"UMAP1\", \"X_umap2\": \"UMAP2\"})\n",
    "        assert (\n",
    "            temp_df.shape[0] == df.shape[0]\n",
    "        ), \"[Error][length not equal] {} {}\".format(\n",
    "            temp_df.shape[0], df.shape[0]\n",
    "        )\n",
    "        temp_df.index = df.index\n",
    "        df = df.join(temp_df)\n",
    "\n",
    "        df[\"is_right\"] = df.eval(\"true_label == pre_label\")\n",
    "        df = df.loc[\n",
    "            :,\n",
    "            np.intersect1d(\n",
    "                \"dataset,cell_type,true_label,pre_label,max_prob,is_right,UMAP1,UMAP2\".split(\n",
    "                    \",\"\n",
    "                ),\n",
    "                df.columns,\n",
    "            ),\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "    def get_res_obs_seurat(row):\n",
    "        return pd.read_csv(\n",
    "            row[\"dir\"].joinpath(\"obs.csv\"), index_col=0, low_memory=False\n",
    "        ).loc[\n",
    "            :,\n",
    "            \"UMAP1,UMAP2,cell_type,dataset,is_right,max_prob,pre_label,true_label\".split(\n",
    "                \",\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_res_obs_SAMap(row):\n",
    "        return get_res_obs_seurat(row)\n",
    "\n",
    "    map_func = {\n",
    "        k: v\n",
    "        for k, v in zip('csMAHN,came,Seurat,SAMap'.split(','),\n",
    "                        [get_res_obs_csMAHN, get_res_obs_came, get_res_obs_seurat, get_res_obs_SAMap])\n",
    "    }\n",
    "    assert row[\"model\"] in map_func.keys(\n",
    "    ), \"can note get res obs. model = {}\".format(row[\"model\"])\n",
    "\n",
    "    res = map_func[row[\"model\"]](row)\n",
    "    res['dataset_type'] = res['dataset'].map({\n",
    "        '{tissue}_{sp_ref}'.format(**row): 'ref',\n",
    "        '{tissue}_{sp_que}'.format(**row): 'que'\n",
    "    })\n",
    "    res['sp'] = res['dataset'].map({\n",
    "        '{tissue}_{sp_ref}'.format(**row): row['sp_ref'],\n",
    "        '{tissue}_{sp_que}'.format(**row): row['sp_que']\n",
    "    })\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_adata_umap(row, intersect_pre_field=True):\n",
    "\n",
    "    df_obs_ref = pd.read_csv(\n",
    "        row['dir'].joinpath('obs_ref.csv'),\n",
    "        index_col=0)\n",
    "    df_obs_que = pd.read_csv(\n",
    "        row['dir'].joinpath('obs_que.csv'),\n",
    "        index_col=0)\n",
    "    if intersect_pre_field:\n",
    "        df_obs_ref, df_obs_que = [_.loc[:, np.intersect1d(\n",
    "            df_obs_ref.columns, df_obs_que.columns)] for _ in [df_obs_ref, df_obs_que]]\n",
    "    df_obs = pd.concat([df_obs_ref, df_obs_que])\\\n",
    "        .rename(columns=lambda x: 'preobs_{}'.format(x))\n",
    "\n",
    "    df_res = get_res_obs(row).join(df_obs)\n",
    "    adata_umap = sc.AnnData(obs=df_res)\n",
    "    adata_umap.obsm['X_umap'] = df_res.loc[:,\n",
    "                                           'UMAP1,UMAP2'.split(',')].to_numpy()\n",
    "    return adata_umap\n",
    "\n",
    "\n",
    "def subset_adata(adata, *args):\n",
    "    def _process_values(values):\n",
    "        if isinstance(values, Iterable):\n",
    "            if isinstance(values, str):\n",
    "                values = [values]\n",
    "        else:\n",
    "            values = [values]\n",
    "        return values\n",
    "    assert len(\n",
    "        args) % 2 == 0, '[Error][{}] length of args must be 2*n'.format(len(args))\n",
    "\n",
    "    for key, values in zip(args[0::2], args[1::2]):\n",
    "        values = _process_values(values)\n",
    "        adata = adata[adata.obs[key].isin(values), :]\n",
    "    return adata\n",
    "\n",
    "\n",
    "def show_umap(row):\n",
    "    \"\"\"\n",
    "    row 为来自get_test_result_df的行\n",
    "\n",
    "    show test result umap\n",
    "    row\n",
    "        umap_dataset      :  png path\n",
    "        umap_umap         :  png path\n",
    "        p_cell_type_table :  csv path\n",
    "    \"\"\"\n",
    "    p_umap_dataset = Path(\n",
    "        row[\"dir\"]).joinpath(\n",
    "        \"figs\",\n",
    "        \"umap_dataset.png\")\n",
    "    p_umap_umap = Path(row[\"dir\"]).joinpath('figs', 'umap_umap.png')\n",
    "    p_cell_type_table = Path(\n",
    "        row[\"dir\"]).joinpath('group_counts.csv')\n",
    "    print(row['name'].ljust(75, '-'))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    ax[0].imshow(mpimg.imread(p_umap_dataset))\n",
    "    ax[0].set_axis_off()\n",
    "    ax[1].imshow(mpimg.imread(p_umap_umap))\n",
    "    ax[1].set_axis_off()\n",
    "    display(fig, pd.read_csv(p_cell_type_table, index_col=0))\n",
    "    fig.clear()\n",
    "\n",
    "\n",
    "def find_path_from_para(df_para, name):\n",
    "    s = pd.Series(\n",
    "        np.concatenate(\n",
    "            (df_para['path_ref'], df_para['path_que'])), index=np.concatenate(\n",
    "            (df_para['name_ref'], df_para['name_que'])))\n",
    "    s = s.drop_duplicates(ignore_index=False)\n",
    "    assert s.index.is_unique, '[Error] df_para name_ref and name_que is not unique'\n",
    "    return s[name]\n",
    "\n",
    "\n",
    "def df_apply_merge_field(df, str_format):\n",
    "    return df.apply(lambda row: str_format.format(**row), axis=1)\n",
    "\n",
    "def df_varmap_query_exists(\n",
    "        df_varmap,\n",
    "        list_gn_ref=[],\n",
    "        list_gn_que=[],\n",
    "        model='both'):\n",
    "    df_varmap = df_varmap.copy()\n",
    "    df_varmap['gn_ref_exists'] = df_varmap['gn_ref'].isin(list_gn_ref)\n",
    "    df_varmap['gn_que_exists'] = df_varmap['gn_que'].isin(list_gn_que)\n",
    "    if model == 'both':\n",
    "        df_varmap = df_varmap.query(\"gn_ref_exists & gn_que_exists\")\n",
    "    elif model == 'ref':\n",
    "        df_varmap = df_varmap.query(\"gn_ref_exists\")\n",
    "    elif model == 'que':\n",
    "        df_varmap = df_varmap.query(\"gn_que_exists\")\n",
    "    else:\n",
    "        raise Exception('[Error] model must be one of both, ref, que')\n",
    "    df_varmap = df_varmap.drop(\n",
    "        columns='gn_ref_exists,gn_que_exists'.split(','))\n",
    "    return df_varmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb74bc-fd06-4e99-8e30-51eea8f31859",
   "metadata": {},
   "source": [
    "# cross species Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f6eea-0f1e-470e-9828-cdc8e576a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adata(p):\n",
    "    def load_h5ad_from_mtx(p):\n",
    "        p = Path(p)\n",
    "        assert p.joinpath(\"matrix.mtx\").exists(\n",
    "        ), '[not exists]matrix.mtx\\nin{}'.format(p)\n",
    "        adata = sc.read_10x_mtx(p)\n",
    "        if p.joinpath(\"obs.csv\").exists():\n",
    "            adata.obs = pd.read_csv(p.joinpath(\"obs.csv\"), index_col=0)\n",
    "            adata.obs.to_numpy()\n",
    "            # adata.obs = adata.obs.loc[:, []].join(\n",
    "            #     pd.read_csv(p.joinpath(\"obs.csv\"), index_col=0))\n",
    "        else:\n",
    "            print('[not exists]obs.csv\\nin {}'.format(p))\n",
    "        return adata\n",
    "    p = Path(p)\n",
    "    if p.match(\"*.h5ad\"):\n",
    "        return sc.read_h5ad(p)\n",
    "    elif p.is_dir() and p.joinpath(\"matrix.mtx\").exists():\n",
    "        return load_h5ad_from_mtx(p)\n",
    "    else:\n",
    "        raise Exception(\"[can not load adata] {}\".format(p))\n",
    "\n",
    "\n",
    "def load_normalized_adata(p, obs=None, update=False):\n",
    "    def _process_adata(adata,p_out):\n",
    "        adata.layers[\"counts\"] = adata.X.copy()\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata, base=np.e)\n",
    "        adata.layers[\"scaled\"] = sc.pp.scale(adata, copy=True).X\n",
    "        adata.write_h5ad(p_out)\n",
    "        return adata\n",
    "        \n",
    "    p = Path(p)\n",
    "    assert p.is_dir, '[Error] please get a path of dir'\n",
    "    p_h5ad = p.joinpath('normalize.h5ad')\n",
    "    adata = sc.read_h5ad(p_h5ad) if p_h5ad.exists() else None\n",
    "\n",
    "    # 判断是否需要重新运行_process_adata\n",
    "    if adata is None:\n",
    "        update = True\n",
    "\n",
    "    if update:\n",
    "        pass\n",
    "    else:# 当update = False时,通过adata的内容,决定是否update\n",
    "        update = not pd.Series('counts,scaled'.split(','))\\\n",
    "            .isin(adata.layers.keys()).all()\n",
    "        \n",
    "        \n",
    "    if update:\n",
    "        adata = load_adata(p)\n",
    "        print('[normalize adata]\\n{}\\n'.format(p_h5ad))\n",
    "        adata = _process_adata(adata,p_h5ad)\n",
    "    if isinstance(obs, pd.DataFrame):\n",
    "        adata.obs = adata.obs.loc[:, []].join(obs)\n",
    "    return adata\n",
    "\n",
    "def get_1v1_matches(\n",
    "        df_match,\n",
    "        key_homology_type='homology_type',\n",
    "        value_homology_type='ortholog_one2one'):\n",
    "    \"\"\"\n",
    "    from came.pp.take_1v_matches\n",
    "    \"\"\"\n",
    "    l, r = df_match.columns[:2]\n",
    "    l_unique = df_match[l].value_counts(\n",
    "    ).to_frame().query(\"count == 1\").index\n",
    "    r_unique = df_match[r].value_counts(\n",
    "    ).to_frame().query(\"count == 1\").index\n",
    "    keep = pd.DataFrame({\n",
    "        'l_is_unique': df_match[l].isin(l_unique),\n",
    "        'r_is_unique': df_match[r].isin(r_unique)\n",
    "    }).min(axis=1)\n",
    "    df_match = df_match[keep]\n",
    "    df_match = df_match.query(\n",
    "        \"{} == '{}'\".format(\n",
    "            key_homology_type,\n",
    "            value_homology_type))\n",
    "    return df_match\n",
    "\n",
    "\n",
    "def get_homology_parameters(adata1, adata2, df_varmap):\n",
    "    res = {}\n",
    "    df_homo_paras = pd.DataFrame({\n",
    "        \"gn_ref\": adata1.var_names\n",
    "    }).merge(df_varmap, on='gn_ref', how='left')\n",
    "    res['homology_one2one_find'] = df_homo_paras['gn_que'].notna().sum()\n",
    "\n",
    "    df_homo_paras['gn_que'] = df_homo_paras.apply(\n",
    "        lambda row: 'not_o2o_' + row['gn_ref'] if pd.isna(\n",
    "            row['gn_que']) else row['gn_que'], axis=1)\n",
    "\n",
    "    assert (df_homo_paras['gn_ref'] == adata1.var_names).all(\n",
    "    ), \"df_homo_paras['gn_ref'] not equal adata1.var_names\"\n",
    "    assert df_homo_paras['gn_ref'].is_unique & df_homo_paras['gn_que'].is_unique, \"df_homo_paras gn_ref or gn_que is not unique\"\n",
    "    # came 和 csMAHN不做替换\n",
    "    # adata1.var.index = df_homo_paras['gn_que'].to_numpy()\n",
    "    res['homology_one2one_use'] = np.intersect1d(\n",
    "        df_homo_paras['gn_que'],\n",
    "        adata2.var.index).size\n",
    "    res = {k: int(v) for k, v in res.items()}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_type_counts_info(adatas, key_class, dsnames):\n",
    "    type_counts_list = []\n",
    "    for i in range(len(adatas)):\n",
    "        type_counts_list.append(pd.value_counts(adatas[i].obs[key_class]))\n",
    "    counts_info = pd.concat(type_counts_list, axis=1, keys=dsnames)\n",
    "    return counts_info\n",
    "\n",
    "\n",
    "def aligned_type(adatas, key_calss):\n",
    "    adata1 = adatas[0].copy()\n",
    "    adata2 = adatas[1].copy()\n",
    "    counts_info = get_type_counts_info(\n",
    "        adatas, key_calss, dsnames=[\"reference\", \"query\"]\n",
    "    )\n",
    "    print(\"----raw----\")\n",
    "    print(counts_info)\n",
    "    counts_info = counts_info.dropna(how=\"any\")\n",
    "    print(\"----new----\")\n",
    "    print(counts_info)\n",
    "\n",
    "    com_type = counts_info.index.tolist()\n",
    "    adata1 = adata1[adata1.obs[key_calss].isin(com_type)]\n",
    "    adata2 = adata2[adata2.obs[key_calss].isin(com_type)]\n",
    "    return adata1, adata2\n",
    "\n",
    "\n",
    "def unify_group_counts_index_name(resdir):\n",
    "    \"\"\"\n",
    "    不知为何，group_counts.index.name参差不齐\n",
    "    故将group_counts_unalign.index.name 赋给 group_counts.index.name\n",
    "    \"\"\"\n",
    "    p_group_counts = resdir.joinpath(\"group_counts.csv\")\n",
    "    p_group_counts_unalign = resdir.joinpath(\"group_counts_unalign.csv\")\n",
    "\n",
    "    lines = p_group_counts.read_text().split(\"\\n\")\n",
    "    lines[0] = \",\".join(\n",
    "        [p_group_counts_unalign.read_text().split(\"\\n\")[0].split(\",\")[0]]\n",
    "        + lines[0].split(\",\")[1:]\n",
    "    )\n",
    "    p_group_counts.write_text(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284785d-d2c5-47e4-afa2-dd9637085e2f",
   "metadata": {},
   "source": [
    "## came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498e303-4909-47c4-8e89-8693fae68e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precess_after_came(resdir, tissue_name, sp1, sp2, is_display=False):\n",
    "    unify_group_counts_index_name(resdir)\n",
    "    resdir = Path(resdir)\n",
    "    figdir = resdir.joinpath(\"figs\")\n",
    "    sc.settings.figdir = figdir\n",
    "\n",
    "    display(\n",
    "        pd.read_csv(resdir.joinpath(\"group_counts.csv\"), index_col=0)\n",
    "    ) if is_display else None\n",
    "    obs = pd.read_csv(resdir.joinpath(\"obs.csv\"), index_col=0)\n",
    "\n",
    "    # umap\n",
    "    # the last layer of hidden states\n",
    "    h_dict = came.load_hidden_states(resdir.joinpath(\"hidden_list.h5\"))[-1]\n",
    "    adt = came.pp.make_adata(\n",
    "        h_dict[\"cell\"], obs=obs, assparse=False, ignore_index=True\n",
    "    )\n",
    "    sc.pp.neighbors(adt, n_neighbors=15, metric=\"cosine\", use_rep=\"X\")\n",
    "    sc.tl.umap(adt)\n",
    "\n",
    "    sc.pl.umap(adt, color=\"dataset\", save=\"_dataset.png\")\n",
    "    sc.pl.umap(adt, color=\"celltype\", save=\"_umap.png\")\n",
    "    adt.write_csvs(resdir.joinpath(\"adata_meta\"))\n",
    "    # # umap.csv umap坐标存储\n",
    "    # pd.DataFrame(\n",
    "    #     adt.obsm[\"X_umap\"],\n",
    "    #     columns=[\"umap_1\", \"umap_2\"],\n",
    "    #     index=sc.get.obs_df(adt, \"original_name\"),\n",
    "    # ).reset_index().to_csv(resdir.joinpath(\"umap.csv\"), index=False)\n",
    "\n",
    "    # test umap.csv\n",
    "    # temp_obs = pd.read_csv(resdir.joinpath(\"obs.csv\"))\n",
    "    # temp_obs = temp_obs.merge(pd.read_csv(resdir.joinpath(\"umap.csv\")),on=\"original_name\")\n",
    "    # temp_adata = sc.AnnData(obs=temp_obs)\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"dataset\")\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"celltype\")\n",
    "    # del temp_adata,temp_obs\n",
    "\n",
    "    # ratio\n",
    "    display(\n",
    "        obs[\"is_right\"].sum() / obs[\"is_right\"].size\n",
    "    ) if is_display else None\n",
    "\n",
    "    obs[\"name\"] = obs[\"original_name\"].str.extract(\";(.+)\", expand=False)\n",
    "    obs[\"sp\"] = obs[\"dataset\"].str.extract(\n",
    "        \"{}_(\\\\w+)\".format(tissue_name), expand=False\n",
    "    )\n",
    "    # 物种\n",
    "    df_sp = (\n",
    "        group_agg(obs, [\"sp\"], {\"is_right\": [\"sum\", \"count\"]})\n",
    "        .eval(\"ratio = is_right_sum/is_right_count\")\n",
    "        .sort_values([\"sp\"])\n",
    "    )\n",
    "    df_sp[\"type\"] = \"species\"\n",
    "    # 各个dataset\n",
    "    df_dataset = (\n",
    "        group_agg(obs, [\"sp\", \"name\"], {\"is_right\": [\"sum\", \"count\"]})\n",
    "        .eval(\"ratio = is_right_sum/is_right_count\")\n",
    "        .sort_values([\"sp\", \"name\"])\n",
    "    )\n",
    "    df_dataset[\"type\"] = \"dataset\"\n",
    "    df_ratio = pd.concat([df_sp, df_dataset], axis=0)\n",
    "    df_ratio[\"tissue\"] = tissue_name\n",
    "    df_ratio = df_ratio.loc[\n",
    "        :,\n",
    "        \"tissue,type,sp,name,is_right_sum,is_right_count,ratio\".split(\",\"),\n",
    "    ]\n",
    "    display(df_ratio) if is_display else None\n",
    "    df_ratio.to_csv(resdir.joinpath(\"ratio.csv\"), index=False)\n",
    "    del df_sp, df_dataset, df_ratio\n",
    "\n",
    "    # heatmap\n",
    "    # all,reference,query\n",
    "    for q, sp in zip(\n",
    "        (\n",
    "            \"{col} == {col}\".format(col=obs.columns[0]),\n",
    "            # all, both sp1 and sp2\n",
    "            \"sp == '{sp}'\".format(sp=sp1),\n",
    "            \"sp == '{sp}'\".format(sp=sp2),\n",
    "        ),\n",
    "        (\"all\", sp1, sp2),\n",
    "    ):\n",
    "        res = (\n",
    "            group_agg(\n",
    "                obs.query(q),\n",
    "                [\"celltype\", \"predicted\"],\n",
    "                {\"predicted\": [\"count\"]},\n",
    "            )\n",
    "            .pivot(\n",
    "                index=\"celltype\",\n",
    "                columns=\"predicted\",\n",
    "                values=\"predicted_count\",\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        display(res) if is_display else None\n",
    "        res.to_csv(\n",
    "            resdir.joinpath(\n",
    "                'predicted_count_{}.csv'.format(sp)),\n",
    "            index=True)\n",
    "        ax = sns.heatmap(\n",
    "            data=stats.zscore(res, axis=1), cmap=plt.get_cmap(\"Greens\")\n",
    "        )\n",
    "\n",
    "        ax.set_title('{}-{}'.format(tissue_name, sp))\n",
    "        ax.figure.savefig(\n",
    "            figdir.joinpath('heatmap_ratio_{}.pdf'.format(ax.get_title())),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=120,\n",
    "        )\n",
    "        ax.figure.clear()\n",
    "\n",
    "\n",
    "def run_came(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path('.'),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2), **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    version:0.0.5\n",
    "    kvargs:\n",
    "        n_epochs: int\n",
    "            default,500,但是为了与csMAHN统一\n",
    "            n_epochs = sum(kvargs.setdefault(\"n_epochs\",[100,200,200]))\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "        n_degs:\n",
    "            default,50\n",
    "            ntop_deg = n_degs\n",
    "            ntop_deg_nodes = n_degs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameter settings\n",
    "    # n_epochs = 500\n",
    "    n_epochs = sum(kvargs.setdefault(\"n_epochs\", [100, 200, 200]))\n",
    "    batch_size = None\n",
    "    n_pass = 100\n",
    "    use_scnets = True\n",
    "    # n_hvgs = kvargs.setdefault('n_hvgs', 2000)\n",
    "    n_degs = kvargs.setdefault('n_degs', 50)\n",
    "    ntop_deg = n_degs  # 50\n",
    "    ntop_deg_nodes = n_degs  # 50\n",
    "    node_source = \"deg,hvg\"\n",
    "    # keep_non1v1_feats = True\n",
    "    keep_non1v1_feats = not kvargs.setdefault(\"is_1v1\", False)\n",
    "\n",
    "    # setting directory for results\n",
    "    if len(resdir_tag) > 0:\n",
    "\n",
    "        resdir_tag = \"{}_{}-corss-{};{}\".format(\n",
    "            tissue_name, sp1, sp2, resdir_tag)\n",
    "    else:\n",
    "        resdir_tag = \"{}_{}-corss-{}\".format(tissue_name, sp1, sp2)\n",
    "\n",
    "    resdir = resdir.joinpath(resdir_tag)\n",
    "\n",
    "    # 终止 判断\n",
    "    p_finish = resdir.joinpath(\"finish\")\n",
    "    if p_finish.exists():\n",
    "        # precess_after_came(resdir,tissue_name,sp1, sp2)\n",
    "        print(\n",
    "            \"[has finish]{} {}\".format(\n",
    "                time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "                resdir.name)\n",
    "        )\n",
    "        return\n",
    "    print(\n",
    "        \"[start]{} {}\".format(\n",
    "            time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "            resdir.name\n",
    "\n",
    "        ))\n",
    "    # return\n",
    "\n",
    "    figdir = resdir.joinpath(\"figs\")\n",
    "    sc.settings.figdir = figdir\n",
    "    resdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    finish_content = [\"[strat] {}\".format(time.time())]\n",
    "\n",
    "    # # setting\n",
    "\n",
    "    dsnames = (\n",
    "        '{}_{}'.format(\n",
    "            tissue_name, sp1), '{}_{}'.format(\n",
    "            tissue_name, sp2))\n",
    "    dsn1, dsn2 = dsnames\n",
    "    homo_method = \"biomart\"\n",
    "\n",
    "    # load data\n",
    "    adata_raw1 = load_adata(path_adata1)\n",
    "    adata_raw2 = load_adata(path_adata2)\n",
    "    key_class = key_class1\n",
    "    if key_class not in adata_raw2.obs.columns:\n",
    "        adata_raw2.obs[key_class] = ''\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_raw1, adata_raw2 = limite_func(adata_raw1, adata_raw2)\n",
    "\n",
    "    # group_counts_unalign.csv\n",
    "    pd.concat(\n",
    "        [\n",
    "            adata_raw1.obs[key_class1].value_counts(),\n",
    "            adata_raw2.obs[key_class2].value_counts(),\n",
    "        ],\n",
    "        axis=1,\n",
    "        keys=dsnames,\n",
    "    ).to_csv(resdir.joinpath(\"group_counts_unalign.csv\"), index=True)\n",
    "    # align\n",
    "    if aligned:\n",
    "        adata_raw1, adata_raw2 = aligned_type(\n",
    "            [adata_raw1, adata_raw2], key_calss=key_class1\n",
    "        )\n",
    "\n",
    "    # 保存obs ,即真正测试的细胞的mata\n",
    "    adata_raw1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    adata_raw2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat(\n",
    "        [\n",
    "            adata_raw1.obs[key_class1].value_counts(),\n",
    "            adata_raw2.obs[key_class2].value_counts(),\n",
    "        ],\n",
    "        axis=1,\n",
    "        keys=dsnames,\n",
    "    )\n",
    "    # came会自行导出\n",
    "    # temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    # if temp.shape[0] < 2:\n",
    "    #     # 错误标记\n",
    "    #     print(\"[Error][group_counts no any item]\")\n",
    "    #     finish_content.append(\n",
    "    #         \"[Error][group_counts no any item] %f\" % time.time()\n",
    "    #     )\n",
    "    #     p_finish.with_name(\"error\").write_text(\"\\n\".join(finish_content))\n",
    "    #     return\n",
    "\n",
    "    adatas = [adata_raw1, adata_raw2]\n",
    "    print(\"cell count --> {}\".format(sum([i.shape[0] for i in adatas])))\n",
    "\n",
    "    df_varmap = pd.read_csv(path_varmap, usecols=range(3))\n",
    "    df_varmap.columns = [\"gn_ref\", \"gn_que\", \"homology_type\"]\n",
    "    if kvargs.setdefault(\"is_1v1\", False):\n",
    "        df_varmap = get_1v1_matches(df_varmap)\n",
    "        homology_parameter = get_homology_parameters(\n",
    "            adata_raw1, adata_raw2, df_varmap)\n",
    "        print(\"\"\"\n",
    "[homology one2one]find {homology_one2one_find} genes\n",
    "[homology one2one]use {homology_one2one_use} genes\"\"\".format(\n",
    "            **homology_parameter))\n",
    "        kvargs.update(homology_parameter)\n",
    "\n",
    "    df_varmap_1v1 = came.pp.take_1v1_matches(df_varmap)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir),\n",
    "                  'n_degs': n_degs}\n",
    "                  )\n",
    "    resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "\n",
    "    finish_content.append(\"[finish before run] {}\".format(time.time()))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    came_inputs, (adata1, adata2) = came.pipeline.preprocess_unaligned(\n",
    "        adatas,\n",
    "        key_class=key_class1,\n",
    "        use_scnets=use_scnets,\n",
    "        ntop_deg=ntop_deg,\n",
    "        ntop_deg_nodes=ntop_deg_nodes,\n",
    "        node_source=node_source,\n",
    "    )\n",
    "\n",
    "    outputs = came.pipeline.main_for_unaligned(\n",
    "        **came_inputs,\n",
    "        df_varmap=df_varmap,\n",
    "        df_varmap_1v1=df_varmap_1v1,\n",
    "        dataset_names=dsnames,\n",
    "        key_class1=key_class1,\n",
    "        key_class2=key_class2,\n",
    "        do_normalize=True,\n",
    "        keep_non1v1_feats=keep_non1v1_feats,\n",
    "        n_epochs=n_epochs,\n",
    "        resdir=resdir,\n",
    "        n_pass=n_pass,\n",
    "        batch_size=batch_size,\n",
    "        plot_results=True,\n",
    "    )\n",
    "\n",
    "    finish_content.append(\"[finish run] {}\".format(time.time()))\n",
    "\n",
    "    dpair = outputs[\"dpair\"]\n",
    "    trainer = outputs[\"trainer\"]\n",
    "    h_dict = outputs[\"h_dict\"]\n",
    "    out_cell = outputs[\"out_cell\"]\n",
    "    predictor = outputs[\"predictor\"]\n",
    "\n",
    "    obs_ids1, obs_ids2 = dpair.obs_ids1, dpair.obs_ids2\n",
    "    obs = dpair.obs\n",
    "    classes = predictor.classes\n",
    "    # 后处理\n",
    "    precess_after_came(resdir, tissue_name, sp1, sp2)\n",
    "    finish_content.append(\"[finish after run] {}\".format(time.time()))\n",
    "\n",
    "    # 完成标记\n",
    "\n",
    "    finish_content.append(\"[end] {}\".format(time.time()))\n",
    "    p_finish.write_text(\"\\n\".join(finish_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f2e23-a640-4c52-a78d-ad0aedb4064b",
   "metadata": {},
   "source": [
    "## csMAHN\n",
    "\n",
    "获取使用的hvg数量 未完成,2024年3月27日16:17:23\n",
    "\n",
    "\n",
    "> `preprocoess.py : 96 > process_for_graph`\n",
    "\n",
    "```python\n",
    "\n",
    "print(\"--------------hvgs, degs info---------------\")\n",
    "print(\"num of reference_hvgs,reference_degs,reference_higs are {0},{1},{2}\".format(\n",
    "    len(reference_hvgs),len(reference_degs),len(reference_higs)))\n",
    "print(\"num of query_hvgs,query_degs,query_higs are {0},{1},{2}\".format(\n",
    "    len(query_hvgs), len(query_degs),len(query_higs)))\n",
    "\n",
    "```\n",
    "\n",
    "```txt\n",
    "\n",
    "--------------hvgs, degs info---------------\n",
    "num of reference_hvgs,reference_degs,reference_higs are 2000,314,2175\n",
    "num of query_hvgs,query_degs,query_higs are 2000,431,2265\n",
    "```\n",
    "\n",
    "\n",
    "> `preprocoess.py : 535 > select_gene_nodes`\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "print(\"--------------gene nodes info---------------\")\n",
    "print(\"num of reference_gene_node is {0}\".format(len(reference_gene_nodes)))\n",
    "print(\"num of query_gene_node is {0}\".format(len(query_gene_nodes)))\n",
    "return reference_gene_nodes, query_gene_nodes\n",
    "```\n",
    "\n",
    "```txt\n",
    "\n",
    "--------------gene nodes info---------------\n",
    "num of reference_gene_node is 3396\n",
    "num of query_gene_node is 2798\n",
    "--------------homo edges---------------\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6278e-2f7e-4bd3-8595-0b816ed6d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precess_after_csMAHN(\n",
    "    resdir, tissue_name, sp1, sp2, is_display=False, **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    kvargs:\n",
    "        adt\n",
    "    \"\"\"\n",
    "    unify_group_counts_index_name(resdir)\n",
    "\n",
    "    resdir = Path(resdir)\n",
    "    assert resdir.joinpath(\"res_2\").exists(), \"[not exists] res_2\"\n",
    "    figdir = resdir.joinpath(\"figs\")\n",
    "    sc.settings.figdir = figdir\n",
    "\n",
    "    display(\n",
    "        pd.read_csv(resdir / \"group_counts.csv\", index_col=0)\n",
    "    ) if is_display else None\n",
    "\n",
    "    # 为obs添加dataset\n",
    "    obs = pd.read_csv(\n",
    "        resdir.joinpath(\"res_2\", \"pre_out_2.csv\"), index_col=0\n",
    "    )\n",
    "    if not obs.index.is_unique:\n",
    "        print(\"[obs index is not unique]\")\n",
    "\n",
    "    pre_obs = {\n",
    "        '{}_{}'.format(tissue_name, k): pd.read_csv(\n",
    "            resdir.joinpath(file_name), index_col=0\n",
    "        )\n",
    "        for k, file_name in zip([sp1, sp2], [\"obs_ref.csv\", \"obs_que.csv\"])\n",
    "    }\n",
    "    for k, value in pre_obs.items():\n",
    "        value[\"dataset\"] = k\n",
    "        pre_obs[k] = value.loc[:, [\"dataset\"]]\n",
    "    obs = pd.concat(pre_obs.values()).join(obs)\n",
    "    obs[\"is_right\"] = obs[\"true_label\"] == obs[\"pre_label\"]\n",
    "    del pre_obs\n",
    "\n",
    "    # umap\n",
    "    adt = kvargs.setdefault(\"adt\", None)\n",
    "    assert adt is not None\n",
    "    sc.pp.neighbors(adt, n_neighbors=15, metric=\"cosine\", use_rep=\"X\")\n",
    "    sc.tl.umap(adt)\n",
    "    sc.pl.umap(adt, color=\"dataset\", save=\"_dataset.png\")\n",
    "    sc.pl.umap(adt, color=\"cell_type\", save=\"_umap.png\")\n",
    "\n",
    "    adt.write_csvs(resdir.joinpath(\"adata_meta\"))\n",
    "    # # 存储adt.X 和 umap坐标\n",
    "    # pd.DataFrame(adt.X, index=adt.obs.index).to_orc(resdir.joinpath(\"adt.X.orc\"))\n",
    "    # pd.DataFrame(\n",
    "    #     adt.obsm[\"X_umap\"], columns=[\"umap_1\", \"umap_2\"], index=adt.obs.index\n",
    "    # ).assign(**{k: adt.obs[k] for k in adt.obs.keys()}).reset_index().to_csv(\n",
    "    #     resdir.joinpath(\"umap.csv\"), index=False\n",
    "    # )\n",
    "\n",
    "    # # test umap.csv\n",
    "    # temp_obs = pd.read_csv(resdir.joinpath(\"res_2\", \"pre_out_2.csv\"), index_col=0)\n",
    "    # temp_obs = temp_obs.join(pd.read_csv(resdir.joinpath(\"umap.csv\"),index_col=0))\n",
    "    # temp_adata = sc.AnnData(obs=temp_obs)\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"dataset\")\n",
    "    # sc.pl.scatter(temp_adata,\"umap_1\", \"umap_2\",color=\"cell_type\")\n",
    "    # del temp_adata,temp_obs\n",
    "\n",
    "    # ratio\n",
    "    obs[\"name\"] = obs.index.to_series().str.extract(\n",
    "        \";([^;]+)$\", expand=False\n",
    "    )\n",
    "    obs[\"sp\"] = obs[\"dataset\"].str.extract(\n",
    "        \"{}_(\\\\w+)\".format(tissue_name), expand=False\n",
    "    )\n",
    "    display(\n",
    "        obs[\"is_right\"].sum() / obs[\"is_right\"].size\n",
    "    ) if is_display else None\n",
    "    # 物种\n",
    "    df_sp = group_agg(obs, [\"sp\"], {\"is_right\": [\"count\", \"sum\"]})\n",
    "    df_sp[\"type\"] = \"species\"\n",
    "    # 各个dataset\n",
    "    df_dataset = group_agg(\n",
    "        obs, [\"sp\", \"name\"], {\"is_right\": [\"count\", \"sum\"]}\n",
    "    )\n",
    "    df_dataset[\"type\"] = \"dataset\"\n",
    "\n",
    "    df_ratio = pd.concat([df_sp, df_dataset]).eval(\n",
    "        \"ratio = is_right_sum/is_right_count\"\n",
    "    )\n",
    "    df_ratio[\"tissue\"] = tissue_name\n",
    "    df_ratio = df_ratio.loc[\n",
    "        :,\n",
    "        \"tissue,type,sp,name,is_right_sum,is_right_count,ratio\".split(\",\"),\n",
    "    ]\n",
    "    df_ratio.to_csv(resdir / \"ratio.csv\", index=False)\n",
    "    del df_sp, df_dataset, df_ratio\n",
    "\n",
    "    # heatmap\n",
    "    # all,reference,query\n",
    "    for q, sp in zip(\n",
    "        (\n",
    "            \"{col} == {col}\".format(col=obs.columns[0]),\n",
    "            # all, both sp1 and sp2\n",
    "            \"sp == '{sp}'\".format(sp=sp1),\n",
    "            \"sp == '{sp}'\".format(sp=sp2),\n",
    "        ),\n",
    "        (\"all\", sp1, sp2),\n",
    "    ):\n",
    "        res = (\n",
    "            group_agg(\n",
    "                obs.query(q),\n",
    "                [\"true_label\", \"pre_label\"],\n",
    "                {\"pre_label\": [\"count\"]},\n",
    "            )\n",
    "            .pivot(\n",
    "                index=\"true_label\",\n",
    "                columns=\"pre_label\",\n",
    "                values=\"pre_label_count\",\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        display(res) if is_display else None\n",
    "\n",
    "        res.to_csv(\n",
    "            resdir.joinpath(\n",
    "                'predicted_count_{}.csv'.format(sp)),\n",
    "            index=True)\n",
    "\n",
    "        ax = sns.heatmap(\n",
    "            data=stats.zscore(res, axis=1), cmap=plt.get_cmap(\"Greens\")\n",
    "        )\n",
    "\n",
    "        ax.set_title(\"{}-{}\".format(tissue_name, sp))\n",
    "        ax.figure.savefig(\n",
    "            figdir.joinpath('heatmap_ratio_{}.pdf'.format(ax.get_title())),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=120,\n",
    "        )\n",
    "        ax.figure.clear()\n",
    "\n",
    "\n",
    "def run_csMAHN(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path('.'),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "\n",
    "    **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    version = 0.0.9\n",
    "    kvargs:\n",
    "        n_epochs:\n",
    "            default,[100, 200, 300]\n",
    "            stages,即res_0,res_1，res_2 的 epochs\n",
    "            累加制，res_0,res_1，res_2,实际epochs分别为100,300,600\n",
    "            故最终epochs为stages之和\n",
    "            stages = kvargs.setdefault(\"n_epochs\",[100, 200, 300])\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "        n_hvgs:\n",
    "            default,2000\n",
    "        n_degs:\n",
    "            default,50\n",
    "    \"\"\"\n",
    "    homo_method = 'biomart'\n",
    "    n_hvgs = kvargs.setdefault('n_hvgs', 2000)\n",
    "    n_degs = kvargs.setdefault('n_degs', 50)\n",
    "    seed = 123\n",
    "    stages = kvargs.setdefault(\n",
    "        'n_epochs', [\n",
    "            100, 200, 200])  # [200, 200, 200]\n",
    "    nfeats = kvargs.setdefault('nfeats', 64)  # 64  # embedding size #128\n",
    "    hidden = kvargs.setdefault('hidden', 64)  # 64  # 128\n",
    "    input_drop = 0.2\n",
    "    att_drop = 0.2\n",
    "    residual = True\n",
    "\n",
    "    threshold = 0.9  # 0.8\n",
    "    lr = 0.01  # lr = 0.01\n",
    "    weight_decay = 0.001\n",
    "    patience = 100\n",
    "    enhance_gama = 10\n",
    "    simi_gama = 0.1\n",
    "\n",
    "    dsnames = (\n",
    "        '{}_{}'.format(\n",
    "            tissue_name, sp1), '{}_{}'.format(\n",
    "            tissue_name, sp2))\n",
    "    assert key_class1 == key_class2, \"key_class is not equal\"\n",
    "    key_class = key_class1\n",
    "\n",
    "    # make file to save\n",
    "    resdir_tag = \"{}_{}-corss-{};{}\".format(tissue_name, sp1, sp2, resdir_tag) if len(\n",
    "        resdir_tag) > 0 else \"{}_{}-corss-{}\".format(tissue_name, sp1, sp2)\n",
    "    # curdir = os.path.join()\n",
    "    resdir = Path(resdir).joinpath(resdir_tag)\n",
    "    model_dir = resdir.joinpath('model_')\n",
    "    figdir = resdir.joinpath('figs')\n",
    "    [_.mkdir(exist_ok=True, parents=True)\n",
    "     for _ in [resdir, model_dir, figdir]]\n",
    "    [resdir.joinpath('res_{}'.format(i)).mkdir(\n",
    "        exist_ok=True, parents=True) for i in range(len(stages))]\n",
    "\n",
    "    checkpt_file = model_dir.joinpath(\"mutistages\")\n",
    "\n",
    "    # is finish\n",
    "    p_finish = Path(resdir).joinpath(\"finish\")\n",
    "    if p_finish.exists():\n",
    "        print(\n",
    "            \"[has finish]{} {}\".format(\n",
    "                time.strftime('%y%m%d-%H%M', time.localtime()), resdir.name\n",
    "            ))\n",
    "        return\n",
    "    print(\n",
    "        \"[start]{} {}\".format(\n",
    "            time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "            resdir.name\n",
    "        ))\n",
    "\n",
    "    finish_content = [\"[strat] {}\".format(time.time())]\n",
    "    print('[path_varmap] {}'.format(path_varmap))\n",
    "    adata_raw1 = load_adata(path_adata1)\n",
    "    adata_raw2 = load_adata(path_adata2)\n",
    "    if key_class not in adata_raw2.obs.columns:\n",
    "        adata_raw2.obs[key_class] = ''\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_raw1, adata_raw2 = limite_func(\n",
    "        adata_raw1, adata_raw2\n",
    "    )\n",
    "    # group_counts_unalign.csv\n",
    "    pd.concat([adata_raw1.obs[key_class].value_counts(),\n",
    "               adata_raw2.obs[key_class].value_counts(),],\n",
    "              axis=1, keys=dsnames,).to_csv(\n",
    "        resdir.joinpath(\"group_counts_unalign.csv\"), index=True\n",
    "    )\n",
    "    # 仅保留公共细胞类群\n",
    "    if aligned:\n",
    "        adata_raw1, adata_raw2 = csMAHN.pp.aligned_type(\n",
    "            [adata_raw1, adata_raw2], key_class\n",
    "        )\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat([adata_raw1.obs[key_class].value_counts(),\n",
    "                      adata_raw2.obs[key_class].value_counts(),],\n",
    "                     axis=1, keys=dsnames)\n",
    "    print(temp)\n",
    "    temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    adata_raw1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    adata_raw2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    # homo = pd.read_csv(path_varmap)\n",
    "    homo = pd.read_csv(path_varmap, usecols=range(3))\n",
    "    homo.columns = [\"gn_ref\", \"gn_que\", \"homology_type\"]\n",
    "    if kvargs.setdefault(\"is_1v1\", False):\n",
    "        homo = get_1v1_matches(homo)\n",
    "        homology_parameter = get_homology_parameters(\n",
    "            adata_raw1, adata_raw2, homo)\n",
    "        print(\"\"\"\n",
    "[homology one2one]find {homology_one2one_find} genes\n",
    "[homology one2one]use {homology_one2one_use} genes\"\"\".format(\n",
    "            **homology_parameter))\n",
    "        kvargs.update(homology_parameter)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir),\n",
    "                  'n_hvgs': n_hvgs,\n",
    "                   'n_degs': n_degs,\n",
    "                   'nfeats': nfeats,\n",
    "                   'hidden': hidden\n",
    "                   })\n",
    "    resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "    print(\n",
    "        \"\"\"Task: refernece:{} {} cells x {} gene -> query:{} {} cells x {} gene in {}\"\"\".format(\n",
    "            dsnames[0],\n",
    "            adata_raw1.shape[0],\n",
    "            adata_raw1.shape[1],\n",
    "            dsnames[1],\n",
    "            adata_raw2.shape[0],\n",
    "            adata_raw2.shape[1],\n",
    "            tissue_name))\n",
    "\n",
    "    start = time.time()\n",
    "    finish_content.append(\"[finish before run] {}\".format(time.time()))\n",
    "    # knn时间较长\n",
    "    print(\"\\n[process_for_graph]\\n\".center(100, '-'))\n",
    "    adatas, features_genes, nodes_genes, scnets, one2one, n2n = csMAHN.pp.process_for_graph(\n",
    "        [adata_raw1, adata_raw2], homo, key_class, 'leiden', n_hvgs=n_hvgs, n_degs=n_degs)\n",
    "    g, inter_net, one2one_gene_nodes_net, cell_label, n_classes, list_idx = csMAHN.pp.make_graph(\n",
    "        adatas, aligned, key_class, features_genes, nodes_genes, scnets, one2one, n2n, has_mnn=True, seed=seed)\n",
    "    end = time.time()\n",
    "    # 包括预处理时间\n",
    "    print('Times preprocess for graph:{:.2f}'.format(end - start))\n",
    "    print(\"\\n[Trainer]\\n\".center(100, '-'))\n",
    "    trainer = csMAHN.Trainer(adatas,\n",
    "                             g,\n",
    "                             inter_net,\n",
    "                             list_idx,\n",
    "                             cell_label,\n",
    "                             n_classes,\n",
    "                             threshold=threshold,\n",
    "                             key_class=key_class)\n",
    "    print(\"\\n[train]\\n\".center(100, '-'))\n",
    "    trainer.train(curdir=str(resdir),\n",
    "                  checkpt_file=str(checkpt_file),\n",
    "                  nfeats=nfeats,\n",
    "                  hidden=hidden,\n",
    "                  enhance_gama=enhance_gama,\n",
    "                  simi_gama=simi_gama)\n",
    "\n",
    "    finish_content.append(\"[finish run] {}\".format(time.time()))\n",
    "    adt = sc.AnnData(\n",
    "        trainer.embedding_hidden.detach().numpy(),\n",
    "        obs=pd.concat(\n",
    "            [\n",
    "                adatas[0]\n",
    "                .obs.loc[:, [key_class]]\n",
    "                .assign(dataset=dsnames[0]),\n",
    "                adatas[1]\n",
    "                .obs.loc[:, [key_class]]\n",
    "                .assign(dataset=dsnames[1]),\n",
    "            ]\n",
    "        ).rename(columns={key_class: \"cell_type\"}),\n",
    "    )\n",
    "    # plot_umap(trainer.embedding_hidden, adatas, dsnames, figdir)\n",
    "    adt.write_h5ad(resdir.joinpath('adt.h5ad'))\n",
    "    precess_after_csMAHN(\n",
    "        resdir, tissue_name, sp1, sp2, is_display=False, adt=adt\n",
    "    )\n",
    "    # 完成标记\n",
    "    finish_content.append(\"[end] {}\".format(time.time()))\n",
    "    p_finish.write_text(\"\\n\".join(finish_content))\n",
    "    # return trainer, adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ab9b8-ea33-4527-884c-0c35b39fa4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_csMAHN_before_custom_trainer(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path('.'),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "\n",
    "    **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "    version = 0.0.9\n",
    "    kvargs:\n",
    "        n_epochs:\n",
    "            default,[100, 200, 300]\n",
    "            stages,即res_0,res_1，res_2 的 epochs\n",
    "            累加制，res_0,res_1，res_2,实际epochs分别为100,300,600\n",
    "            故最终epochs为stages之和\n",
    "            stages = kvargs.setdefault(\"n_epochs\",[100, 200, 300])\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "        n_hvgs:\n",
    "            default,2000\n",
    "        n_degs:\n",
    "            default,50\n",
    "    \"\"\"\n",
    "    homo_method = 'biomart'\n",
    "    n_hvgs = kvargs.setdefault('n_hvgs', 2000)\n",
    "    n_degs = kvargs.setdefault('n_degs', 50)\n",
    "    seed = 123\n",
    "    stages = kvargs.setdefault(\n",
    "        'n_epochs', [\n",
    "            100, 200, 200])  # [200, 200, 200]\n",
    "    nfeats = kvargs.setdefault('nfeats', 64)  # 64  # enbedding size #128\n",
    "    hidden = kvargs.setdefault('hidden', 64)  # 64  # 128\n",
    "    input_drop = 0.2\n",
    "    att_drop = 0.2\n",
    "    residual = True\n",
    "\n",
    "    threshold = 0.9  # 0.8\n",
    "    lr = 0.01  # lr = 0.01\n",
    "    weight_decay = 0.001\n",
    "    patience = 100\n",
    "    enhance_gama = 10\n",
    "    simi_gama = 0.1\n",
    "\n",
    "    dsnames = (\n",
    "        '{}_{}'.format(\n",
    "            tissue_name, sp1), '{}_{}'.format(\n",
    "            tissue_name, sp2))\n",
    "    assert key_class1 == key_class2, \"key_class is not equal\"\n",
    "    key_class = key_class1\n",
    "\n",
    "    # make file to save\n",
    "    resdir_tag = \"{}_{}-corss-{};{}\".format(tissue_name, sp1, sp2, resdir_tag) if len(\n",
    "        resdir_tag) > 0 else \"{}_{}-corss-{}\".format(tissue_name, sp1, sp2)\n",
    "    # curdir = os.path.join()\n",
    "    resdir = Path(resdir).joinpath(resdir_tag)\n",
    "    model_dir = resdir.joinpath('model_')\n",
    "    figdir = resdir.joinpath('figs')\n",
    "    [_.mkdir(exist_ok=True, parents=True)\n",
    "     for _ in [resdir, model_dir, figdir]]\n",
    "    [resdir.joinpath('res_{}'.format(i)).mkdir(\n",
    "        exist_ok=True, parents=True) for i in range(len(stages))]\n",
    "\n",
    "    checkpt_file = model_dir.joinpath(\"mutistages\")\n",
    "\n",
    "    # # is finish\n",
    "    # p_finish = Path(resdir).joinpath(\"finish\")\n",
    "    # if p_finish.exists():\n",
    "    #     print(\n",
    "    #         \"[has finish]{} {}\".format(\n",
    "    #             time.strftime('%y%m%d-%H%M', time.localtime()), resdir.name\n",
    "    #         ))\n",
    "    #     return\n",
    "    # print(\n",
    "    #     \"[start]{} {}\".format(\n",
    "    #         time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "    #         resdir.name\n",
    "    #     ))\n",
    "\n",
    "    finish_content = [\"[strat] {}\".format(time.time())]\n",
    "    print('[path_varmap] {}'.format(path_varmap))\n",
    "    adata_raw1 = load_adata(path_adata1)\n",
    "    adata_raw2 = load_adata(path_adata2)\n",
    "    if key_class not in adata_raw2.obs.columns:\n",
    "        adata_raw2.obs[key_class] = ''\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_raw1, adata_raw2 = limite_func(\n",
    "        adata_raw1, adata_raw2\n",
    "    )\n",
    "    # # group_counts_unalign.csv\n",
    "    # pd.concat([adata_raw1.obs[key_class].value_counts(),\n",
    "    #            adata_raw2.obs[key_class].value_counts(),],\n",
    "    #           axis=1, keys=dsnames,).to_csv(\n",
    "    #     resdir.joinpath(\"group_counts_unalign.csv\"), index=True\n",
    "    # )\n",
    "    # 仅保留公共细胞类群\n",
    "    if aligned:\n",
    "        adata_raw1, adata_raw2 = csMAHN.pp.aligned_type(\n",
    "            [adata_raw1, adata_raw2], key_class\n",
    "        )\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat([adata_raw1.obs[key_class].value_counts(),\n",
    "                      adata_raw2.obs[key_class].value_counts(),],\n",
    "                     axis=1, keys=dsnames)\n",
    "    print(temp)\n",
    "    # temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    # adata_raw1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    # adata_raw2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    # homo = pd.read_csv(path_varmap)\n",
    "    homo = pd.read_csv(path_varmap, usecols=range(3))\n",
    "    homo.columns = [\"gn_ref\", \"gn_que\", \"homology_type\"]\n",
    "    if kvargs.setdefault(\"is_1v1\", False):\n",
    "        homo = get_1v1_matches(homo)\n",
    "        homology_parameter = get_homology_parameters(\n",
    "            adata_raw1, adata_raw2, homo)\n",
    "        print(\"\"\"\n",
    "[homology one2one]find {homology_one2one_find} genes\n",
    "[homology one2one]use {homology_one2one_use} genes\"\"\".format(\n",
    "            **homology_parameter))\n",
    "        kvargs.update(homology_parameter)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir),\n",
    "                  'n_hvgs': n_hvgs,\n",
    "                   'n_degs': n_degs,\n",
    "                   'nfeats': nfeats,\n",
    "                   'hidden': hidden\n",
    "                   })\n",
    "    # resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "    print(\n",
    "        \"\"\"Task: refernece:{} {} cells x {} gene -> query:{} {} cells x {} gene in {}\"\"\".format(\n",
    "            dsnames[0],\n",
    "            adata_raw1.shape[0],\n",
    "            adata_raw1.shape[1],\n",
    "            dsnames[1],\n",
    "            adata_raw2.shape[0],\n",
    "            adata_raw2.shape[1],\n",
    "            tissue_name))\n",
    "\n",
    "    start = time.time()\n",
    "    finish_content.append(\"[finish before run] {}\".format(time.time()))\n",
    "    # knn时间较长\n",
    "    print(\"\\n[process_for_graph]\\n\".center(100, '-'))\n",
    "    adatas, features_genes, nodes_genes, scnets, one2one, n2n = csMAHN.pp.process_for_graph(\n",
    "        [adata_raw1, adata_raw2], homo, key_class, 'leiden', n_hvgs=n_hvgs, n_degs=n_degs)\n",
    "    g, inter_net, one2one_gene_nodes_net, cell_label, n_classes, list_idx = csMAHN.pp.make_graph(\n",
    "        adatas, aligned, key_class, features_genes, nodes_genes, scnets, one2one, n2n, has_mnn=True, seed=seed)\n",
    "    end = time.time()\n",
    "    # 包括预处理时间\n",
    "    print('Times preprocess for graph:{:.2f}'.format(end - start))\n",
    "    return {\n",
    "        'process_for_graph': {'adatas': adatas,\n",
    "                              'features_genes': features_genes,\n",
    "                              'nodes_genes': nodes_genes,\n",
    "                              'scnets': scnets,\n",
    "                              'one2one': one2one,\n",
    "                              'n2n': n2n},\n",
    "        'make_graph': {'g': g,\n",
    "                       'inter_net': inter_net,\n",
    "                       'one2one_gene_nodes_net': one2one_gene_nodes_net,\n",
    "                       'cell_label': cell_label,\n",
    "                       'n_classes': n_classes,\n",
    "                       'list_idx': list_idx}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9eac9f-ada1-4262-afbe-4193b3dfc8a6",
   "metadata": {},
   "source": [
    "## SAMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f913bc6-a2f2-469b-9c6c-5a48eba92e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SAMap(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    path_varmap,\n",
    "    aligned=False,\n",
    "    resdir_tag=\".\",\n",
    "    resdir=Path('.'),\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "    **kvargs\n",
    "):\n",
    "\n",
    "    path_specie_1 = path_adata1\n",
    "    path_specie_2 = path_adata2\n",
    "    tissue = tissue_name\n",
    "    species = [sp1, sp2]\n",
    "    dsnames = (f\"{tissue_name}_{sp1}\", f\"{tissue_name}_{sp2}\")\n",
    "    assert key_class1 == key_class2, \"key_class is not equal\"\n",
    "    key_class = key_class1\n",
    "\n",
    "    # make file to save\n",
    "    resdir_tag = f\"{tissue_name}_{sp1}-corss-{sp2};{resdir_tag}\" if len(\n",
    "        resdir_tag) > 0 else f\"{tissue_name}_{sp1}-corss-{sp2}\"\n",
    "    curdir = os.path.join(resdir, resdir_tag)\n",
    "    resdir = Path(curdir)\n",
    "    figdir = os.path.join(curdir, 'figs')\n",
    "    [Path(_).mkdir(exist_ok=True, parents=True)\n",
    "     for _ in [curdir, figdir]]\n",
    "\n",
    "    # is finish\n",
    "    p_finish = Path(resdir).joinpath(\"finish\")\n",
    "    if p_finish.exists():\n",
    "        print(\n",
    "            \"[has finish]{} {}\".format(\n",
    "                time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "                resdir.name))\n",
    "        return\n",
    "    print(\n",
    "        \"[start]{} {}\".format(\n",
    "            time.strftime('%y%m%d-%H%M', time.localtime()),\n",
    "            resdir.name\n",
    "        ))\n",
    "\n",
    "    finish_content = [\"[strat] {}\".format(time.time())]\n",
    "    print('[path_varmap] {}'.format(path_varmap))\n",
    "    adata_1 = load_adata(path_specie_1)\n",
    "    adata_2 = load_adata(path_specie_2)\n",
    "    assert pd.Series(\n",
    "        np.concatenate(\n",
    "            (adata_1.obs.index, adata_2.obs.index))).is_unique, '[Error] index is not unique'\n",
    "\n",
    "    # limite 进一步对adata进行限制，默认不操作直接返回\n",
    "    adata_1, adata_2 = limite_func(\n",
    "        adata_1, adata_2\n",
    "    )\n",
    "    # group_counts_unalign.csv\n",
    "    pd.concat([adata_1.obs[key_class].value_counts(),\n",
    "               adata_2.obs[key_class].value_counts(),],\n",
    "              axis=1, keys=dsnames,).to_csv(\n",
    "        resdir.joinpath(\"group_counts_unalign.csv\"), index=True\n",
    "    )\n",
    "    # 仅保留公共细胞类群\n",
    "    if aligned:\n",
    "        adata_1, adata_2 = csMAHN.pp.aligned_type(\n",
    "            [adata_1, adata_2], key_class\n",
    "        )\n",
    "\n",
    "    # group_counts.csv\n",
    "    temp = pd.concat([adata_1.obs[key_class].value_counts(),\n",
    "                      adata_2.obs[key_class].value_counts(),],\n",
    "                     axis=1, keys=dsnames)\n",
    "    print(temp)\n",
    "    temp.to_csv(resdir.joinpath(\"group_counts.csv\"), index=True)\n",
    "    # if temp.shape[0] < 2:\n",
    "    #     # 错误标记\n",
    "    #     print(\"[Error][group_counts no any item]\")\n",
    "    #     finish_content.append(\n",
    "    #         \"[Error][group_counts no any item] %f\" % time.time()\n",
    "    #     )\n",
    "    #     p_finish.with_name(\"error\").write_text(\"\\n\".join(finish_content))\n",
    "    #     return\n",
    "\n",
    "    adata_1.obs.to_csv(resdir.joinpath(\"obs_ref.csv\"), index=True)\n",
    "    adata_2.obs.to_csv(resdir.joinpath(\"obs_que.csv\"), index=True)\n",
    "\n",
    "    kvargs.update({'path_adata1': str(path_adata1),\n",
    "                   'path_adata2': str(path_adata2),\n",
    "                   'key_class1': key_class1,\n",
    "                   'key_class2': key_class2,\n",
    "                   'sp1': sp1,\n",
    "                   'sp2': sp2,\n",
    "                   'tissue_name': tissue_name,\n",
    "                   'path_varmap': str(path_varmap),\n",
    "                   'aligned': aligned,\n",
    "                   'resdir_tag': resdir_tag,\n",
    "                   'resdir': str(resdir)})\n",
    "    resdir.joinpath(\"kvargs.json\").write_text(dumps(kvargs))\n",
    "    start = time.time()\n",
    "    finish_content.append(\"[finish before run] {}\".format(time.time()))\n",
    "    # SAMap -----------------------------------------------------\n",
    "\n",
    "    sq_ref_SAMap = map_sp_SAMap[map_sp[sp1]]\n",
    "    sq_que_SAMap = map_sp_SAMap[map_sp[sp2]]\n",
    "    _SAMP = {\n",
    "        sq_ref_SAMap: SAM(counts=adata_1),\n",
    "        sq_que_SAMap: SAM(counts=adata_2)\n",
    "    }\n",
    "    [v.preprocess_data() for k, v in _SAMP.items()]\n",
    "    [v.run() for k, v in _SAMP.items()]\n",
    "\n",
    "    keys = {\n",
    "        sq_ref_SAMap: key_class1,\n",
    "        sq_que_SAMap: key_class2\n",
    "\n",
    "    }\n",
    "    sm = SAMAP(\n",
    "        _SAMP,\n",
    "        # f_maps + \"{}{}/{}_to_{}.txt\".format(id2, id1, id2, id1)\n",
    "        # 。。。昂,不能传Path,得传str,还得加个 os.sep\n",
    "        f_maps=str(path_varmap) if str(path_varmap).endswith(\n",
    "            os.sep) else str(path_varmap) + os.sep,\n",
    "        keys=keys, is_1v1=kvargs.setdefault('is_1v1', False)\n",
    "\n",
    "    )\n",
    "\n",
    "    # run SAMap\n",
    "    sm.run(pairwise=False)\n",
    "    samap = sm.samap  # SAM object with three species stitched together\n",
    "\n",
    "    finish_content.append(\"[finish run] {}\".format(time.time()))\n",
    "\n",
    "    alignment_score = samap.adata.obs.loc[:, ['species']].join(\n",
    "        get_alignment_score_for_each_cell(sm, keys))\n",
    "    alignment_score.to_csv(resdir.joinpath(\n",
    "        'alignment_score_for_each_cell.csv'), index=True)\n",
    "    alignment_score.head(2)\n",
    "\n",
    "    alignment_score_query = alignment_score\\\n",
    "        .query(\"species == '{}'\".format(sq_que_SAMap))\\\n",
    "        .filter(regex=\"^{}\".format(sq_ref_SAMap))\n",
    "    # 取每个细胞 在各类型上alignment score 最大值对应的类型\n",
    "    alignment_score_query['pre_label'] = [\n",
    "        alignment_score_query.columns[i] for i in np.argmax(\n",
    "            alignment_score_query, axis=1)]\n",
    "    alignment_score_query['pre_label'] = alignment_score_query['pre_label'].str.replace(\n",
    "        '{}_'.format(sq_ref_SAMap), '').str.replace('{}_'.format(sq_que_SAMap), '')\n",
    "    alignment_score_query['max_prob'] = alignment_score_query.filter(\n",
    "        regex=\"^{}\".format(sq_ref_SAMap)).max(axis=1)\n",
    "    alignment_score_query.head(2)\n",
    "\n",
    "    # constrect the res_obs\n",
    "    res_obs = samap.adata.obs.loc[:, ['species']].copy()\n",
    "    res_obs = res_obs.join(pd.DataFrame(samap.adata.obsm['X_umap'],\n",
    "                                        columns='UMAP1,UMAP2'.split(','),\n",
    "                                        index=samap.adata.obs.index))\n",
    "    res_obs['dataset'] = res_obs['species'].map(\n",
    "        {k: v for k, v in zip([sq_ref_SAMap, sq_que_SAMap], dsnames)})\n",
    "    res_obs['cell_type'] = samap.adata.obs['{};{}_mapping_scores'.format(key_class1, key_class2)].str.replace(\n",
    "        '^{}_'.format(sq_ref_SAMap), '', regex=True\n",
    "    ).str.replace(\n",
    "        '^{}_'.format(sq_que_SAMap), '', regex=True\n",
    "    )\n",
    "    res_obs['true_label'] = res_obs['cell_type']\n",
    "    res_obs = res_obs.drop(columns='species')\n",
    "\n",
    "    res_obs = res_obs.join(\n",
    "        alignment_score_query.loc[:, 'pre_label,max_prob'.split(',')])\n",
    "    res_obs['is_right'] = res_obs.eval('true_label == pre_label')\n",
    "    res_obs.to_csv(resdir.joinpath('obs.csv'), index=True)\n",
    "\n",
    "    # df_ratio\n",
    "    df_ratio = group_agg(res_obs, 'dataset,is_right'.split(','), {\n",
    "        'is_right': ['sum']\n",
    "    }).merge(\n",
    "        res_obs['dataset'].value_counts().to_frame(name='dataset_count'),\n",
    "        on='dataset'\n",
    "    )\n",
    "    df_ratio = df_ratio.query('is_right').drop(\n",
    "        columns='is_right').rename(\n",
    "        columns={\n",
    "            'dataset_count': 'is_right_count'})\n",
    "    df_ratio = df_ratio.join(df_ratio['dataset'].str.extract(\n",
    "        '(?P<tissue>[^_]+)_(?P<sp>[^_]+)'))\n",
    "    assert df_ratio['sp'].is_unique, '[Error] not unique'\n",
    "    df_ratio.index = df_ratio['sp'].to_numpy()\n",
    "    df_ratio.loc[sp1, 'is_right_sum'] = np.nan\n",
    "\n",
    "    df_ratio['ratio'] = df_ratio.eval(\"is_right_sum/is_right_count\")\n",
    "\n",
    "    df_ratio['type'] = 'species'\n",
    "    df_ratio['name'] = ''\n",
    "    df_ratio = df_ratio.loc[:,\n",
    "                            'tissue,type,sp,name,is_right_sum,is_right_count,ratio'.split(',')]\n",
    "    df_ratio.to_csv(resdir.joinpath('ratio.csv'), index=False)\n",
    "\n",
    "    # plot umap\n",
    "    samap.adata.obs['cell_type'] = samap.adata.obs['{};{}_mapping_scores'.format(key_class1, key_class2)].str.replace(\n",
    "        '^{}_'.format(sq_ref_SAMap), '', regex=True).str.replace('^{}_'.format(sq_que_SAMap), '', regex=True)\n",
    "    samap.adata.obs['dataset'] = samap.adata.obs['species'].map(\n",
    "        {k: v for k, v in zip([sq_ref_SAMap, sq_que_SAMap], dsnames)})\n",
    "    display(samap.adata.obs.head(2))\n",
    "\n",
    "    ax = sc.pl.umap(samap.adata, color='dataset', show=False)\n",
    "    ax.figure.savefig(\n",
    "        Path(figdir).joinpath('umap_dataset.png'),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=120)\n",
    "    ax = sc.pl.umap(samap.adata, color='cell_type', show=False)\n",
    "    ax.figure.savefig(\n",
    "        Path(figdir).joinpath('umap_umap.png'),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=120)\n",
    "\n",
    "    # 完成标记\n",
    "    finish_content.append(\"[end] {}\".format(time.time()))\n",
    "    p_finish.write_text(\"\\n\".join(finish_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df548d2e-9528-48b1-abaf-98db1d6e1d51",
   "metadata": {},
   "source": [
    "## run_cross_species_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a35cd-0b71-46cd-b19e-08cc5c4cf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func_run_cross_species_models = {\n",
    "    'came': run_came,\n",
    "    'csMAHN': run_csMAHN,\n",
    "    'SAMap': run_SAMap,\n",
    "    'csMAHN_before_custom_trainer':run_csMAHN_before_custom_trainer\n",
    "}\n",
    "\n",
    "del run_came,run_csMAHN,run_SAMap,run_csMAHN_before_custom_trainer\n",
    "\n",
    "def run_cross_species_models(\n",
    "    path_adata1,\n",
    "    path_adata2,\n",
    "    key_class1,\n",
    "    key_class2,\n",
    "    sp1,\n",
    "    sp2,\n",
    "    tissue_name,\n",
    "    resdir,\n",
    "    resdir_tag=\"\",\n",
    "    aligned=False,\n",
    "    limite_func=lambda adata1, adata2: (adata1, adata2),\n",
    "    models=''.split(','),\n",
    "    **kvargs\n",
    "):\n",
    "    \"\"\"\n",
    "        models: came,csMAHN,SAMap,csMAHN_before_custom_trainer\n",
    "        kvargs:\n",
    "        n_epochs:\n",
    "            default,[100, 200, 300]\n",
    "            stages,即res_0,res_1，res_2 的 epochs\n",
    "            累加制，res_0,res_1，res_2,实际epochs分别为100,300,600\n",
    "            故最终epochs为stages之和\n",
    "            stages = kvargs.setdefault(\"n_epochs\",[100, 200, 300])\n",
    "\n",
    "\n",
    "        is_1v1: bool\n",
    "            default,False\n",
    "    \"\"\"\n",
    "\n",
    "    assert pd.Series([model in map_func_run_cross_species_models.keys() for model in models]).all(\n",
    "    ), \"[Error] not all models in {}\\nmodels {}\".format(','.join(map_func.keys()), ','.join(models), )\n",
    "    res = {}\n",
    "    for model in models:\n",
    "        path_varmap = get_path_varmap(\n",
    "            map_sp[sp1], map_sp[sp2], model=model)\n",
    "        print(path_varmap)\n",
    "        print('[path_varmap] {}\\t{}'.format(model, Path(path_varmap).name))\n",
    "        _res = map_func_run_cross_species_models[model](\n",
    "            path_adata1,\n",
    "            path_adata2,\n",
    "            key_class1,\n",
    "            key_class2,\n",
    "            sp1,\n",
    "            sp2,\n",
    "            tissue_name,\n",
    "            path_varmap,\n",
    "            aligned=aligned,\n",
    "            resdir_tag=\";\".join([model, resdir_tag]),\n",
    "            resdir=resdir,\n",
    "            limite_func=limite_func,\n",
    "            **kvargs,\n",
    "        )\n",
    "        res.update( {model:_res})\n",
    "    if len(res.keys()) == 1:\n",
    "        res = list(res.values())[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8eb6c-b5c0-4958-8137-2ac19d9847ba",
   "metadata": {},
   "source": [
    "# help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfeb372-4327-4deb-9315-2e21f088eb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func_help(show_absolute=False):\n",
    "    text = \"\"\"\n",
    "-------------------------func_help-------------------------\n",
    "> parameter\n",
    "    p_root\\t{}\n",
    "        p_run, p_plot, p_res, p_cache, p_pdf\n",
    "    p_df_varmap\n",
    "    map_sp_reverse\n",
    "    rng\n",
    "> run\n",
    "    run_cross_species_models\n",
    "    h5ad_to_mtx\n",
    "    load_adata\n",
    "    get_path_varmap\n",
    "    find_path_from_para\n",
    "    load_normalized_adata\n",
    "\n",
    "> res\n",
    "    get_test_result_df\n",
    "    get_res_obs\n",
    "    get_adata_umap\n",
    "    show_umap\n",
    "\n",
    "> plot\n",
    "    get_color_map\n",
    "    show_color_map\n",
    "    show_color\n",
    "    plot_umap\n",
    "    savefig\n",
    "\"\"\".format(p_root.absolute() if show_absolute else '[name] {}'.format(p_root.name))\n",
    "    print(text)\n",
    "\n",
    "\n",
    "if __name__ != '__main__':\n",
    "    func_help()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "publish",
   "language": "python",
   "name": "publish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
